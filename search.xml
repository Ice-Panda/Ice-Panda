<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[项目实战——Embedding Visualizer]]></title>
    <url>%2F2018%2F11%2F01%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94Embedding%20Visualizer%2F</url>
    <content type="text"><![CDATA[项目实战——Embedding Visualizer TensorBoard提供了PROJECTOR界面来可视化高维向量之间的关系。 在PROJECTOR中，通过展示MNIST图片信息以及每张图片对应的真实标签，PROJECTOR要求用户准备一个sprite图像(所谓sprite图像就是将一组图片组合成一整张大图片)和一个tsv(tab seperated file)文件给出每张图片对应的标签信息(通过定义各类标签颜色来区分)。 代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293import matplotlib.pyplot as pltimport tensorflow as tfimport numpy as npimport osimport sslfrom tensorflow.contrib.tensorboard.plugins import projectorfrom tensorflow.examples.tutorials.mnist import input_datassl._create_default_https_context = ssl._create_unverified_contexttf.app.flags.DEFINE_boolean('init', False, '是否开启文件初始化过程')tf.app.flags.DEFINE_string('log_dir', 'log', '日志文件目录')tf.app.flags.DEFINE_string('pic_name', 'meta_sprite.png', '保存的sprite图片名称')tf.app.flags.DEFINE_string('color_name', 'meta_color.tsv', '保存的color文件名称')tf.app.flags.DEFINE_string('variable_name', 'meta_variable', '需要展示的变量名称')# tf.app.flags.DEFINE_integer('count', '5000', '显示数量')tf.app.flags.DEFINE_integer('max_step', '500', '最大迭代次数')FLAGS = tf.app.flags.FLAGSclass Spriter(object): def __init__(self): self.log_dir = FLAGS.log_dir self.pic_name = FLAGS.pic_name self.color_name = FLAGS.color_name assert self.color_name.endswith('.tsv') def create_sprite_image(self, images): ''' 返回一个sprite图像，该图像由作为参数传递的图像组成。图像需要保证高x宽y :param images: images matrix :return: sprite image ''' if isinstance(images, list): images = np.array(images) # 获取图片高和宽 img_h = images.shape[1] img_w = images.shape[2] # sprite图像可以理解成是小图片平成的大正方形矩阵，大正方形矩阵中的每一个元素就是原来的小图片。于是这个正方形的边长就是sqrt(n),其中n为小图片的数量。 # 获取图片数量，并组织成x*x的sprite图像 n_plots = int(np.ceil(np.sqrt(images.shape[0]))) spriteimage = np.ones((img_h * n_plots, img_w * n_plots)) for i in range(n_plots): for j in range(n_plots): this_filter = i * n_plots + j if this_filter &lt; images.shape[0]: this_img = images[this_filter] spriteimage[i * img_h:(i + 1) * img_h, j * img_w:(j + 1) * img_w] = this_img return spriteimage def vector_to_matrix_mnist(self, inputs): ''' reshape mnist数据矩阵 :param inpust : [batch, h * w] :return: [batch, h, w] ''' return np.reshape(inputs, (-1, 28, 28)) def invert_grayscale(self, inputs): ''' 反转灰度 :param inputs : inputs (0, 1) :return: 1 =&gt; 0, 0 =&gt; 1 ''' return 1 - inputs def save_sprite_image(self, batch_xs): ''' 生成并保存sprite图像 :param batch_xs: Variables that need to visualise ''' to_visualise = batch_xs to_visualise = self.vector_to_matrix_mnist(to_visualise) to_visualise = self.invert_grayscale(to_visualise) sprite_image = self.create_sprite_image(to_visualise) plt.imsave(os.path.join(self.log_dir, self.pic_name), sprite_image, cmap='gray') plt.imshow(sprite_image, cmap='gray') def save_color_data(self, batch_ys): ''' 嵌入可视化工具需要提供每个标签对应的颜色。保存在“TSV(标签分离)”文件中。 :return: ''' with open(os.path.join(self.log_dir, self.color_name), 'w') as f: f.write("Index\tLabel\n") for index, label in enumerate(batch_ys): print(index, label) f.write("%d\t%d\n" % (index, label))class Mnist(object): def __init__(self): self.W = None self.b = None def weight_variable(self, shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(self, shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def inputs(self): '''准备数据''' # x [None, 28*28]; y_true [None, 10] X = tf.placeholder(tf.float32, [None, 28 * 28]) y_true = tf.placeholder(tf.float32, [None, 10]) return X, y_true def conv_model(self, x): ''' 卷积网络结构建模 :return: x, y_true, y_predict ''' # 1.conv1 with tf.variable_scope('conv1'): # 随机初始化权重[5, 5, 1, 32](其中的1为输入的通道数), 偏置[32] w_conv1 = self.weight_variable([5, 5, 1, 32]) b_conv1 = self.bias_variable([32]) # 首先进行卷积计算, K=32, F=5, S=1, P="SAME" # x [None, 784] =&gt; [None, 28, 28, 1] x_conv1 =&gt; [None, 28, 28, 32] # -1 can also be used to infer the shape x_conv1_reshape = tf.reshape(x, [-1, 28, 28, 1]) # input =&gt; 4D x_conv1 = tf.nn.conv2d(x_conv1_reshape, w_conv1, strides=[1, 1, 1, 1], padding="SAME") + b_conv1 # 进行激活函数计算 # x_relu1 =&gt; [None, 28, 28, 32] x_relu1 = tf.nn.relu(x_conv1) # 进行池化层计算 # F=2, S=2 # [None, 28, 28, 32] =&gt; [None, 14, 14, 32] x_pool1 = tf.nn.max_pool(x_relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") # 2.conv2 # input =&gt; [None, 14, 14, 32] with tf.variable_scope('conv2'): # 每个filter带32张5*5的观察权重，一共有64个filter # 随机初始化权重[5, 5, 32, 64](其中的32为输入的通道数), 偏置[64] w_conv2 = self.weight_variable([5, 5, 32, 64]) b_conv2 = self.bias_variable([64]) # 首先进行卷积计算, K=64, F=5, S=1, P="SAME" # x [None, 14, 14, 32]; x_conv2 =&gt; [None, 14, 14, 64] # input =&gt; 4D x_conv2 = tf.nn.conv2d(x_pool1, w_conv2, strides=[1, 1, 1, 1], padding="SAME") + b_conv2 # 进行激活函数计算 # x_relu2 =&gt; [None, 14, 14, 64] x_relu2 = tf.nn.relu(x_conv2) # 进行池化层计算 # F=2, S=2 # [None, 14, 14, 64] =&gt; [None, 7, 7, 64] x_pool2 = tf.nn.max_pool(x_relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") # 3.fc # 每个样本输出类别的个数10个结束 # input =&gt; x_pool2 [None, 7, 7, 64] # 矩阵运算 [None, 7*7*64] * [7*7*64, 10] + [10] = [None, 10] with tf.variable_scope('fc'): # 确定全连接权重和偏置 w_fc = self.weight_variable([7 * 7 * 64, 10]) b_fc = self.bias_variable([10]) # 对上一层的输出结果的形状进行处理成2维形状 x_fc = tf.reshape(x_pool2, [-1, 7 * 7 * 64]) # 进行全连接运算 y_predict = tf.matmul(x_fc, w_fc) + b_fc return y_predict def cross_entropy(self, y_true, y_pre): '''softmax回归以及交叉熵损失计算''' # labels=标签值; logits=样本加权之后的值 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pre)) return loss def sgd_op(self, loss): # optimizer = tf.train.AdamOptimizer(0.1).minimize(loss) optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss) return optimizer def evaluation(self, y_true, y_pre): equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pre, 1)) accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)) return accuracy def merge_summary(self, loss, accuracy): '''收集合并变量''' # 收集损失和准确率 tf.summary.scalar('loss', loss) tf.summary.scalar('acc', accuracy) # 合并所有变量op merged = tf.summary.merge_all() return merged def train(self, mnist): # 创建默认图 g = tf.get_default_graph() with g.as_default(): '''将g图作为默认图''' X, y_true = self.inputs() # 全连接层神经网络计算 y_pre = self.conv_model(X) # 交叉熵损失 loss = self.cross_entropy(y_true, y_pre) # 梯度下降优化 optimizer = self.sgd_op(loss) # 评估准确率 accuracy = self.evaluation(y_true, y_pre) # 创建saver对象 saver = tf.train.Saver() # 开启会话 with tf.Session() as sess: # 初始化变量 sess.run(tf.global_variables_initializer()) # 检查模型 checkpoint = tf.train.latest_checkpoint(FLAGS.log_dir) if checkpoint: print('restore', checkpoint) # 加载模型 saver.restore(sess, checkpoint) for i in range(FLAGS.max_step): batch_xs, batch_ys = mnist.train.next_batch(50) sess.run(optimizer, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;) print('训练第%d步的准确率为:%f, 损失为%f' % ( i + 1, sess.run(accuracy, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;), sess.run(loss, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;))) if i%100 == 0: saver.save(sess, os.path.join(FLAGS.log_dir, 'model.ckpt'), 1) res = sess.run(y_pre, feed_dict=&#123;X: mnist.test.images&#125;) # print(res) return resclass Embedding(object): def __init__(self): pass def visualise(self, batch_xs): # 创建embeddings embedding = tf.Variable(batch_xs, name=FLAGS.variable_name) writer = tf.summary.FileWriter(FLAGS.log_dir) # 创建embedding projector config = projector.ProjectorConfig() # 增加需要可视化的embedding结果 embedding_config = config.embeddings.add() # 指定embedding张量名称 embedding_config.tensor_name = embedding.name # 指定tsv文件路径(此处会自动拼接tensorboard --logdir的路径) embedding_config.metadata_path = FLAGS.color_name # 指定sprite图像路径(此处会自动拼接tensorboard --logdir的路径) embedding_config.sprite.image_path = FLAGS.pic_name print(embedding_config.sprite.image_path) # 指定单个缩略图的高度和宽度 embedding_config.sprite.single_image_dim.extend([28, 28]) # 指定可视化的writer与config projector.visualize_embeddings(writer, config)def main(argv): if FLAGS.init: mnist = input_data.read_data_sets('../data/mnist', one_hot=False) batch_xs, batch_ys = mnist.test.images,mnist.test.labels # [batch, 784] spriter = Spriter() spriter.save_sprite_image(batch_xs) spriter.save_color_data(batch_ys) m = input_data.read_data_sets('../data/mnist', one_hot=True) embedding = Embedding() mnist = Mnist() visual_batch = mnist.train(m) embedding.visualise(visual_batch)if __name__ == '__main__': tf.app.run() 在terminal中打开tensorboard1tensorboard --logdir='模型保存位置' 在浏览器中输入localhost:6006出现如下PCA投影到三维空间的结果]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Convolution Neural Network</tag>
        <tag>TensorFlow</tag>
        <tag>Mnist</tag>
        <tag>Embedding Visualizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能六大领域]]></title>
    <url>%2F2018%2F10%2F29%2FArtificialIntelligence%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%85%AD%E5%A4%A7%E9%A2%86%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[人工智能六大领域深度学习深度学习作为人工智能领域的一个应用分支，不管是从市面上公司的数量还是投资人投资喜好的角度来说，都是一重要应用领域。说道深度学习，大家第一个想到的肯定是AlphaGo，通过一次又一次的学习、更新算法，最终在人机大战中打败围棋大师李世石。百度的机器人“小度”多次参加最强大脑的“人机大战”，并取得胜利，亦是深度学习的结果。深度学习的技术原理 构建一个网络并且随机初始化所有连续的权重 将大量的数据情况输出到这个网络中 网络处理这些动作并且进行学习 如果这个动作符合指定的动作，将会增加权重；如果不符合，将会降低权重 系统通过如上过程调整权重 在成千上万次学习之后，超过人类的表现 计算机视觉计算机视觉是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉有着广泛的细分应用，其中包括，医疗成像分析被用来提高疾病的预测、诊断和治疗；人脸识别被支付宝或者网上一些自助服务用来自动识别照片里的人物。同时在安防及监控领域，也有很多的应用……计算机视觉的技术原理 计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务。比如，一些技术能够从图像中检测到物体的边缘及纹理。分类技术可被用作确定识别到的特征是否能够代表系统已知的一类物体 语音识别语音识别技术最通俗易懂的讲法就是语音转化为文字，并对其进行识别任职和处理。语音识别的主要应用包括医疗听写、语音书写、电脑系统声控、电话客服等。语音识别技术原理 对声音进行处理，使用移动窗函数对声音进行分帧 声音被分帧后，变为很多波形，需要将波形做声学体征提取，变为状态 特征提取之后，声音就编程一个N行、N列的矩阵。通过音素组合成单词 语言处理自然语言处理(NLP)，像计算机视觉技术一样，将各种有助于实现目标的多种技术进行了融合，实现了人机间自然语言通信。同时也包括自然语言生成(NLG)。语言处理技术原理 汉字编码词法分析 句法分析 语义分析 文本生成 语音识别 智能机器人智能机器人在生活中随处可见，扫地机器人、陪伴机器人……这些机器人不管是跟人语音聊天，还是自主定位导航行走、安防监控等，都离不开人工智能技术的支持。智能机器人技术原理 人工智能技术把机器视觉、自动划分等认知技术、各种传感器整合到机器人身上，使得机器人拥有判断、决策的能力，能在各种不同的环境中处理不同的任务 智能穿戴设备、智能家电、智能出行或者无人机设备其实都是类似的原理 引擎推荐(推荐系统)不知道大家现在上网有没有这样的体验，那就是网站会根据你之前浏览过的页面、搜索过的关键字推送给你一些相关的网站内容。这其实就是引擎推荐技术的一种表现。Google为什么会做免费搜索引擎，目的就是为了搜集大量的自然搜索数据，丰富他的大数据库，为后面的人工智能数据库做准备。引擎推荐技术原理 推荐引擎是基于用户的行为、属性(用户浏览网站产生的数据)，通过算法分析和处理，主动发现用户当前或潜在需求，并主动推送信息给用户的信息网络 快速推荐给用户信息，提高浏览效率和转化率]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据与MapReduce学习笔记——本地HADOOP部署(Mac)]]></title>
    <url>%2F2018%2F10%2F29%2FBigData%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EMapReduce%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%AC%E5%9C%B0HADOOP%E9%83%A8%E7%BD%B2(Mac)%2F</url>
    <content type="text"><![CDATA[大数据与MapReduce学习笔记——本地HADOOP部署(Mac)ssh的配置以及验证配置ssh 确认mac的远程登录是否开启系统偏好设置-&gt;共享-&gt;勾选远程登录 打开terminal并输入如下命令1ssh-keygen 然后一直回车，当执行完这条命令后可以前往~/.ssh目录下发下如下两个文件当出现这两个文件之后，再在terminal中执行如下代码(该代码用于ssh免登陆设置，若不需要可以不进行设置)1cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys 至此，完成ssh免登陆设置 验证ssh配置完ssh之后可以在terminal中输入如下命令进行验证1ssh localhost 安装jdk Java SE Development Kit 8u191(JDK) https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 安装Hadoophadoop-2.9.2 http://mirror.bit.edu.cn/apache/hadoop/common/stable2/ 修改Hadoop配置文件需要修改的配置文件均在hadoop-2.9.2/etc/hadoop目录下 在hadoop-env.sh中加入以下代码 1export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home 在core-site.xml中加入以下代码 1234567891011121314&lt;configuration&gt; &lt;!-- 指定namenode的通信地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://0.0.0.0:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/Library/hadoop-2.9.2/temp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 在hdfs-site.xml中加入以下代码 默认副本数3，修改为1，dfs.namenode.name.dir指明fsimage存放目录，多个目录用逗号隔开。dfs.datanode.data.dir指定块文件存放目录，多个目录逗号隔开12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/Library/hadoop-2.9.2/tmp/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/Library/hadoop-2.9.2/tmp/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50070&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在mapred-site.xml中加入以下代码 12345678&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在yarn-site.xml中加入以下代码 1234567&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/configuration&gt; 配置Hadoop环境变量 在/etc/profile中加入以下代码123# HADOOP_HOME=hadoop安装目录下/binexport HADOOP_HOME=/Library/hadoop-2.9.2/binexport PATH=$PATH:$HADOOP_HOME 修改完成以后重启terminal或者在terminal中输入1source /etc/profile 运行Hadoop打开terminal并按以下步骤12345678# 进入hadoop安装目录cd $HADOOP_HOME# 初始化namenodehdfs namenode -format# 启动hdfs../sbin/start-dfs.sh# 启动yarn../sbin/start-yarn.sh 当启动完hdfs以及yarn可以打开浏览器，在浏览器中输入以下地址1http://localhost:50070 1http://loaclhost:8088 测试demo123hdfs dfs -mkdir -p /user/(你本机用户名)/input//3.这个时候你可以执行以下命令来查看目录是否在hdfs上创建成功hadoop fs -ls /user/(你本机用户名)/ 测试成功的结果如下图 此处的output文件夹为执行程序之后生成的文件夹，output文件夹切勿手动创建！！！ 准备工作完毕，继续测试demo的步骤12345678910# 1.将文件上传到hdfs上hadoop fs -copyFromLocal inputFile.txt input# 也可以使用 hadoop dfs -put inputFile.txt input# 2.启动任务hadoop jar /Library/hadoop-2.9.2/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar -input input -output output -mapper "python mrMeanMapper.py" -reducer "python mrMeanReducer.py"# 3.查看结果(part-00000为生成文件)hadoop fs -cat output/part-00000# 4.下载结果hadoop fs -copyToLocal output/part-00000# 也可以使用 hadoop dfs -get output/part-00000 查看下载到本地的结果至此，完成demo测试]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据与MapReduce学习笔记——Hadoop流]]></title>
    <url>%2F2018%2F10%2F28%2FBigData%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EMapReduce%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Hadoop%E6%B5%81%2F</url>
    <content type="text"><![CDATA[大数据与MapReduce学习笔记——Hadoop流Hadoop流Apache的Hadoop项目是MapReduce框架的一个实现。Hadoop是一个开源的Java项目，为运行MapReduce作业提供了大量所需的功能。除了分布式计算之外，Hadoop自带分布式文件系统。Hadoop开源运行Java之外的其他语言编写的分布式程序。Hadoop流很像Linux系统中的管道(管道使用符号|，可以将一个命令的输出作为另一个命令的输入)。如果用mapper.py调用mapper，用reducer.py调用reducer，那么Hadoop流就可以像Linux命令一样执行，例如：1cat inputFile.txt | python mapper.py | sort | python reducer.py &gt; outputFile.txt 这样，类似的Hadoop流就可以在多台机器上分布式执行，用户可以通过Linux命令来测试Python语言编写的MapReduce脚本。 分布式计算均值和方差的mapper接下来我们将构建一个海量数据上分布式计算均值和方差的MapReduce作业。示范起见，这里只选取了一个小数据集。在文本编辑器中创建文件mrMeanMapper.py，并加入如下程序清单中的代码。12345678910111213141516'''mapper代码'''import sysfrom numpy import mat, mean, powerdef read_input(file): for line in file: yield line.rstrip() input = read_input(sys.stdin)input = [float(line) for line in input]numInputs = len(input)input = mat(input)sqInput = power(input, 2)print('%d\t%f\t%f' % (numInputs, mean(input), mean(sqInput)))print('report: still alive', file = sys.stderr) 这是一个很简单的例子：该mapper首先按行读取所有的输入并创建一组对应的浮点数，然后得到数组的长度并创建NumPy矩阵。再对所有的值进行平方，最后将均值和平方后的均值发送出去。这些值将用于计算全局的均值和方差。 一个好的习惯是向标准错误输出发送报告。如果某作业10分钟内没有报告输出，则将被Hadoop终止。 首先确认一下在已下载的源码文件inputFile.txt中包含了100个数。查看inputFile.txt首部五条信息1cat inputFile.txt | tail -n 5 运行结果如下123450.5657070.4136700.0805070.9299780.609755 在正式使用Hadoop之前，先来测试一下mapper1cat inputFile.txt | python mrMeanMapper.py 如果在Windows系统下，可在DOS窗口输入一下命令：1python mrMeanMapper.py &lt; inputFile.txt 运行结果如下12100 0.509570 0.344439report: still alive 其中第一行是标准输出，也就是reducer的输入；第二行是标准错误输出，即对主节点做出的响应报告，表明本节点工作正常。 分布式计算均值和方差的reducermapper接受原始的输入并产生中间值传递给reducer。很多mapper是并行执行的，所以需要将这些mapper的输出合并成一个值。接下来给出reducer的代码：将中间的键值对进行组合。打开文本编辑器，建立文件mrMeanReducer.py，然后输入如下程序清单中的代码。12345678910111213141516171819202122'''reducer代码'''import sysfrom numpy import mat, mean, powerdef read_input(file): for line in file: yield line.rstrip()input = read_input(sys.stdin)mapperOut = [line.split('\t') for line in input]cumVal = 0.0cumSumSq = 0.0cumN = 0.0for instance in mapperOut: nj = float(instance[0]) cumN += nj cumVal += nj * float(instance[1]) cumSumSq += nj * float(instance[2])mean = cumVal / cumNvarSum = (cumSumSq - 2 * mean * cumVal + cumN * mean * mean) / cumNprint('%d\t%df\t%f' % (cumN, mean, varSum))print('report: still alive',file = sys.stderr) 以上就是reducer的代码，它接受mapper程序的输出，并将它们合成为全局的均值和误差，从而完成任务。执行reducer1cat inputFile.txt | python mrMeanMapper.py | python mrMeanReducer.py 如果是DOS环境，键入如下命令：1python mrMeanMapper.py &lt; inputFile.txt | python mrMeanReducer.py 运行结果如下123report: still alive100 0f 0.084777report: still alive]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据与MapReduce学习笔记——MapReduce]]></title>
    <url>%2F2018%2F10%2F27%2FBigData%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EMapReduce%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94MapReduce%2F</url>
    <content type="text"><![CDATA[大数据与MapReduce学习笔记——MapReduceMapReduce：分布式计算的框架 优点：可在短时间内完成大量工作 缺点：算法必须经过重写，需要对系统工程有一定的理解 使用数据类型：数值型和标称型数据 MapReduce是一个软件框架，可以将单个计算作业分给多台计算机执行。它假定这些作业在单机熵需要很长的运行时间，因此使用多台机器缩短运行时间。常见的例子是日常统计数字的汇总，该任务单机熵执行时间将超过一整天。MapReduce在大量节点组成的集群上运行。它的工作流程是：单个作业被分成很多小份，输入数据也被切片分发到每个节点，各个节点只在本地数据上做运算，对应的运算代码称为mapper，这个过程被称作map阶段。每个mapper的输出通过某种方式组合(一般还会做排序)。排序后的结果再被分成小份分发到各个节点进行下一步处理工作。第二步的处理阶段被称为reduce阶段，对应的运行代码被称为reducer。reducer的输出就是程序的最终执行结果。MapReduce的优势在于，它使得程序以并行方式执行。如果集群由10个节点组成，而原先的作业需要10个小时来完成，那么应用MapReduce，该作业将在一个多小时之后得到同样的结果。 在任何时候，每个mapper或reducer之间都不进行通讯(这里指mapper各自之间不通信，reducer各自之间不通信，而reducer会接收mapper生成的数据)。每个节点只处理自己的事务，且在本地分配的数据集熵运算。 不同类型的作业可能需要不同数目的reducer。数据会以键值对的形式传递，sort阶段按照key把数据分类，之后合并。最终每个reducer就会收到相同的key值。reducer的数量并不是固定的。MapReduce的框架中海油其他一些灵活。MapReduce的整个编配工作由主节点(master node)控制。这些主节点控制整个MapReduce作业编配，包括每份数据存放的节点位置，以及map、sort和reduce等阶段的时序控制等。此外，主节点还要包含容错机制。一般地，每份mapper的输入数据会同时分发到多个节点形成多份副本，用于事务失效处理。一个MapReduce集群的示意图如下：如上MapReduce框架的示意图。在该集群中有3台双核机器，如果机器0失效，作业扔可以正常继续。图中每台机器都有两个处理器，可以同时处理两个map或者reduce任务。如果机器0在map阶段宕机，主节点在发现该问题之后，会将机器0移出集群，并在剩余的节点上继续执行作业。在一些MapReduce的实现中，在多个机器上都保存有数据的多个备份，例如在机器0上存放的输入数据可能还存放在机器1上，以防机器0出现问题。同时，每个节点都必须与主节点通信，表明自己工作正常。如果某个节点失效或者工作异常，主节点将重启该节点或者将该节点移出可用机器池。 MapReduce总结 主节点控制MapReduce的作业流程 Mapreduce的作业可用分成map任务和reduce任务 map任务之间不做数据交流，reduce任务也一样 在map和reduce阶段中间，有一个sort或combine阶段 数据被重复存放在不同的机器上，以防某个机器失效 mapper和reducer传输的数据形式为键值对]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数]]></title>
    <url>%2F2018%2F06%2F21%2FMachineLearning%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[激活函数定义神经网络中的每个节点接受输入值，并将输入值传递给下一层，输入节点会将输入属性值直接传递给下一层(隐层或输出层)。在神经网络中，隐层和输出层节点的输入和输出之间具有函数关系，这个函数称之为激活函数。 作用如果不适用激活函数，每一层输出都是上一层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机(Perceptron)。如果使用激活函数，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。 常用的激活函数Sigmoid函数Sigmoid函数是一个在生物学中常见的S型函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。公式如下：$$f(x)=\frac{1}{1+e^{-x}}$$函数图像如下： tanh函数tanh是上去西安函数中的一个，tanh()为双曲正切。在数学中，双曲正切tanh是由基本双曲函数双曲正弦和双曲余弦推导而来。公式如下：$$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{2}{1+e^{-2x}}-1=2sigmoid(2x)-1$$函数图像如下： softplus函数公式如下：$$f(x)=log(1+e^x)$$函数图像如下： softsign函数公式如下：$$f(x)=\frac{x}{|x|+1}$$函数图像如下： ReLU函数ReLU激活函数(Rectified Linear Unit)，线性整流函数，又称修正线性单元，用于隐层神经元输出。ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient。ReLU虽然简单，但是却是近几年的重要成果，有以下几大优点： 1.解决了gradient vanishing问题 (在正区间) 2.计算速度非常快，只需要判断输入是否大于0 3.收敛速度远快于sigmoid和tanh ReLU也有几个需要特别注意的问题： 1.ReLU的输出不是zero-centered 2.Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生：1.非常不幸的参数初始化，这种情况比较少见2.learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 公式如下：$$f(x)=max(0,x)$$函数图像如下： ELU函数Exponential Linear unit，指数线性单元，ELU函数是针对ReLU函数的一个改进型，相对于ReLU函数，在输入为复数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除ReLU死掉的问题，不过还是有梯度消失和指数运算的问题。公式如下：$$f(x)=\begin{cases}x,&amp; x&gt;0\\alpha(e^x-1),&amp; x \leq 0\end{cases} $$函数图像如下： LReLU函数即Leaky ReLU，泄漏整流线性单元，为了解决ReLU的死去问题，提出了将ReLU的前半段设为$\alpha{x}$而非0，通常$\alpha=0.01$。理论上来讲，LReLU函数有ReLU的所有优点，外加不会有ReLU死去问题，但是在实际操作当中，并没有完全证明LReLU总是优于ReLU。公式如下：$$f(x)=\begin{cases}x,&amp; x&gt;0\\alpha{x},&amp; x \leq 0\end{cases} $$其中$\alpha$是固定值，默认$\alpha=0.01$函数图像如下： PReLU函数另外一种解决ReLU死去问题的直观的想法是基于参数的方法，即Parametric ReLU函数，参数化修正线性单元。公式如下：$$f(x)=\begin{cases}x,&amp; x&gt;0\\alpha{x},&amp; x \leq 0\end{cases} $$其中$\alpha$是可以学习的。如果$\alpha=0$，那么PReLU退化为ReLU；如果$\alpha$是一个很小的固定值(如$\alpha=0.01$)，则PReLU退化为Leaky ReLU(LReLU)。PReLU只增加了极少量的参数，也就意味着网络的计算量以及过拟合的危险性都只增加了一点点。特别的，当不同通道使用相同的$\alpha$时候，参数就更少了。BP更新$\alpha$时，采用的是带动量的更新方式。 RReLU函数即Randomized Leaky ReLU函数，随机带泄露的修正线性单元，与Leaky ReLU以及PReLU很相似，为负责输入添加了一个线性项。而最关键的区别是，这个线性项的斜率在每一个节点上都是随机分配的(通常服从均匀分布)。 函数 优点 缺点 sigmoid函数 在整个定义域内可导。 梯度在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓收敛速度。计算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。 tanh函数 在整个定义域内可导。 梯度在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓收敛速度。计算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。 softplus函数 在整个定义域内可导。 梯度在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓收敛速度。计算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。 ReLU函数 ReLU的梯度在大多数情况下是常数，有助于解决深层网络收敛问题。ReLU更容易学习优化。因为其分段线性性质，导致其前传、后传、求导都是分段线性。ReLU会使一部分神经元的输出为0.这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生，也更接近真实的神经元激活模型。 如果后层的某一个梯度特别大，导致W更新以后变得特别大，导致该层的输入&lt;0，输出为0，这时该层就会死去，没有更新。当学习率比较大时可能会有40%的神经元都会在训练开始就会死去，因此需要对学习率进行一个好的设置。 注意： tanh特征相差明显时的效果，在循环过程中会不断扩大特征效果显示出来，但是有，在特征相差比较复杂或是相差不是特别大时，需要更细微的分类判断的时候，sigmoid效果就好了。 sigmoid和tanh作为激活函数时，需要注意对input进行归一化，否则激活后的值都会进入平坦区，使隐层的输出全部趋同，但是ReLU并不需要输入归一化来放置它们达到饱和。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Nerual Network</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[太仓房价热力图]]></title>
    <url>%2F2018%2F06%2F20%2FProject%2F%E5%A4%AA%E4%BB%93%E6%88%BF%E4%BB%B7%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[太仓房价热力图 ul,li{list-style: none;margin:0;padding:0;float:left;} html{height:100%} body{height:100%;margin:0px;padding:0px;font-family:"微软雅黑";} #Heatmap{height:70vh;width:100%;} #r-result{width:100%;position: relative;top: -68vh;left: 10px;} button{height: 30px;width:100px;font-size: 16px;border-radius: 6px} p{margin-left:5px; font-size:14px;} 显示热力图 关闭热力图 function loadJScript() { var script = document.createElement("script"); script.type = "text/javascript"; script.src = "https://api.map.baidu.com/api?v=2.0&ak=dTGv8eqlGerVY2gEkO2TyarYozkAKZpM&callback=init"; document.body.appendChild(script); } function init() { var Heatmap = new BMap.Map("Heatmap"); // 创建Map实例 var center_lng = 121.11707440329229; var center_lat = 31.45102211467977; var point = new BMap.Point(center_lng,center_lat); // 创建点坐标 Heatmap.centerAndZoom(point, 15); Heatmap.enableScrollWheelZoom(); //启用滚轮放大缩小 if(!isSupportCanvas()){ alert('热力图目前只支持有canvas支持的浏览器,您所使用的浏览器不能使用热力图功能~') } //详细的参数,可以查看heatmap.js的文档 https://github.com/pa7/heatmap.js/blob/master/README.md //参数说明如下: /* visible 热力图是否显示,默认为true * opacity 热力的透明度,1-100 * radius 势力图的每个点的半径大小 * gradient {JSON} 热力图的渐变区间 . gradient如下所示 * { .2:'rgb(0, 255, 255)', .5:'rgb(0, 110, 255)', .8:'rgb(100, 0, 255)' } 其中 key 表示插值的位置, 0~1. value 为颜色值. */ heatmapOverlay = new BMapLib.HeatmapOverlay({"radius":20}); Heatmap.addOverlay(heatmapOverlay); heatmapOverlay.setDataSet({data:constant,max:50000}); // 是否显示热力图 s = document.getElementById('show'); h = document.getElementById('hide'); // alert(h.innerHTML); flag = false; openHeatmap = function(){ if(flag){ heatmapOverlay.show(); s.style.display = 'none'; h.style.display = 'inline-block'; flag = false } else{ heatmapOverlay.hide(); h.style.display = 'none'; s.style.display = 'inline-block'; flag = true } }; openHeatmap(); function setGradient(){ /*格式如下所示: { 0:'rgb(102, 255, 0)', .5:'rgb(255, 170, 0)', 1:'rgb(255, 0, 0)' }*/ var gradient = {}; var colors = document.querySelectorAll("input[type='color']"); colors = [].slice.call(colors,0); colors.forEach(function(ele){ gradient[ele.getAttribute("data-key")] = ele.value; }); heatmapOverlay.setOptions({"gradient":gradient}); } // 判断浏览区是否支持canvas function isSupportCanvas(){ var elem = document.createElement('canvas'); return !!(elem.getContext && elem.getContext('2d')); } // 添加带有定位的导航控件 var navigationControl = new BMap.NavigationControl({ // 靠左上角位置 anchor: BMAP_ANCHOR_TOP_RIGHT, // LARGE类型 type: BMAP_NAVIGATION_CONTROL_LARGE, // 启用显示定位 enableGeolocation: true }); Heatmap.addControl(navigationControl); // 添加定位控件 var geolocationControl = new BMap.GeolocationControl({anchor:BMAP_ANCHOR_BOTTOM_RIGHT}); geolocationControl.addEventListener("locationSuccess", function(e){ // 定位成功事件 var address = ''; address += e.addressComponent.province; address += e.addressComponent.city; address += e.addressComponent.district; address += e.addressComponent.street; address += e.addressComponent.streetNumber; alert("当前定位地址为：" + address); }); geolocationControl.addEventListener("locationError",function(e){ // 定位失败事件 alert(e.message); }); Heatmap.addControl(geolocationControl); } // 异步加载地图 window.onload = loadJScript;]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>html/css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP算法广义误差]]></title>
    <url>%2F2018%2F06%2F15%2FMachineLearning%2FBP%E7%AE%97%E6%B3%95%E5%B9%BF%E4%B9%89%E8%AF%AF%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[BP算法广义误差$二次方误差和E = \frac{1}{2}\sum\limits_{j=1}^{n_l}(期望输出 - 实际输出)^2$$$E = \frac{1}{2}\sum_{j=1}^{n_l} (y_j-a_j)^2$$将误差函数向量化$$E = \frac{1}{2}||y-a^{(n_l)}||^2$$将误差函数进行化简$$\begin{split}E &amp;=\frac{1}{2}||y-a^{(n_l)}||^2\\&amp;=\frac{1}{2}(y-a)^T(y-a)\\&amp;=\frac{1}{2}(y^Ty-y^Ta-a^Ty+a^Ta)\end{split}$$首先给出定义：$a\in\mathbb{R}^{n\times{1}}$ $y\in\mathbb{R}^{n\times{1}}$ $\delta\in\mathbb{R}^{n\times{1}}$则输出层广义误差为：$$\begin{split}\delta^{(n_l)} &amp;= \frac{\partial{E^{(n_l)}}}{\partial{z^{(n_l)}}} \\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}\frac{\partial{E^{(n_l)}}}{\partial{a^{(n_l)}}} \\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}\frac{\partial}{\partial{a^{(n_l)}}}[\frac{1}{2}(y^Ty-y^Ta-a^Ty+a^Ta)] \\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}[\frac{1}{2}(0-y-y+2a^{(n_l)})] \\&amp;=\frac{\partial{g(z^{(n_l)})}}{\partial{z^{(n_l)}}}(a^{(n_l)}-y)\end{split}$$ 隐含层广义误差为：$$\begin{split}\delta^{(n_l-1)}&amp;=\frac{\partial{E^{(n_l)}}}{\partial{z^{(n_l-1)}}}\\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l-1)}}}\frac{\partial{E^{(n_l)}}}{\partial{a^{(n_l)}}}\\&amp;=\frac{\partial{z^{(n_l)}}}{\partial{z^{(n_l-1)}}}\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}(a^{(n_l)}-y)\\&amp;=\frac{\partial{z^{(n_l)}}}{\partial{z^{(n_l-1)}}}\delta^{(n_l)}\\&amp;=\frac{\partial{\Theta^{(n_l-1)}g(z^{(n_l-1)})}}{\partial{z^{(n_l-1)}}}\delta^{(n_l)}\\&amp;=\frac{\partial{g(z^{(n_l-1)})}}{\partial{z^{(n_l-1)}}}(\Theta^{(n_l-1)})^T\delta^{(n_l)}\\\end{split}$$$a^{(n_l)}=h_{\Theta}(x)=g(z^{(n_l)})$由于sigmoid标量导数$g’(z)=g(z)[1-g(z)]$，固在向量化后$g’(z^{(n_l)})=a^{(n_l)}*(1-a^{(n_l)})$，所以可得隐含层广义误差与下一层广义误差的关系为：$$\delta^{(n_l-1)}=(\Theta^{(n_l-1)})^T\delta^{(n_l)}*{g’(z^{(n_l-1)})} $$从而实现BP算法]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Nerual Network</tag>
        <tag>Backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同方差性与异方差性]]></title>
    <url>%2F2018%2F06%2F10%2FMachineLearning%2F%E5%90%8C%E6%96%B9%E5%B7%AE%E6%80%A7%E4%B8%8E%E5%BC%82%E6%96%B9%E5%B7%AE%E6%80%A7%2F</url>
    <content type="text"><![CDATA[同方差性与异方差性所谓同方差，是为了保证回归参数估计量具有良好的统计性质，经典线性回归模型的一个重要假定，总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。 若线性回归模型存在异方差性，则用传统的最小二乘法(OLS)估计模型，得到的参数估计量不是有效估计量，甚至也不是渐进有效哦的估计量；此时也无法对模型参数进行有关显著性校验。 异方差的检测事实证明，实际问题中经常会出现异方差性，这将影响回归模型的估计、检验和应用。因此在建立回归模型时应检验模型是否存在异方差性。异方差性检验方法如下: 1.图示校验法 2.Goldfeld-Quandt校验 3.White校验发 4.Park校验法 5.Gleiser校验法 异方差破坏古典模型的基本假定在古典回归模型的假定下，普通最小二乘法估计量是线性、无偏、有效估计量，即在所有无偏估计量中，最小二乘法估计量具有最小方差性——它是个有效估计量。如果在其他假定不变的条件下，允许随机扰动项ui存在异方差性，即ui的方差随观测值的变化而变化，这就违背了最小二乘法估计的高斯——马尔柯夫假设，这时如果继续使用最小二乘法对参数进行估计，就会产生以下后果： 1.参数估计量仍然是线性无偏的，但不是有效的； 2.异方差模型中的方差不再具有最小方差性； 3.T检验失去作用； 4.模型的预测作用遭到破坏。 T检验，亦称student t检验(Student’s t test)，主要用户样本含量较小(例如n&lt;30)总体标准差$\sigma$未知的正态分布。T检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。它与F检验、卡方检验并列。T检验是戈斯特为了观测酿酒质量而发明的，并于1908年在Biometrika上发布。 存在异方差性解决方法: 对模型变换，当可以确定异方差的具体形式时，将模型作适当变换有可能消除或减轻异方差的影响。 使用加权最小二乘法，对原模型变换的方法与加权二乘法实际上是等价的，可以消除异方差。 对数变换，运用对数变换能使测定变量值的尺度缩小。它可以将两个数值之间原来10倍的差异缩小到只有2倍的差异。其次，经过对数变换后的线性模型，其残差表示相对误差，而相对误差往往比绝对误差有较小的差异。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>方差</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归的梯度下降算法]]></title>
    <url>%2F2018%2F06%2F08%2FMachineLearning%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[逻辑回归的梯度下降算法 本文阐述逻辑回归代价函数的梯度下降算法推导过程，为满足广义性，采用多变量的逻辑回归代价函数进行推导。 首先给出逻辑回归的代价函数(Cost Function)(即交叉熵)：$$J(\theta) = \frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$$其中假设函数$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx^{(i)}}}$，$m$为样本总数，参数$\theta$为$n+1$维列向量。由于使用多变量进行梯度下降，固可以使用批量梯度下降法来获得全局最优解。推导过程如下，首先化简代价函数：$$\begin{split}J(\theta) &amp;= -\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]\\&amp;= -\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log(\frac{1}{1+e^{-\theta^Tx^{(i)}}})-(1-y^{(i)})log(1-\frac{1}{1+e^{-\theta^Tx^{(i)}}})]\\&amp;= -\frac{1}{m}\sum\limits_{i=1}^m[-y^{(i)}log(1+e^{-\theta^Tx^{(i)}})-(1-y^{(i)})log(1+e^{\theta^Tx^{(i)}})]\\\end{split}$$对$\theta_j$求偏导:$$\begin{split}\frac{\partial}{\partial\theta_j}J(\theta)&amp;=-\frac{\partial}{\partial\theta_j}\frac{1}{m}\sum\limits_{i=1}^m[-y^{(i)}log(1+e^{-\theta^Tx^{(i)}})-(1-y^{(i)})log(1+e^{\theta^Tx^{(i)}})]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[-y^{(i)}\frac{-x_j^{(i)}e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}-(1-y^{(i)})\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{x_j^{(i)}y^{(i)}e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}-\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}+\frac{x_j^{(i)}y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{x_j^{(i)}y^{(i)}e^{-\theta^Tx^{(i)}}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}-\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}+\frac{x_j^{(i)}y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{x_j^{(i)}y^{(i)}-x_j^{(i)}e^{\theta^Tx^{(i)}}+x_j^{(i)}y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{y^{(i)}-e^{\theta^Tx^{(i)}}+y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{y^{(i)}(1+e^{\theta^Tx^{(i)}})-e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}-\frac{e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}-\frac{1}{1+e^{-\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}-h_\theta(x^{(i)})]x_j^{(i)}\\&amp;=\frac{1}{m}\sum\limits_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}\\\end{split}$$在参数$\theta$中引入学习速率$\alpha$：$$\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta),(j=0,1,…,n)$$将$\frac{\partial}{\partial\theta_j}J(\theta)$代入，得出逻辑回归的批量梯度下降算法：$$\theta_j=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m[(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}],(j=0,1,…,n)$$将上述公式进行向量化：$\Theta\in\mathbb{R}^{n+1\times{1}}$ $X\in\mathbb{R}^{m\times{n+1}}$ $y\in\mathbb{R}^{m\times{1}}$则批量梯度下降算法可表示为：$$\Theta=\Theta-\frac{\alpha}{m}X^{T}(X\Theta-y)$$向量化后，可以使计算更简洁，并且也能保证各$\theta$的值保持同步更新，以下是代码实现，其中数据来源于斯坦福Andrew Ng公开课：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import numpy as npimport pandas as pdimport matplotlib.pylab as pltdef Sigmoid(z): return 1 / (1 + np.exp(-z))def h(X, theta): return Sigmoid(X @ theta)def CostFunction(X, y, theta): return -np.mean(np.multiply(y, np.log(h(X, theta))) + np.multiply(1 - y, np.log(1 - h(X, theta))))def Gradient(X, y, theta, alpha): return theta - alpha * X.T @ (h(X, theta) - y) / X.shape[0]def BatchGradientDecent(X, y, theta, alpha=0.01, iters=500000): cost = np.zeros(iters) for i in range(iters): theta = Gradient(X, y, theta, alpha) cost[i] = CostFunction(X, y, theta) return theta, costdef predict(theta, X): return [1 if x &gt;= 0.5 else 0 for x in h(X, theta)]if __name__ == '__main__': path = 'ex2data1.txt' data = pd.read_csv(path, header=None, names=['A', 'B', 'Res']) data.insert(0, 'x0', 1) cols = data.shape[1] X = data.iloc[:, :cols - 1] y = data.iloc[:, cols - 1:cols] X = np.mat(X.values) y = np.mat(y.values) positive = data[data['Res'].isin(['1'])] negative = data[data['Res'].isin(['0'])] theta = np.mat(np.zeros(3)).T # 学习率alpha为0.01时过大，导致CostFunction不收敛 alpha = 0.001 theta, cost = BatchGradientDecent(X, y, theta, alpha) print(theta) # print(cost) # theta = [[-25.16131863], [0.20623159], [0.20147149]] predictions = predict(theta, X) correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)] accuracy = (sum(map(int, correct)) % len(correct)) print('accuracy = &#123;0&#125;%'.format(accuracy)) fig, ax = plt.subplots(figsize=(12, 8)) ax.scatter(positive['A'], positive['B'], marker='o', label='P') ax.scatter(negative['A'], negative['B'], marker='x', label='N') # 显示图例 ax.legend() ax.set_xlabel('A') ax.set_ylabel('B') x = np.linspace(data.A.min(), data.A.max(), 100) y = -(theta[0, 0] + theta[1, 0] * x) / theta[2, 0] ax.plot(x, y, 'r') plt.show()]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息论与熵]]></title>
    <url>%2F2018%2F06%2F07%2FMachineLearning%2F%E4%BF%A1%E6%81%AF%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[信息论与熵香农信息量 只考虑连续型随机变量的情况。设p为随机变量X的概率分布，即p(x)为随机变量X在X=x处的概率密度函数值，随机变量X在x处的香农信息量定义为：$$ -logp(x) = log\frac{1}{p(x)}$$香农信息量用于刻画消除随机变量X在x处的不确定性所需的信息量的大小。可以近似地将不确定性视为信息量。即一个消息带来的不确定性大，就是带来的信息量大。 必定——信息量为0 高确定性——低信息量 高不确定性——高信息量 自信息用来衡量单一事件发生时所包含的信息量多寡。 互信息是点间互信息的期望值，是度量两个时间集合之间的相关性。两个离散随机变量X和Y的互信息可以定义为：$$I(X;Y)= \sum\limits_{y\in Y}\sum\limits_{x\in X}p(x,y)log\Big(\frac{p(x,y)}{p(x)p(y)}\Big)$$在连续随机变量的情形下，求和被替换成了二重定积分：$$I(X;Y) = \int_Y\int_Xp(x,y)log\Big(\frac{p(x,y)}{p(x)p(y)}\Big)dxdy$$其中$p(x,y)$是X和Y的联合概率分布函数，而$p(x)$和$p(y)$分别是X和Y的边缘概率分布函数。 熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策数据学习中的信息增益等价于训练数据集中类与特征的互信息。 熵/香农熵/信息熵熵是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。信息熵$H(p)$是香农信息量-log$p(x)$的数学期望，即所有X=x处的香农信息量的和，由于每一个x的出现概率不一样(用概率密度函数值$p(x)$衡量)，需要用$p(x)$加权求和。因此信息熵是用于刻画消除随机变量X的不确定性所需要的总体信息量的大小，其定义如下：$$ H(p)= H(X) = E_{x\to p(x)}[-logp(x)] = -\int{p(x)logp(x)dx}$$概率越大的时间，信息熵反而越小，哪些接近确定性的分布，香农熵比较低，而越是接近平均分布的，香农熵比较高。这个和发生概率越低的事情信息量越大的基本思想是一致的。从这个角度看，信息可以看做是不确定性的衡量，而信息熵就是对这种不确定性的数学描述。信息熵不仅定量衡量了信息的大小，并且为信息编码提供了理论上的最优值：使用的编码平均码长度的理论下界就是信息熵。或者说，信息熵就是数据压缩的极限。 微分熵当随机变量x是连续的，香农熵就被称为微分熵。 相对熵又称KL散度，信息散度，记为DKL(P||Q)。它度量当真实分布为p时，假设分布q的无效性。有人将KL散度称为KL距离，但事实上，KL散度并不满足距离的概念，因为：(1)KL散度不是对称的；(2)KL散度不满足三角不等式。设P(x)和Q(x)是X取值的两个离散概率分布，则P对Q的相对熵为：$$D(P||Q) = \sum{P(x)log(\frac{P(x)}{Q(x)})}$$对于连续的随机变量，定义为：$$D(P||Q) = \int{P(x)log(\frac{P(x)}{Q(x)})dx}$$ 交叉熵交叉熵主要用于度量两个分布间的差异性信息。语言模型的性能通常用交叉熵和复杂度来衡量。交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。对KL散度进行变形可以得到d：$$\begin{split}D(P||Q) &amp;= \sum{P(x)log(\frac{P(x)}{Q(x)})dx}\\&amp;= \sum{P(x)log(P(x))}-\sum{P(x)log(Q(x))}\\&amp;= -H(p(x))+[-\sum{P(x)log(Q(x))]}\end{split}$$交叉熵的公式定义如下：$$H(P,Q) = -\sum{P(x)log(Q(x))}$$由于KL散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>信息论</tag>
        <tag>Entropy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归的正规方程]]></title>
    <url>%2F2018%2F06%2F06%2FMachineLearning%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[线性回归的正规方程 本文阐述线性回归的正规方程推导过程，为满足广义性，采用多变量的线性回归代价函数进行推导。 多变量线性回归的梯度下降算法是用来求其代价函数最小值的算法，但是对于某些线性回归问题，可以直接使用正规方程的方法来找出使得代价函数最小的参数，即$\frac{\partial}{\partial\theta_j}J(\theta)=0$。梯度下降与正规方程的比较： 优缺点 梯度下降 正规方程(标准方程) 是否需要引入其他参数 需要选择学习率$\alpha$ 不需要 迭代或运算次数 需要多次迭代 一次运算得出 特征数量是否有影响 当特征数量$n$大时也能较好适用 需要计算$(X^TX)^{-1}$如果特征数量$n$较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说$n$小于10000时还是可以接受的 适用模型类 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 首先给出线性回归的代价函数(Cost Function)的向量化表示：$$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$其中假设函数$h_\theta(x) = \theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$m$为样本总数，参数$\theta$与特征矩阵$X$均为$n+1$维列向量。 将假设函数代入，并将向量表达式转化为矩阵表达式，即将$\sum\limits_{i=1}^m$写成矩阵相乘的形式：$$J(\theta) = \frac{1}{2}(X\theta-y)^2$$其中$X$为$m$行$n+1$列的矩阵，$m$为样本个数，$n+1$为特征个数，$\theta$为$n+1$维行向量，$y$为$m$维行向量。由于$X$非方阵，不存在逆矩阵，固对$J(\theta)$进行如下变换： $$\begin{split}J(\theta) &amp; = \frac{1} {2} (X\theta-y)^T(X\theta-y) \\&amp;= \frac{1}{2}[(X\theta)^T-y^T] (X\theta-y) \\&amp;= \frac{1}{2}(\theta^TX^T-y^T) (X\theta-y) \\&amp;= \frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty)\end{split}$$ 接下来对$J(\theta)$求偏导，需要用到以下几个矩阵对矩阵的分母布局求导法则：①$\frac{dAX}{dX}=A^T$②$\frac{dX^TAX}{dX}=2AX$③$\frac{dX^TA}{dX}=A$ 首先化简$\frac{\partial}{\partial\theta}J(\theta)$$$\begin{split}\frac{\partial}{\partial\theta}J(\theta)&amp;=\frac{1}{2}[2X^TX\theta-X^Ty-(y^TX)^T+0]\\&amp;=\frac{1}{2}[2X^TX\theta-X^Ty-X^Ty+0]\\&amp;=X^TX\theta-X^Ty\end{split}$$ 再令$\frac{\partial}{\partial\theta}J(\theta)=X^TX\theta-X^Ty=0$$$\begin{split}X^TX\theta-X^Ty&amp;=0\X^TX\theta&amp;=X^Ty\end{split}$$ 不难发现，$(X^TX)$为方阵，则有$(X^TX)$的逆矩阵$(X^TX)^{-1}$，固在等式两边同时左乘$(X^TX)^{-1}$，并求出$\theta$$$\begin{split}(X^TX)^{-1}X^TX\theta&amp;=(X^TX)^{-1}X^Ty\\(X^TX)^{-1}(X^TX)\theta&amp;=(X^TX)^{-1}X^Ty\\E\theta&amp;=(X^TX)^{-1}X^Ty\\\theta&amp;=(X^TX)^{-1}X^Ty\end{split}$$至此，完成线性回归的正规方程推导，代码实现如下，其中数据来源于斯坦福Andrew Ng公开课：1234567891011121314151617181920212223242526272829303132import numpy as npimport pandas as pdimport matplotlib.pylab as pltdef NormalEquation(X,y):return ((X.T@X).I)@X.T@yif __name__ == '__main__':path = 'ex1data1.txt'data = pd.read_csv(path,header=None,names=['Population','Profit'])# insert the column of x0data.insert(0,'Ones',1)# get the matrix of X and the matrix of yx = data.iloc[:,:-1]y = data.iloc[:,-1:]X = np.mat(x.values)y = np.mat(y.values)theta = NormalEquation(X,y)x = np.linspace(data.Population.min(),data.Population.max(),100)f = theta[0,0] + theta[1,0] * x# display the resultfig,ax = plt.subplots(2,1,figsize=(8,12))ax[0].scatter(data.Population,data.Profit)ax[0].set_xlabel('Population')ax[0].set_ylabel('Profit')ax[0].set_title('Training Data')ax[1].plot(x,f,'r')ax[1].scatter(data.Population,data.Profit)ax[1].set_xlabel('Population')ax[1].set_ylabel('Profit')ax[1].set_title('Prediction Profit vs. Population Size')plt.show() 输出结果如下：]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>正规方程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归的梯度下降算法]]></title>
    <url>%2F2018%2F06%2F04%2FMachineLearning%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[线性回归的梯度下降算法 本文阐述线性回归代价函数的梯度下降算法推导过程，为满足广义性，采用多变量的线性回归代价函数进行推导。 首先给出线性回归的代价函数(Cost Function)：$$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$其中假设函数$h_\theta(x) = \theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$m$为样本总数，参数$\theta$与特征矩阵$X$均为$n+1$维列向量。由于使用多变量进行梯度下降，固可以使用批量梯度下降法来获得全局最优解。推导过程如下，首先将代价函数对$\theta_j$求偏导，得到$\frac{\partial}{\partial\theta_j}J(\theta)$：$$\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum\limits_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}$$在参数$\theta$中引入学习速率$\alpha$：$$\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta),(j=0,1,…,n)$$将$\frac{\partial}{\partial\theta_j}J(\theta)$代入，得出多变量线性回归的批量梯度下降算法：$$\theta_j=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m[(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}],(j=0,1,…,n)$$将上述公式进行向量化：$\Theta\in\mathbb{R}^{n+1\times{1}}$ $X\in\mathbb{R}^{m\times{n+1}}$ $y\in\mathbb{R}^{m\times{1}}$则批量梯度下降算法可表示为：$$\Theta=\Theta-\frac{\alpha}{m}X^{T}(X\Theta-y)$$向量化后，可以使计算更简洁，并且也能保证各$\theta$的值保持同步更新，以下是代码实现，其中数据来源于斯坦福Andrew Ng公开课：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport pandas as pdimport matplotlib.pylab as pltdef CostFunction(X, y, theta):return np.mean(np.power(X @ theta - y, 2)) / 2def Gradient(X, y, theta, alpha):return theta - alpha * (X.T @ (X @ theta - y)) / X.shape[0]def BatchGradientDecent(X, y, theta, alpha=0.01, iters=1000):cost = np.zeros(iters)for i in range(iters):theta = Gradient(X, y, theta, alpha)cost[i] = CostFunction(X, y, theta)return theta, costif __name__ == '__main__':path = 'ex1data1.txt'data = pd.read_csv(path,header=None,names=['Population','Profit'])# insert the column of x0data.insert(0,'Ones',1)# get the matrix of X and the matrix of yX = data.iloc[:,:-1]y = data.iloc[:,-1:]X = np.mat(X.values)y = np.mat(y.values)# init theta,alpha,iterstheta = np.mat([[0,0]])alpha = 0.01iters = 1000# batch gradientdecenttheta,cost = BatchGradientDecent(X,y,theta,alpha,iters)x = np.linspace(data.Population.min(),data.Population.max(),100)f = theta[0,0] + theta[0,1] * x# display the resultfig,ax = plt.subplots(2,1,figsize=(8,12))ax[0].scatter(data.Population,data.Profit)ax[0].set_xlabel('Population')ax[0].set_ylabel('Profit')ax[0].set_title('Training Data')ax[1].plot(x,f,'r')ax[1].scatter(data.Population,data.Profit)ax[1].set_xlabel('Population')ax[1].set_ylabel('Profit')ax[1].set_title('Prediction Profit vs. Population Size ')plt.show() 输出结果如下：]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[积分微分表]]></title>
    <url>%2F2018%2F06%2F02%2FUtils%2F%E7%A7%AF%E5%88%86%E5%BE%AE%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[积分微分表基本积分表24个基本积分：两个由基本积分②推导的常用积分 基本微分表 矩阵微分表①$\frac{dAX}{dX}=A^T$②$\frac{dX^TAX}{dX}=2AX$③$\frac{dX^TA}{dX}=A$]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Utils</tag>
        <tag>微积分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法(Gradient Descent)算法]]></title>
    <url>%2F2018%2F06%2F02%2FMachineLearning%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D(Gradient%20Descent)%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度下降(Gradient Descent)算法梯度下降梯度下降是一个用来求函数最小值的算法，是迭代法的一种，可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法(Stochastic Gradient Descent，简称SGD)和批量梯度下降法(Batch Gradient Descent，简称BGD)。随机梯度下降：随机梯度下降是每次迭代使用一个样本来对参数进行更新，其计算速度较快，但由于计算得到的并不是准确的一个梯度，即准确度较低，且容易陷入到局部最优解中，也不易于并行实现。批量梯度下降：批量梯度下降是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新(这里的更新指同步更新)。相对的，批量梯度下降在样本数据较多的情况下，其计算速度较慢，但是可以获得全局最优解，且易于并行实现。 &amp;nbsp Batch mini-Batch SGD 样本数 m个(所有)样本 n个(固定大小)样本，一般取$2^n$与计算机信息存储方式相适应 一个样本(Mini-Batch=1) 优点 相对噪声较低，代价函数总是向减小方向下降 训练速度快，效果较好 训练速度快 缺点 迭代速度慢，训练过程慢 丢失了向量化带来的计算加速(并行运算)，噪声较多 丢失了向量化带来的计算加速(并行运算)，有很多噪声，需要适当减少学习率，代价函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动，容易陷入局部最优(鞍点问题) 指数加权平均指数加权平均（Exponentially Weight Average）是一种常用的序列数据处理方式，通常用在序列场景如金融序列分析、温度变化序列分析。$$S_t=\begin{cases}Y_1,&amp;t =1\\\beta S_{t-1}+(1-\beta)T_t,&amp;t&gt;1\end{cases}$$ 指数移动平均值(偏差修正) $\beta$参数越大，曲线越平缓 $\beta$参数越小，曲线越曲折 动量梯度下降法动量梯度下降（Gradient Descent with Momentum）是计算梯度的指数加权平均数，并利用该值来更新参数值。$$S_{dW^{(l)}} = \beta S_{dW^{(l)}}+(1-\beta)dW^{(l)}$$$$S_{db^{(l)}} = \beta S_{db^{(l)}}+(1-\beta)db^{(l)}$$$$W^{(l)} := W^{(l)}-\alpha S_{dW^{(l)}}$$$$b^{(l)} := b^{(l)}-\alpha S_{db^{(l)}}$$ 使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色或紫色的曲线。当前后梯度一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够一致震荡。 RMSProp算法RMSProp（Root Mean Square Prop）算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。$$S_{dW^{(l)}} = \beta S_{dW^{(l)}}+(1-\beta)(dW^{(l)})^2$$$$S_{db^{(l)}} = \beta S_{db^{(l)}}+(1-\beta)(db^{(l)})^2$$$$W^{(l)} := W^{(l)}-\alpha\frac{dW^{(l)}}{\sqrt{S_{dW^{(l)}}+\epsilon}}$$$$b^{(l)} := b^{(l)}-\alpha\frac{db^{(l)}}{\sqrt{S_{db^{(l)}}+\epsilon}}$$其中$\epsilon$是一个非常小的数，防止分母太小导致不稳定,当$dW^{(l)}$或$db^{(l)}$较大时，对应的$(dW^{(l)})^2$、$(db^{(l)})^2$也会较大，进而$S_{dW^{(l)}}$也会较大，最终使得$\frac{dW^{(l)}}{\sqrt{S_{dW^{(l)}}+\epsilon}}$等结果变得非常小。最终RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。 Adam算法Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）将 Momentum 和 RMSProp 算法结合在一起。$$V_{dW^{(l)}} = \beta_1V_{dW^{(l)}}+(1-\beta_1)dW^{(l)}$$$$V_{db^{(l)}} = \beta_1V_{db^{(l)}}+(1-\beta_1)db^{(l)}$$$$V_{dW^{(l)}}^{corrected}=\frac{V_{dW^{(l)}}}{1-(\beta_1)^t}$$$$S_{dW^{(l)}} = \beta_2S_{dW^{(l)}}+(1-\beta_2)(dW^{(l)})^2$$$$S_{db^{(l)}} = \beta_2S_{db^{(l)}}+(1-\beta_2)(db^{(l)})^2$$$$S_{dW^{(l)}}^{corrected}=\frac{V_{dW^{(l)}}}{1-(\beta_2)^t}$$其中l为层，t为移动平均次数$$W^{(l)} := W^{(l)}-\alpha\frac{V_{dW^{(l)}}^{corrected}}{\sqrt{S_{dW^{(l)}}^{corrected}+\epsilon}}$$$$b^{(l)} := b^{(l)}-\alpha\frac{V_{db^{(l)}}^{corrected}}{\sqrt{S_{db^{(l)}}^{corrected}+\epsilon}}$$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[点到超平面距离]]></title>
    <url>%2F2018%2F05%2F11%2FStatisticalLearning%2F%E7%82%B9%E5%88%B0%E8%B6%85%E5%B9%B3%E9%9D%A2%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[点到超平面距离首先给出以下定义：输入空间(特征空间) $X\subseteq\mathbb{R}^n$输出空间 $Y= \lbrace +1,-1 \rbrace $特征向量 $x\in{X}$类别 $y\in{Y}$权值(权值向量) $w\in\mathbb{R}^n$偏置 $b\in\mathbb{R}$给出线性方程：$$w\cdot{x}+b=0$$此线性方程对应于特征空间$\mathbb{R}^n$中的一个超平面S，其中$w$是超平面的法向量，$b$是超平面的截距，$w\cdot{x}$表示$w$和$x$的内积。点到超平面距离推导过程如下。 首先给出如下例图：其中，某个超平面S$$w\cdot{x}+b=0$$$w$是超平面的法向量，方向垂直超平面向上。A为平面外一点且坐标为$x_0$，B为超平面S内一点且坐标为$x_1$，$|\vec{AO}|=d$为点A到超平面垂直距离。根据勾股定理，易推得：$$d=|\vec{AB}|cos\angle{OAB}$$根据向量内积进行转化：$$\begin{split}d&amp;=|\vec{AB}|cos\angle{OAB}\\&amp;=||\vec{AB}||\frac{\vec{AB}\cdot\vec{AO}}{|\vec{AB}||\vec{AO}|}\\&amp;=\frac{\vec{AB}\cdot\vec{AO}}{|\vec{AO}|}\end{split}$$由于$|\vec{AO}|$为点A到超平面垂直距离，易证$\vec{AO}//w$，则有：$$\frac{\vec{AO}}{|\vec{AO}|}=-\frac{w}{|w|}$$且$\vec{AB}=x_1-x_0$，可得：$$\begin{split}d&amp;=-\frac{w\cdot{(x_1-x_0)}}{|w|}\\&amp;=\frac{w\cdot{x_0}-w\cdot{x_1}}{|w|}\end{split}$$此时利用条件“B为超平面S内的一点且坐标为$x_1$”，故有$w\cdot{x_1}+b=0$：此时完成距离$d$的推导：$$d=\frac{w\cdot{x_0}+b}{|w|}$$]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>超平面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归问题]]></title>
    <url>%2F2018%2F05%2F10%2FStatisticalLearning%2F%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[回归问题回归(regression)是监督学习的另一个重要问题。回归用于预测输入变量(自变量)和输出变量(因变量)之间的关系，特别是当输入变量的值发生变化时，输出变量的值岁=随之发生的变化。回归模型正是表示从输入变量到输出变量之间映射的函数。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。回归问题分为学习和预测两个过程，如下图。首先给定一个训练数据集：$$T=\lbrace(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\rbrace$$这里，$x_i\in{R^n}$是输入，$y\in{R}$是对应的输出，$i=1,2,\cdots,N$。学习系统基于训练数据构建一个模型，即函数$Y=f(X)$；对新的输入$x_{N+1}$，预测系统根据学习的模型$Y=f(X)$确定相应的输出$y_{N+1}$。回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型及模型的类型，分为线性回归和非线性回归。回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法(least squares)求解。许多领域的任务都可以形式化为回归问题，比如，回归可以用于商务领域，作为市场趋势预测、产品质量管理、客户满意度调查、投资风险分析的工具。作为例子，简单介绍股价预测问题。假设知道某一公司在过去不同时间点(比如，每天)的市场上的股票价格(比如，股票平均价格)，以及在各个时间点之前可能影响该公司股价的信息(比如，该公司前一周的营业额、利润)。目标是从过去的数据学习一个模型，使它可以基于当前的信息预测该公司下一个时间点的股票价格。可以将这个问题作为回归问题解决。具体地，将影响股价的信息视为自变量(输入的特征)，而将股价视为因变量(输出的值)。将过去的数据作为训练数据，就可以学习一个回归模型，并对未来的股价进行预测。可以看出这是一个困难的预测问题，因为影响股价的因素非常多，我们未必能判断到哪些信息(输入的特征)有用并能得到这些信息。]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>回归问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[标注问题]]></title>
    <url>%2F2018%2F05%2F10%2FStatisticalLearning%2F%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[标注问题标注(tagging)也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题有时更负责的结构预测(structure prediction)问题的简单形式。标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级别增长的。标注问题分为学习和标注两个过程(如下图所示)。首先给定一个训练数据集：$$T=\lbrace(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\rbrace$$这里，$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,i=1,2,\cdots,N$，是输入观测序列，$y_i=(y_i^{(1)},y_i^{(2)},\cdots,y_i^{(n)})^T,i=1,2,\cdots,N$，是响应的输出标记序列，$n$是序列的长度，对不同样本可以有不同的值。学习系统基于训练数据集构建一个模型，表示为条件概率分布：$$P(Y^{(1)},Y^{(2)},\cdots,Y^{(n)}|X^{(1)},X^{(2)},\cdots,X^{(n)})$$这里，每一个$X^{(i)}(i=1,2,\cdots,n)$取值为所有可能的观测，每一个$Y^{(i)}(i=1,2,\cdots,n)$取值为所有坑你的标记，一般$n&lt;&lt;N$。标注系统按照学习得到的条件概率分布模型，对新的输入观测序列找到相应的输出标记序列。具体地，对一个观测序列$x_{N+1}=(x_{N+1}^{(1)},x_{N+1}^{(2)},\cdots,x_{N+1}^{(n)})^T$找到使条件概率$P((y_{N+1}^{(1)},y_{N+1}^{(2)},\cdots,y_{N+1}^{(n)})^T|(x_{N+1}^{(1)},x_{N+1}^{(2)},\cdots,x_{N+1}^{(n)})^T)$最大标记序列$y_{N+1}=(y_{N+1}^{(1)},y_{N+1}^{(2)},\cdots,y_{N+1}^{(n)})^T$。评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、青雀率和召回率。其定义与分类模型相同。标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。标注问题在信息抽取、自然语言处理等领域被广泛应用，是这些领域的基本问题。例如，自然语言处理中的词性标注(part of speech tagging)就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标标记序列。举一个信息抽取的例子。从英文文章中抽取基本名词短语(base noun phrase)。为此，要对文章进行标注。英文单词是一个观测，英文句子是一个观测序列，标记表示名词短语的“开始”、“结束”或“其他”(分别以B，E，O表示)，标记序列表示英文句子中基本名词短语的所在位置。信息抽取时，将标记“开始”到标记“结束”的单词作为名词短语。列入，给出以下的观测序列，即英文句子，标注系统产生相应的标记序列，即给出句子中的基本名词短语。输入：At Microsoft Research, we have an insatiable curiosity and the desire to create new technology that will help define the computing experience.输出：At/O Microsoft/B Research/E, we/O have/O an/O insatiable/B curiosity/E and/O the/O desire/BE to/O create/O new/B technology/E that/O will/O help/O define/O the/O computing/B experience/E.]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>标注问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类问题]]></title>
    <url>%2F2018%2F05%2F08%2FStatisticalLearning%2F%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[分类问题分类是监督学习的一个核心问题。在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便成为分类问题。这时，输入变量$X$可以使离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，成为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification)。可能的输出称为类(class)。分类的类别为多个时，称为多类分类问题。分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。分类问题可用下图描述。学习系统由训练数据$(x^{(i)},y^{(i)})$学习一个分类器$P(Y|X)$或$Y=f(X)$；分类系统通过学到的分类器$P(Y|X)$或$Y=f(X)$对于新的输入实例$x_{N+1}$进行分类，即预测其输出的类标记$y_{N+1}$。评价分类器性能的指标一般是分类准确率(accuracy)，其定义是：对于给定的测试数据集，分类器正确份额类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率：$$r_{rest}=\frac{1}{N’}\sum\limits_{i=1}^{N’}I(y_i=\hat{f}(x_i)$$误差率(error rate)为：$$e_{rest}=\frac{1}{N’}\sum\limits_{i=1}^{N’}I(y_i\neq\hat{f}(x_i)$$则显然有：$$r_{rest}+e_{test}=1$$对于二类分类问题常用的评价指标是精确率(precision,查准率)与召回率(recall,查全率)。通常以关注的类(一般为出现较少的类)为正类(y=1)，其他为负类(y=0)，分类器在测试数据集熵的预测或正确或不正确，4种情况出现的总数分别记作： TP——将正类预测为正类数 FN——将正类预测为负类数 FP——将负类预测为正类数 TN——将负类预测为负类数精确率(查准率)定义为：$$P=\frac{TP}{TP+FP}$$召回率(查全率)定义为：$$P=\frac{TP}{TP+FN}$$ 此外，还有$F_1$值，是精确率与召回率的调和均值，即$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$$$F_1=\frac{2TP}{2TP+FP+FN}$$精确率和召回率都高时，$F_1$值也会高。许多统计学习方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。分类在于根据其特性将数据“分门别类”，所以在许多领域都有广泛的应用。例如在银行业务中，可以构建一个客户分类模型，对客户按照贷款风险的大小进行分类；在网络安全区域，可以利用日志数据的分类对非法入侵进行检测；在图像处理中，分类可以用来检测图像中是否有人脸出现；在手写识别中，分类可以用于识别手写的数字；在互联网搜索中，网页的分类可以帮助网页的抓取、索引与排序。举一个分类应用的例子——文本分类(test classification) 。这里的文本可以是新闻报道、网页、电子邮件、学术论文等。类别往往是关于文本内容的，例如政治、经济、体育等；也有关于文本特点的，如正面意见、反面意见；还可以根据应用确定，如垃圾邮件、非垃圾邮件等。文本分类是根据文本的特征将其划分到已有的类中。输入是文本的特征向量，输出是文本的类别。通常把文本中的单词定义为特征，每个单词对应一个特征。单词的特征可以是二值的，如果单词在文本中出现则取值是1，否则是0；也可以是多值的，表示单词在文本中出现的频率。直观地，如果“股票”“银行”“货币”这些词出现很多，这个文本可能属于经济类，如果“网球”“比赛”“运动员”这些词频繁出现，这个文本可能属于体育类。]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成模型与判别模型]]></title>
    <url>%2F2018%2F05%2F06%2FStatisticalLearning%2F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[生成模型与判别模型监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：$$Y=f(X)$$或者条件概率分布：$$P(Y|X)$$监督学习方又可以分为生成方法(generative approach)和判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative approach)。生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$这样的方法之所以称之为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。典型的生成模型有：朴素贝叶斯法(Naive bayes,NB)和隐马尔可夫模型(Hidden Markov Model,HMM)。判别方法由数据直接学习策略函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：k近邻法(K-Nearest Neighbours Algorithm,KNN)、感知机(Perceptron)、决策树(Decision Tree,DT)、逻辑斯谛回归模型(Logistic Regression,LR)、最大熵模型(Maximum Entropy,ME)、支持向量机(Support Vector Machine,SVM)、提升方法(Boosting)、条件随机场(Conditional Random Fields,CRF)等。在监督学习中，生成方法和判别方法各有优缺点，适合于不同条件下的学习问题。 生成方法的特点生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。 判别方法的特点判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高；由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>生成模型与判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2018%2F05%2F05%2FStatisticalLearning%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化模型选择的典型方法是正则化(regularization)。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。正则化一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化可以是模型参数向量的范数。正则化一般具有如下形式：$$\min\limits_{f\in{F}}\frac{1}{N}\sum\limits_{i=1}^{N}L(y_i,f(x_i))+\lambda{J(f)}$$其中，第一项$L(y_i,f(x_i))$是经验风险，第二项$J(f)$是正则化项，$\lambda\geq0$为调整两者之间关系的系数。正则化项可以取不通的形式。例如，回归问题中，代价函数是平方损失，正则化项可以是参数向量的$L_2$范数：$$L(w)=\frac{1}{N}\sum\limits_{i=1}^{N}(f(x_i;w)-y_i)^2+\frac{\lambda}{2}||w||^2$$这里，$||w||$表示参数向量$w$的$L_2$范数。正则化项也可以使参数向量的$L_1$范数：$$L(w)=\frac{1}{N}\sum\limits_{i=1}^{N}(f(x_i;w)-y_i)^2+\lambda||w||_1$$这里，$||w||_1$表示参数向量$w$的$L_1$范数。第一项的风险较小的模型可能较复杂(有多个非零参数)，这时第二项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。正则化符合奥尔卡姆剃刀(Occams’s razor)原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，正则化项是对应于模型的先验概率。可以假设模型有较小的先验概率，简单的模型有较大的先验概率。设置惩罚项，即使用正则化，可以使假设函数更简单，且不易发生过拟合问题。在线性回归中，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta$值减少了一个额外的值，即减小$\theta$的平方范数。 先验概率先验概率(prior probability)是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。在贝叶斯统计推断中，不确定数量的先验概率分布是在考虑一些因素之前表达对这一数量的置信程度的概率分布。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。 ####奥尔卡姆剃刀原理奥卡姆剃刀定律（Occam’s Razor, Ockham’s Razor）又称“奥康的剃刀”，它是由14世纪英格兰的逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“简单有效原理”。正如他在《箴言书注》2卷15题说“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。”]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欠拟合与过拟合]]></title>
    <url>%2F2018%2F05%2F05%2FMachineLearning%2F%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%2F</url>
    <content type="text"><![CDATA[欠拟合与过拟合欠拟合模型在训练集上学习的不够好，经验误差大，称为欠拟合。模型训练完成后，用训练数据进行测试，如果错误率高，我们就很容易发现模型还是欠拟合的。 过拟合当模型对训练集学习得太好的时候（学习数据集通性的时候，也学习了数据集上的特性，导致模型在新数据集上表现差，也就是泛化能力差），此时表现为经验误差很小，但泛化误差很大，这种情况称为过拟合。 发生欠拟合的主要原因(高偏差) 训练次数过少 根本的原因是特征维度过少，导致拟合的函数无法满足训练集，误差较大。 由此对应的降低欠拟合(解决高偏差)的方法有： 增加训练次数。 添加其他特征项，例如，组合特征、泛化特征、相关性特征。 添加多项式特征，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。 减少正则化参数，正则化的目的是防止过拟合。发生过拟合的主要原因(高方差) 使用过于复杂的模型(涉及到核函数)； 数据噪声较大； 训练数据少。 由此对应的降低过拟合(解决高方差)的方法有： 1.增加正则化程度,减少特征的数量——简化模型假设，或使用惩罚项限制模型复杂度； 2.进行数据清洗——减少噪声； 3.获得更多的训练样本——收集更多训练数据。 选择合适的核函数以及软边缘参数C就是训练SVM的重要因素。一般来讲，核函数越复杂，模型越偏向于过拟合；C越大模型越偏向于过拟合，反之则拟合不足。 具体模型对应解决欠拟合方法 正则化 正则化方法包括L0正则、L1正则和L2正则。L0范数是指向量中非0的元素的个数。L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。两者都可以实现稀疏性。L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果。 剪枝 剪枝是决策树中一种控制过拟合的方法，预剪枝通过在训练过程中控制树深、叶子节点数、叶子节点中样本的个数等来控制树的复杂度。后剪枝则是在训练好树模型之后，采用交叉验证的方式进行剪枝以找到最优的树模型。提前终止迭代主要是用在神经网络中的，在神经网络的训练过程中我们会初始化一组较小的权值参数，此时模型的拟合能力较弱，通过迭代训练来提高模型的拟合能力，随着迭代次数的增大，部分的权值也会不断的增大。如果我们提前终止迭代可以有效的控制权值参数的大小，从而降低模型的复杂度。上面的几种方法都是操作在一个模型上 ，通过改变模型的复杂度来控制过拟合。另一种可行的方法是结合多种模型来控制过拟合。 Bagging和Boosting 是机器学习中的集成方法，多个模型的组合可以弱化每个模型中的异常点的影响，保留模型之间的通性，弱化单个模型的特性。 Dropout 是深度学习中最常用的控制过拟合的方法，主要用在全连接层处。在一定的概率上（通常设置为0.5，原因是此时随机生成的网络结构最多）隐式的去除网络中的神经元，但会导致网络的训练速度慢2、3倍，而且数据小的时候，Dropout的效果并不会太好。因此只会在大型网络上使用。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>欠拟合与过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD]]></title>
    <url>%2F2018%2F04%2F24%2FDeepLearning%2FSSD%2F</url>
    <content type="text"><![CDATA[SSDSSD(Single Shot MultiBox Detector)简介SSD算法源于2016年发表的算法论文，论文网址：https://arxiv.org/abs/1512.02325SSD的特点在于: SSD结合了YOLO中的回归思想和Faster-RCNN中的Anchor机制，使用全图各个位置的多尺度区域进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster-RCNN一样比较精准。 SSD的核心是在不同尺度的特征特征图上采用卷积核来预测一系列Default Bounding Boxes的类别、坐标偏移。 结构以VGG-16为基础，使用VGG的前五个卷积，后面增加从CONV6开始的5个卷积结构，输入图片要求300*300。 流程 SSD中引入了Defalut Box，实际上与Faster R-CNN的anchor box机制类似，就是预设一些目标预选框，不同的是在不同尺度feature map所有特征点上是使用不同的prior boxes Detector &amp; classifier Detector &amp; classifier的三个部分： default boxes: 默认候选框 localization：4个位置偏移 confidence：21个类别置信度(要区分出背景) default boxesdefault boxex类似于RPN当中的滑动窗口生成的候选框，SSD中也是对特征图中的每一个像素生成若干个框只不过SSD当中的默认框有生成的公式 ratio:长宽比 默认框的大小计算参数：s_min:最底层的特征图计算参数，s_max最顶层的特征图计算参数 localization与confidence定位与置信度的意义如下，主要作用用来过滤，训练经过这一次过滤操作，会将候选框筛选出数量较少的prior boxes。关于三种boxes的解释区别： gournd truth boxes：训练集中，标注好的待检测类别的的位置，即真实的位置，目标的左下角和右上角坐标 default boxes：在feature map上每一个点上生成的某一类别图片的位置。feature map每个点生成4或6个box（数量是事先指定的），格式为转换过后的(x, y, w, h) prior boxes：经过置信度阈值筛选后，剩下的可能性高的boxes。这个box才是会被真正去做回归也就是说SSD中提供事先计算好的候选框这样的机制，只不过不需要再像RPN那种筛选调整，而是直接经过prior boxes之后做回归操作(因为confidence中提供了21个类别概率可以筛选出背景) SSD中的多个Detector &amp; classifier的作用SSD的核心是在不同尺度的特征图上来进行Detector &amp; classifier 容易使得SSD观察到更小的物体 训练与测试流程train流程 输入-&gt;输出-&gt;结果与ground truth标记样本回归损失计算-&gt;反向传播, 更新权值样本标记利用anchor与对应的ground truth进行标记正负样本,每次并不训练8732张计算好的default boxes, 先进行置信度筛选，并且训练指定的正样本和负样本, 如下规则 正样本 与GT重合最高的boxes, 其输出对应label设为对应物体. 物体GT与anchor IoU满足大于0.5 负样本：其它的样本标记为负样本 在训练时, default boxes按照正负样本控制positive：negative=1：3 损失网络输出预测的predict box与ground truth回归变换之间的损失计算， 置信度是采用 Softmax Loss(Faster R-CNN是log loss)，位置回归则是采用 Smooth L1 loss (与Faster R-CNN一样) test流程 输入-&gt;输出-&gt;nms-&gt;输出 比较从图中看出SSD算法有较高的准确率和性能，兼顾了速度和精度 TensorFlow-SSD接口接口文件TensorFlow在github上面已经做了接口的封装，只不过过没有提供在官网的接口当中，是官方案例当中提供的Python文件，需要自行去下载，参考：https://github.com/tensorflow/models/tree/master/research/object_detection我们下载到项目当中如下： 网络配置 模型搭建处理： ssd_net.net:网络结构定义函数 输出：predictions, localisations, logits, end_points，分别表示bbox分类预测值（经过softmax）、bbox位移预测值、bbox分类预测值（未经过softmax）、模型节点 获取默认anchor: ssd_net.anchors:所有阶段特征图的default boxes生成, 得到每层特征图的预设框(x, y, w, h)（公式计算得来） Default boxes的对应Ground Truth标记处理：使 Ground Truth 数量与预测结果一一对应 ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)，grounding truth处理函数，利用gt与每层生成的anchor进行标记得分，返回gclasses, glocalisations, gscores 定义网络的损失： ssd_net.losses： 网络的输出预测结果与Ground Truth之间损失计算]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO]]></title>
    <url>%2F2018%2F04%2F22%2FDeepLearning%2FYOLO%2F</url>
    <content type="text"><![CDATA[YOLOYOLO(You only look once)可以看出YOLO的最大特点是速度快 结构一个网络搞定一切，GoogleNet + 4个卷积+2个全连接层 流程 原始图片resize到448x448，经过前面卷积网络之后，将图片输出成了一个7x7x30的结构 默认7 7个单元格，这里用3 3的单元格图演示 个单元格预测两个bbox框 进行NMS筛选,筛选概率以及IoU 单元格(grid cell) 每个单元格负责预测一个物体类别，并且直接预测物体的概率值 每个单元格预测两个(默认)bbox位置，两个bbox置信度(confidence)772=98个bbox 30=(4+1+4+1+20), 4个坐标信息，1个置信度(confidence)代表一个bbox的结果， 20代表 20类的预测概率结果 网格输出筛选一个网格预测多个Bbox，在训练时我们只有一个Bbox专门负责（一个Object 一个Bbox），共30个参数 通过置信度大小比较 每个bounding box都对应一个confidence score，如果grid cell里面没有object，confidence就是0，如果有，则confidence score等于预测的box和ground truth的IOU值所以如何判断一个grid cell中是否包含object呢？如果一个object的ground truth的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。 这个概率可以理解为不属于任何一个bbox，而是属于这个单元格所预测的类别。 输出位置结果 其中(x,y,w,h)：x,y为为中心相对于单元格的offset，w,h为bbox的宽高相对整张图片的占比$$x=\frac{x_c}{w_i}S-x_{col},y=\frac{y_c}{h_i}S-y_{row}$$$$w=\frac{w_b}{w_i},h=\frac{h_b}{h_i}$$ 非最大抑制(NMS)每个Bbox的Class-Specific Confidence Score以后，设置阈值，滤掉概率的低的bbox，对每个类别过滤IoU，就得到最终的检测结果 训练 预测框对应的目标值标记 confidence：格子内是否有目标 20类概率：标记每个单元格的目标类别 损失 三部分损失 bbox损失+confidence损失+classfication损失 与Faster R-CNN比较Faster R-CNN利用RPN网络与真实值调整了候选区域，然后再进行候选区域和卷积特征结果映射的特征向量的处理来通过与真实值优化网络预测结果。而这两步在YOLO当中合并成了一个步骤，直接网络输出预测结果进行优化。所以经常也会称之为YOLO算法为直接回归法代表。YOLO的特点就是快 YOLO总结 优点 速度快 缺点 准确率会打折扣 YOLO对相互靠的很近的物体（挨在一起且中点都落在同一个格子上的情况），还有很小的群体检测效果不好，这是因为一个网格中只预测了两个框]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster R-CNN]]></title>
    <url>%2F2018%2F04%2F20%2FDeepLearning%2FFaster%20R-CNN%2F</url>
    <content type="text"><![CDATA[Faster R-CNNFaster R-CNN在Faster R-CNN中加入一个提取边缘的神经网络，也就说找候选框的工作也交给神经网络来做了。这样，目标检测的四个基本步骤（候选区域生成，特征提取，分类，位置精修）终于被统一到一个深度网络框架之内。Faster R-CNN可以简单地看成是区域生成网络+Fast R-CNN的模型，用区域生成网络（Region Proposal Network，简称RPN）来代替Fast R-CNN中的选择性搜索方法，结构如下：流程 首先向CNN网络(VGG-16)输入任意大小图片 Faster RCNN使用一组基础的conv+relu+pooling层提取feature map。该feature map被共享用于后续RPN层和全连接层。 Region Proposal Networks。RPN网络用于生成region proposals，该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals，输出其Top-N(默认为300)的区域给RoI pooling生成anchors -&gt; softmax分类器提取fg anchors -&gt; bbox reg回归fg anchors -&gt; Proposal Layer生成proposals 第2步得到的高维特征图和第3步输出的区域建合并输入RoI池化层(类), 该输出到全连接层判定目标类别。 利用proposal feature maps计算每个proposal的不同类别概率，同时bounding box regression获得检测框最终的精确位置 RPN原理RPN网络的主要作用是得出比较准确的候选区域。整个过程分为两步 用n×n(默认3×3=9)的大小窗口去扫描特征图，每个滑窗位置映射到一个低维的向量(默认256维)，并为每个滑窗位置考虑k种(在论文设计中k=9)可能的参考窗口(论文中称为anchors) 低维特征向量输入两个并行连接的1 x 1卷积层然后得出两个部分：reg窗口回归层(用于修正位置)和cls窗口分类层(是否为前景或背景概率) anchors 3*3卷积核的中心点对应原图上的位置，将该点作为anchor的中心点，在原图中框出多尺度、多种长宽比的anchors,三种尺度{ 128，256，512 }， 三种长宽比{1:1，1:2，2:1} 举个例子： 候选区域的训练 训练样本anchor标记 每个ground-truth box有着最高的IoU的anchor标记为正样本 剩下的anchor/anchors与任何ground-truth box的IoU大于0.7记为正样本，IoU小于0.3，记为负样本 剩下的样本全部忽略 正负样本比例为1：3 训练损失 RPN classification (anchor good / bad) ，二分类，是否有物体，是、否 RPN regression (anchor -&gt; proposal) ，回归 注：这里使用的损失函数和Fast R-CNN内的损失函数原理类似，同时最小化两种代价 候选区域的训练是为了让得出来的 正确的候选区域， 并且候选区域经过了回归微调。在这基础之上做Fast RCNN训练是得到特征向量做分类预测和回归预测。 Faster R-CNN的训练Faster R-CNN的训练分为两部分，即两个网络的训练。前面已经说明了RPN的训练损失，这里输出结果部分的的损失（这两个网络的损失合并一起训练）： Fast R-CNN classification (over classes) ，所有类别分类N+1 Fast R-CNN regression (bbox regression) 效果对比 R-CNN Fast R-CNN Faster R-CNN Test time/image(s) 50.0 2.0 0.2 mAP(VOC2007) 66.0 66.9 66.9 Faster R-CNN总结 优点 提出RPN网络 端到端网络模型 缺点 训练参数过大 对于真实训练使用来说还是依然过于耗时 可以改进的需求： RPN（Region Proposal Networks） 改进 对于小目标选择利用多尺度特征信息进行RPN 速度提升 如YOLO系列算法，删去了RPN，直接对proposal进行分类回归，极大的提升了网络的速度]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Faster R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast R-CNN]]></title>
    <url>%2F2018%2F04%2F19%2FDeepLearning%2FFast%20R-CNN%2F</url>
    <content type="text"><![CDATA[Fast R-CNNFast R-CNN——相对SPP的改进 提出一个RoI pooling，然后整合整个模型，把CNN、SPP变换层、分类器、bbox回归几个模块一起训练 步骤 首先将整个图片输入到一个基础卷积网络，得到整张图的feature map 将region proposal（RoI）映射到feature map中 RoI pooling layer提取一个固定长度的特征向量，每个特征会输入到一系列全连接层，得到一个RoI特征向量（此步骤是对每一个候选区域都会进行同样的操作） 其中一个是传统softmax层进行分类，输出类别有K个类别加上”背景”类 另一个是bounding box regressor RoI pooling首先RoI pooling只是一个简单版本的SPP，目的是为了减少计算时间并且得出固定长度的向量。 RoI池层使用最大池化将任何有效的RoI区域内的特征转换成具有H×W的固定空间范围的小feature map，其中H和W是超参数 它们独立于任何特定的RoI 例如：VGG16 的第一个 FC 层的输入是 7 x 7 x 512，其中 512 表示 feature map 的层数。在经过 pooling 操作后，其特征输出维度满足 H x W。假设输出的结果与FC层要求大小不一致，对原本 max pooling 的单位网格进行调整，使得 pooling 的每个网格大小动态调整为 h/H,w/W, 最终得到的特征维度都是 HxWxD。 它要求 Pooling 后的特征为 7 x 7 x512，如果碰巧 ROI 区域只有 6 x 6 大小怎么办？每个网格的大小取 6/7=0.85 , 6/7=0.85，以长宽为例，按照这样的间隔取网格：[0,0.85,1.7,2.55,3.4,4.25,5.1,5.95]，取整后，每个网格对应的起始坐标为：[0,1,2,3,3,4,5] 设计单个尺度的原因：涉及到single scale与multi scale两者的优缺点 single scale：直接将image定为某种scale，直接输入网络来训练即可。（Fast R-CNN） multi scal：也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本 后者比前者更加准确些，没有突更多，但是第一种时间要省很多，所以实际采用的是第一个策略，因此Fast R-CNN要比SPPNet快很多也是因为这里的原因 End-to-End model从输入端到输出端直接用一个神经网络相连，整体优化目标函数。特征提取CNN的训练和SVM分类器的训练在时间上是先后顺序，两者的训练方式独立，因此SVM的训练Loss无法更新SPP-Layer之前的卷积层参数，去掉了SVM分类这一过程，所有特征都存储在内存中，不占用硬盘空间，形成了End-to-End模型（proposal除外，end-to-end在Faster-RCNN中得以完善） 使用了softmax分类 RoI pooling能进行反向传播，SPP层不适合 多任务损失-Multi-task loss 对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景，使用交叉熵损失 对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，使用平均绝对误差（MAE）损失即L1损失 $\frac{1}{m}\sum\limits_{i=1}^m \left|(y_i-\hat{y_i})\right|$ fine-tuning训练: 在微调时，调整 CNN+RoI pooling+softmax+bbox regressor网络当中的参数 R-CNN、SPPNet、Fast R-CNN效果对比 参数 R-CNN SPPNet Fast R-CNN 训练时间(h) 84 25 9.5 测试时间/图片(s) 47.0 2.3 0.32 mAP 66.0 63.1 66.9 其中有一项指标为mAP，这是一个对算法评估准确率的指标，mAP衡量的是学出的模型在所有类别上的好坏 Fast R-CNN总结 缺点 使用Selective Search提取Region Proposals，没有实现真正意义上的端对端，操作也十分耗时]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Fast R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPPNet]]></title>
    <url>%2F2018%2F04%2F18%2FDeepLearning%2FSPPNet%2F</url>
    <content type="text"><![CDATA[SPPNetSPPNet——相对R-CNN的改进SPPNet主要存在两点改进地方，提出了SPP层 减少卷积计算 防止图片内容变形 |R-CNN模型|SPPNet模型||:-|:-||1.R-CNN是让每个候选区域经过crop/wrap等操作变换成固定大小的图像2.固定大小的图像塞给CNN 传给后面的层做训练回归分类操作|1.SPPNet把全图塞给CNN得到全图的feature map2.让候选区域与feature map直接映射，得到候选区域的映射特征向量3.映射过来的特征向量大小不固定，这些特征向量塞给SPP层(空间金字塔变换层)，SPP层接收任何大小的输入，输出固定大小的特征向量，再塞给FC层| 特征映射原始图片经过CNN变成了feature map,原始图片通过选择性搜索(SS)得到了候选区域，现在需要将基于原始图片的候选区域映射到feature map中的特征向量。映射过程图参考如下：整个映射过程有具体的公式，如下假设$(x′,y′)$表示特征图上的坐标点，坐标点$(x,y)$表示原输入图片上的点，那么它们之间有如下转换关系，这种映射关系与网络结构有关：$(x,y)=(S∗x′,S∗y′)$，即 左上角的点： $x′=\frac{x+S}{S}=\frac{x}{S}+1$ 右下角的点： $x′=\frac{x-S}{S}=\frac{x}{S}-1$ 其中 SS 就是CNN中所有的strides的乘积，包含了池化、卷积的stride。论文中使用的S计算出来为16拓展：如果关注这个公式怎么计算出来，请参考：http://kaiminghe.com/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf spatial pyramid pooling通过空间金字塔池化(spatial pyramid pooling)将任意大小的特征图转换成固定大小的特征向量示例：假设原图输入是224x224，对于conv出来后的输出是13x13x256的，可以理解成有256个这样的Filter，每个Filter对应一张13x13的feature map。接着在这个特征图中找到每一个候选区域映射的区域，spp layer会将每一个候选区域分成1x1，2x2，4x4三张子图，对每个子图的每个区域作max pooling，得出的特征再连接到一起，就是(16+4+1)x256的特征向量，即每个候选区域得到21x256的特征向量，接着给全连接层做进一步处理，如下图： SPPNet总结来看下SPPNet的完整结构 优点 SPPNet在R-CNN的基础上提出了改进，通过候选区域和feature map的映射，配合SPP层的使用，从而达到了CNN层的共享计算，减少了运算时间， 后面的Fast R-CNN等也是受SPPNet的启发 缺点 训练依然过慢、效率低，特征需要写入磁盘(因为SVM的存在) 分阶段训练网络：选取候选区域、训练CNN、训练SVM、训练bbox回归器, SPP-Net在fine-tuning阶段无法使用反向传播微调SPP-Net前面的Conv层]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>SPPNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Overfeat与R-CNN]]></title>
    <url>%2F2018%2F04%2F16%2FDeepLearning%2FOverfeat%E4%B8%8ER-CNN%2F</url>
    <content type="text"><![CDATA[Overfeat与R-CNN目标检测-Overfeat模型对于一张图片当中多个目标，多个类别的时候。前面的输出结果是不定的，有可能是以下有四个类别输出这种情况。或者N个结果，这样的话，网络模型输出结构不定所以需要一些他的方法解决目标检测（多个目标）的问题，试图将一个检测问题简化成分类问题 滑动窗口目标检测的暴力方法是从左到右、从上到下滑动窗口，利用分类识别目标。为了在不同观察距离处检测不同的目标类型，我们使用不同大小和宽高比的窗口。如下图所示：这样就变成每张子图片输出类别以及位置，变成分类问题。但是滑动窗口需要初始设定一个固定大小的窗口，这就遇到了一个问题，有些物体适应的框不一样，所以需要提前设定K个窗口，每个窗口滑动提取M个，总共K x M 个图片，通常会直接将图像变形转换成固定大小的图像，变形图像块被输入 CNN 分类器中，提取特征后，我们使用一些分类器识别类别和该边界框的另一个线性回归器。 训练数据集首先我们会准备所需要的训练集数据，每张图片的若干个子图片以及每张图片的类别位置，如下我们从某张图片中滑动出的若干的图片。 Overfeat模型总结这种方法类似一种暴力穷举的方式，会消耗大量的计算力量，并且由于窗口大小问题可能会造成效果不准确。但是提供了一种解决目标检测问题的思路。 目标检测-R-CNN模型RCNN(Regions with CNN)，在CVPR 2014年中Ross Girshick提出R-CNN。 完整R-CNN结构不使用暴力方法，而是用候选区域方法(region proposal method)，创建目标检测的区域改变了图像领域实现物体检测的模型思路，R-CNN是以深度神经网络为基础的物体检测的模型 ，R-CNN在当时以优异的性能令世人瞩目，以R-CNN为基点，后续的SPPNet、Fast R-CNN、Faster R-CNN模型都是照着这个物体检测思路。步骤（以AlexNet网络为基准） 找出图片中可能存在目标的侯选区域region proposal，得到2000个候选区域，统一大小 通过选择性搜索(SS)算法，进行筛选 统一大小：通过crop+warp crop：截取原图片的一个固定大小的patch warp：将原图片的ROI(候选区域)放到一个固定大小的patch 进行CNN提取特征，得出2000特征向量 使用AlexNet结构，输入要去227*227 提取出的特征会保存在磁盘中 每个候选区都要进行特征提取=&gt;[2000, 4096] 将2000×4096维特征与20个SVM组成的权值矩阵4096×20相乘(20种分类，SVM是二分类器，则有20个SVM)，获得2000×20维矩阵 20代表目标检测当前数据集一共需要检测20种类别 得出[2000, 20]的得分矩阵，打分 分别对2000×20维矩阵中每一列即每一类进行非极大值抑制（NMS:non-maximum suppression）剔除重叠建议框，得到该列即该类中得分最高的一些建议框 NMS：即通过比较选取的最高得分建议框与其他建议框的IoU，保留最高得分的建议框作为预测结果，并剔除与最高得分框比较并且其之间IoU大于指定阈值(一般为0.5)的建议框，循环迭代重复以上过程。 修正bbox，对bbox做回归微调 通过线性回归，特征值是候选区域，目标是对应的Ground-Truth(GT) 建立回归方程学习参数 候选区域选择性搜索(SelectiveSearch，SS)中，首先将每个像素作为一组。然后，计算每一组的纹理，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的ROI(即候选区域)。SelectiveSearch在一张图片上提取出来约2000个侯选区域，需要注意的是这些候选区域的长宽不固定。 而使用CNN提取候选区域的特征向量，需要接受固定长度的输入，所以需要对候选区域做一些尺寸上的修改。 Crop+Warp传统的CNN限制了输入必须固定大小，所以在实际使用中往往需要对原图片进行crop或者warp的操作 crop：截取原图片的一个固定大小的patch warp：将原图片的ROI缩放到一个固定大小的patch 无论是crop还是warp，都无法保证在不失真的情况下将图片传入到CNN当中。会使用一些方法尽量让图片保持最小的变形。 各向异性缩放：即直接缩放到指定大小，这可能会造成不必要的图像失真 各向同性缩放：在原图上出裁剪侯选区域， (采用侯选区域的像素颜色均值)填充到指定大小在边界用固定的背景颜色 CNN网络提取特征在侯选区域的基础上提取出更高级、更抽象的特征，这些高级特征是作为下一步的分类器、回归的输入数据。提取的这些特征将会保存在磁盘当中(这些提取的特征才是真正的要训练的数据) 特征向量训练分类器SVM假设一张图片的2000个侯选区域，那么提取出来的就是2000 x 4096这样的特征向量（R-CNN当中默认CNN层输出4096特征向量）。那么最后需要对这些特征进行分类，R-CNN选用SVM进行二分类。假设检测N个类别，那么会提供20个不同类别的SVM分类器，每个分类器都会对2000个候选区域的特征向量分别判断一次，这样得出[2000， 20]的得分矩阵，如下图所示 每个SVM分类器判断2000个候选区域是某类别，还是背景 非最大抑制(NMS) 目的 筛选候选区域，得到最终候选区域结果 迭代过程 对于所有的2000个候选区域得分进行概率筛选 然后对剩余的候选框，每个类别进行IoU（交并比）&gt;= 0.5 筛选 假设现在滑动窗口有：A、B、C、D、E 5个候选框， 第一轮：假设B是得分最高的，与B的IoU＞0.5删除。现在与B计算IoU，DE结果＞0.5，剔除DE，B作为一个预测结果 第二轮：AC中，A的得分最高，与A计算IoU，C的结果＞0.5，剔除C，A作为一个结果 最终结果为在这个5个中检测出了两个目标为A和B 修正候选区域那么通过非最大一直筛选出来的候选框不一定就非常准确怎么办？R-CNN提供了这样的方法，建立一个bbox regressor 回归用于修正筛选后的候选区域，使之回归于ground-truth，默认认为这两个框之间是线性关系，因为在最后筛选出来的候选区域和ground-truth很接近了 修正过程(线性回归) 给定：anchor $A=(A_{x}， A_{y}， A_{w}， A_{h})$ 和 $GT=[G_{x}， G_{y}， G_{w}， G_{h}]$寻找一种变换F，使得：$F(A_{x}， A_{y}， A_{w}， A_{h})=(G_{x}^{‘}， G_{y}^{‘}， G_{w}^{‘}， G_{h}^{‘})$，其中$(G_{x}^{‘}， G_{y}^{‘}， G_{w}^{‘}， G_{h}^{‘})≈(G_{x}， G_{y}， G_{w}， G_{h})$ R-CNN训练过程R-CNN的训练过程这些部分，正负样本准备+预训练+微调网络+训练SVM+训练边框回归器 正负样本准备 预训练+微调 预训练：别人已经在大数据集上训练好的CNN网络参数模型，model1 微调：利用标记好的样本，输入到model1中，继续性训练，得出model2(CNN网络，迁移学习) 训练SVM分类器，每个类别训练一个分类器 特征M*4096，一个SVM 正负样本标记结果(100个正，900个负) 总共得到4096*20的SVM权重 回归训练 筛选候选框，只对那些跟GT的IoU超过某个阈值且IoU最大的region proposal(候选区域)回归 训练得到回归的参数 正负样本准备对于训练集中的所有图像，采用selective search方式来获取，最后每个图像得到2000个region proposal。但是每个图像不是所有的候选区域都会拿去训练。保证正负样本比例1：3|样本|描述||:-|:-||正样本|某个region proposal和当前图像上的所有ground truth中重叠面积最大的那个的IOU大于等于0.5，则该region proposal作为这个ground truth类别的正样本||负样本|某个region proposal和当前图像上的所有ground truth中重叠面积最大的那个的IOU都小于0.5，则该region proposal作为这个ground truth类别的负样本| 这样得出若干个候选区域以及对应的标记结果。 预训练(pre-training)CNN模型层数多，模型的容量大，通常会采用2012年的著名网络AlexNet来学习特征，包含5个卷积层和2个全连接层，利用大数据集训练一个分类器，比如著名的ImageNet比赛的数据集，来训练AlexNet，保存其中的模型参数。 微调(fine-tuning)——迁移学习AlexNet是针对ImageNet训练出来的模型，卷积部分可以作为一个好的特征提取器，后面的全连接层可以理解为一个好的分类器。R-CNN需要在现有的模型上微调卷积参数。 将第一步中得到的样本进行尺寸变换，使得大小一致，然后作为预训练好的网络的输入，继续训练网络(迁移学习) SVM分类器针对每个类别训练一个SVM的二分类器。举例：猫的SVM分类器，输入维度是2000 4096，目标还是之前第一步标记是否属于该类别猫，训练结果是得到SVM的权重矩阵W，W的维度是409620。 bbox回归器训练只对那些跟ground truth的IoU超过某个阈值且IOU最大的region proposal回归，其余的region proposal不参与。 R-CNN测试过程 输入一张图像，利用selective search得到2000个region proposal。 对所有region proposal变换到固定尺寸并作为已训练好的CNN网络的输入，每个候选框得到的4096维特征 采用已训练好的每个类别的svm分类器对提取到的特征打分，所以SVM的weight matrix是4096 N，N是类别数，这里一共有20个SVM， 得分矩阵是200020 采用non-maximun suppression(NMS)去掉候选框 通过上一步得到region proposal进行回归 R-CNN总结流程在VOC2007数据集上的平均精确度达到66% 缺点 训练阶段多 步骤繁琐 微调网络+训练SVM+训练边框回归器 训练耗时大 占用磁盘空间大 5000张图片产生几百G的特征文件(VOC数据集的检测结果，因为SVM的存在) 处理速度慢 使用GPU VGG16模型处理一张图像需要47s 图片变形问题 候选区域要经过crop/warp进行固定大小，无法保证图片不变形]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Overfeat</tag>
        <tag>R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测]]></title>
    <url>%2F2018%2F04%2F15%2FDeepLearning%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[目标检测图像识别三大任务 目标识别：输出类别(图a) 目标检测：输出类别以及物体在图片当中的位置(图b) 技术成熟并且使用更多的场景 目标分割：描述物体形状，剔除背景(图c，图d) 适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割 图像识别的发展 通用场景 谷歌、微软、Facebook、百度、阿里巴巴在内的科技巨头都花费了大量的人力财力做研究，搭建了很多图像识别的平台。 垂直场景 医疗领域：医疗影像的检测 林木产业：木板树种检测识别 垂直应用场景里的行业特质挖掘和经验积累往往会被忽视，所以在垂直领域的行业中大量的公司正在开发相当多的图像应用。 目标检测定义识别图片中的物体以及物体的位置(坐标位置) 物体即图像中存在的物体对象，但是能检测哪些物体会受到人为设定限制。 目标检测中能检测出来的物体取决于当前任务（数据集）需要检测的物体有哪些。假设我们的目标检测模型定位是检测动物（牛、羊、猪、狗、猫五种结果），那么模型对任何一张图片输出结果不会输出鸭子、书籍等其它类型结果。 位置目标检测的位置信息一般由两种格式（以图片左上角为原点(0,0)）： 极坐标表示：(xmin, ymin, xmax, ymax) xmin,ymin:x,y坐标的最小值 xmin,ymin:x,y坐标的最大值 中心点坐标：(x_center, y_center, w, h) x_center, y_center:目标检测框的中心点坐标 w,h:目标检测框的宽、高 假设这个图像是1000x800，所有这些坐标都是构建在像素层面上：中心点坐标结果如下： 目标检测的技术发展历史 传统目标检测方法（候选区域+手工特征提取+分类器） HOG+SVM、DPM region proposal+CNN提取分类的目标检测框架 (R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN) 端到端（End-to-End）的目标检测框架 YOLO、SSD 应用场景行业 公安行业的应用 公安行业用户的迫切需求是在海量的视频信息中，发现犯罪嫌疑人的线索。人工智能在视频内容的特征提取、内容理解方面有着天然的优势。可实时分析视频内容，检测运动对象，识别人、车属性信息，并通过网络传递到后端人工智能的中心数据库进行存储。 农作物的应用 农业中农作物表面的病虫害识别也需要用到目标检测技术 医疗影像检测 人工智能在医学中的应用目前是一个热门的话题，医学影像图像中病变部位检测和识别对于诊断的自动化，提供优质的治疗具有重要的意义。 电商行业的应用 电商行业中充满无数的商品，利用检测功能查询相关商品，快速找到用户需要的商品类型或者品牌类别，从而提高电商领域的用户满意度 应用类别 道路检测 动物检测 商品检测 车牌检测 菜品检测 车型检测 目标检测算法分类 两步走的目标检测：先进行区域推荐，而后进行目标分类 代表：R-CNN、SPP-net、Fast R-CNN、Faster R-CNN 端到端的目标检测：采用一个网络一步到位 代表：YOLO、SSD 目标检测的任务 分类： N个类别 输入：图片 输出：类别标签 评估指标：Accuracy 定位： N个类别 输入：图片 输出：物体的位置坐标 主要评估指标：IoU 其中我们得出来的(x,y,w,h)有一个专业的名词，叫做bounding box(bbox). 两种Bounding box名称 在目标检测当中，对bbox主要由两种类别。 Ground-truth bounding box：图片当中真实标记的框 Predicted bounding box：预测的时候标记的框 一般在目标检测当中，我们预测的框有可能很多个，真实框GT也有很多个。 检测的评价指标 任务 描述 输入 输出 评价标准 检测和定位 在输入图片中找出存在的物体类别和位置(可能存在多种物体) 图片 类别标签和位置 IoU (Intersection over Union) mAP (Mean Average Precision) IoU(交并比) 两个区域的重叠程度overlap：侯选区域和标定区域的IoU值 目标定位的简单实现在分类的时候我们直接输出各个类别的概率，如果再加上定位的话，我们可以考虑在网络的最后输出加上位置信息。 回归位置增加一个全连接层，即为FC1、FC2 - FC1：作为类别的输出 - FC2：作为这个物体位置数值的输出 假设有10个类别，输出[p1,p2,p3,…,p10]，然后输出这一个对象的四个位置信息[x,y,w,h]。同理知道要网络输出什么，如果衡量整个网络的损失 - 对于分类的概率，还是使用交叉熵损失 - 位置信息具体的数值，可使用MSE均方误差损失(L2损失) 位置数值的处理对于输出的位置信息是四个比较大的像素大小值，在回归的时候不适合。目前统一的做法是，每个位置除以图片本身像素大小。假设以中心坐标方式，那么x = x/x_image,y/y_image, w/x_image,h/y_image,也就是这几个点最后都变成了0~1之间的值。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CapsuleNet]]></title>
    <url>%2F2018%2F04%2F13%2FDeepLearning%2FCapsuleNet%2F</url>
    <content type="text"><![CDATA[CapsuleNet出现原因2017年，CapsuleNet的出现是Hinton对于卷积神经网络等的思考，想去构建一种新的网络结构，如何克服CNN存在的问题 CNN的目标不正确 CNN对于旋转类型图片不确定 同一个图像的旋转识别不同的问题 CNN对于图片整体结构关系不确定 图像识别中的“毕加索问题” 因此，Hinton认为人的视觉系统会有不一样的做法，即人的视觉系统会建立坐标框架，坐标框架是参与到识别过程中，识别过程受到了空间概念的支配 定义胶囊神经网络（CapsuleNet）是一种机器学习系统，该方法试图更接近地模仿生物神经组织，该想法是将称为胶囊的结构添加到CNN当中。 改进特点 添加一个Capsule层 Capsule 是一组神经元，其输入输出向量表示特定实体类型的实例化参数(即特定物体、概念实体等出现的概率与某些属性)。假设有手写数字10类别的分类任务，比如说10 x 16，输出表示了图像中存在的特定实体16个的各种性质。例如姿势(位置，大小，方向)、变形、速度、反射率，色彩、纹理等等。即输出概率之外还会输出特定类别的实体属性。输入输出向量的长度表示了某个实体出现的概率，所以它的值必须在 0 到 1 之间。 结构 第一个卷积层：使用了256个9×9 卷积核，步幅为 1，ReLU 激活函数。输出的张量才能是20×20×256 第二个卷积层：作为Capsule层的输入而构建相应的张量结构。 32个,9×9 的卷积核，步幅为 2下做卷积, 得到6×6×32的张量，等价于 6×6×1×32 8次不同权重的 Conv2d 操作，得到6 x 6 x 8 x 32 6×6×32=1152Capsule单元，每个向量长度为8 第三层:有10个标准的Capsule单元，每个Capsule的输出向量有16 个元素，10 X 16 参数： $W_{i,j}$有1152x10个，每个是8x16的向量 效果 Capsules on MNIST 达到约0.25%的错误率，相比之前CNN0.39%的错误率提高]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>CapsuleNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN]]></title>
    <url>%2F2018%2F04%2F12%2FDeepLearning%2FGAN%2F</url>
    <content type="text"><![CDATA[GAN定义一个生成器G(Generator)和一个判别器D(Discriminator) 生成器：输入噪点数据，生成输出 判别器：训练样本，对生成输出进行判别真伪 原理从训练库里获取很多训练样本，从而学习这些训练案例生成的概率分布 黑色虚线：真是样本的分布 绿色实线：生成样本的分布 蓝色虚线：判别器判断的概率分布 $z$表示噪声，$z$到$x$表示生成器生成的分布映射 过程分析 定义GAN结构生成数据 (a)状态处于最初始的状态，生成器生成的分布和真实分布区别较大，并且判别器判别出样本的概率不稳定 在真实数据上训练n_epochs判别器，产生fake(假数据)并训练判别器识别为假 通过多次训练判别器来到(b)样本状态，此时判别样本区别得非常显著 训练生成器达到欺骗判别器的效果 训练生成器之后达到(c)样本状态，此时生成器分布相比之前，逼近了真实样本分布。经过多次反复训练迭代之后 最终希望能够达到(d)状态，生成样本分布拟合于真实样本分布，并且判别器分辨不出样本是生成的还是真实的。 训练损失$$\min\limits_{G}\max\limits_{D}V(D,G)=\mathbb E_{x-p_{data}(x)}logD(x)+\mathbb E_{z-p_{z}(z)}log(1-D(G(z)))$$ $V(G,D)$：表示P_x和P_z的差异程度 $\max\limits_{D}V(D,G)$：固定生成器G，尽可能地让判别器能够最大化地判别出样本来自于真实数据还是生成的数据 $\min\limits_{G}L$：固定判别器D的条件下得到生成器G，能够最小化真实样本与生成样本的差异 整个优化看做一个部分 判别器：相当于一个分类器，判断图片的真伪，二分类问题，使用交叉熵损失 对于真实样本：对数预测概率损失，提高预测概率$$E_{x-p_{data}(x)}log(D(x))$$对于正是样本：对数预测概率损失，降低预测概率$$E_{z-p_{z}(z)}log(1-D(G(z)))$$最终可以转化为$$\max\tilde{V}=\frac{1}{m}\sum\limits_{i=1}^mlogD(x^{(i)})+\frac{1}{m}\sum\limits_{i=1}^m{log(1-D(\tilde{x}^{(i)}))}$$$$\min{L}=-\frac{1}{m}\sum\limits_{i=1}^mlogD(x^{(i)})-\frac{1}{m}\sum\limits_{i=1}^m{log(1-D(\tilde{x}^{(i)}))}$$ Generator与Discriminator结构G、D结构是两个网络，特点是能够反向传播可导计算，不同版本的GAN结构不同 2014年最开始的模型： G、D都是multilayer perceptron（MLP） 缺点：实践证明训练难度大，效果不行 2015：使用卷积神经网络+GAN(Deep Convolutional GAN,DCGAN) 改进： 判别器D中取出pooling，全部变成卷积、生成器G中使用反卷积(下图) D、G中都增加了BN层 去除了所有的全连接层 判别器D中全部使用Leaky ReLU，生成器除了最后输出层使用tanh其它层全换成ReLU Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191from keras.optimizers import Adamfrom keras.layers import Input, Dense, Reshape, Flatten, Dropoutfrom keras.layers import BatchNormalization, Activation, ZeroPadding2Dfrom keras.layers.advanced_activations import LeakyReLUfrom keras.layers.convolutional import UpSampling2D, Conv2Dfrom keras.models import Sequential, Modelfrom keras.datasets import mnistimport numpy as npimport matplotlib.pylab as pltimport sslssl._create_default_https_context=ssl._create_unverified_contextclass DCGAN(object): def __init__(self): # 输入图片的形状 self.img_rows = 28 self.img_cols = 28 self.channels = 1 self.img_shape = (self.img_rows,self.img_cols,self.channels) def init_model(self): ''' 初始化模型 :return: ''' # 生成原始噪点数据大小 self.latent_dim = 100 # 获取定义好的优化器 optimizer = Adam(0.0002, 0.5) # 1、建立判别器训练参数 # 选择损失，优化器，以及衡量准确率 self.discriminator = self.build_discriminator() self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) # 2、联合建立生成器训练参数，指定生成器损失 self.generator = self.build_generator() # 定义输出的噪点数据结构，输入到生成器中，得到图片 z = Input(shape=(self.latent_dim,)) img = self.generator(z) # 合并模型的损失，并且之后只训练生成器，判别器不训练 self.discriminator.trainable = False valid = self.discriminator(img) # 训练生成器欺骗判别器 self.combined = Model(z, valid) # 损失优化 self.combined.compile(loss='binary_crossentropy', optimizer=optimizer) def build_discriminator(self): ''' 创建Discrimination :return: ''' model = Sequential() model.add(Conv2D(32,kernel_size=3,strides=2,input_shape=self.img_shape, padding="SAME")) model.add(LeakyReLU(alpha=0.2)) model.add(Dropout(0.25)) model.add(Conv2D(64, kernel_size=3, strides=2, padding="same")) model.add(ZeroPadding2D(padding=((0, 1), (0, 1)))) model.add(BatchNormalization(momentum=0.8)) model.add(LeakyReLU(alpha=0.2)) model.add(Dropout(0.25)) model.add(Conv2D(128, kernel_size=3, strides=2, padding="same")) model.add(BatchNormalization(momentum=0.8)) model.add(LeakyReLU(alpha=0.2)) model.add(Dropout(0.25)) model.add(Conv2D(256, kernel_size=3, strides=1, padding="same")) model.add(BatchNormalization(momentum=0.8)) model.add(LeakyReLU(alpha=0.2)) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(1, activation='sigmoid')) model.summary() img = Input(shape=self.img_shape) validity = model(img) return Model(img, validity) def build_generator(self): ''' 创建Generator :return: ''' model = Sequential() model.add(Dense(128 * 7 * 7, activation="relu", input_dim=self.latent_dim)) model.add(Reshape((7, 7, 128))) model.add(UpSampling2D()) model.add(Conv2D(128, kernel_size=3, padding="same")) model.add(BatchNormalization(momentum=0.8)) model.add(Activation("relu")) model.add(UpSampling2D()) model.add(Conv2D(64, kernel_size=3, padding="same")) model.add(BatchNormalization(momentum=0.8)) model.add(Activation("relu")) model.add(Conv2D(self.channels, kernel_size=3, padding="same")) model.add(Activation("tanh")) model.summary() noise = Input(shape=(self.latent_dim,)) img = model(noise) return Model(noise, img) def train(self, epochs, batch_size=32): # 加载手写数字 (X_train, _), (_, _) = mnist.load_data() # 进行归一化[60000, 28, 28] X_train = X_train / 127.5 - 1. # 扩充通道数[60000, 28, 28, 1] X_train = np.expand_dims(X_train, axis=3) # 正负样本的目标值建立 # batch_size大小真实样本的目标值1 # batch_size大小假样本的目标值0 valid = np.ones((batch_size, 1)) fake = np.zeros((batch_size, 1)) # 循环迭代训练 for epoch in range(epochs): # 1、训练判别器 # 选择随机的一些真实样本 # 准备batch_size个真样本 idx = np.random.randint(0, X_train.shape[0], batch_size) imgs = X_train[idx] # 生成器产生假样本 # 准备batch_size个假样本,[batch_size, ] noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) # 使用generator生成假样本 gen_imgs = self.generator.predict(noise) # 训练判别器过程 d_loss_real = self.discriminator.train_on_batch(imgs, valid) d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake) # 计算平均两部分损失 d_loss = np.add(d_loss_real, d_loss_fake) / 2 # 2、训练生成器，停止判别器 # 合并训练，并停止训练判别器 # 用目标值为1去训练，目的使得生成器生成的样本越来越接近真是样本 g_loss = self.combined.train_on_batch(noise, valid) # 画出结果 print("迭代次数:%d [D 损失: %f, 准确率: %.2f%%], [G 损失: %f]" %(epoch, d_loss[0], 100 * d_loss[1], g_loss)) # 保存生成的图片 if epoch % 50 == 0: self.save_imgs(epoch) def save_imgs(self, epoch): # 生成随机样本 r, c = 5, 5 noise = np.random.normal(0, 1, (r * c, self.latent_dim)) gen_imgs = self.generator.predict(noise) # Rescale images 0 - 1 gen_imgs = 0.5 * gen_imgs + 0.5 fig, axs = plt.subplots(r, c) cnt = 0 for i in range(r): for j in range(c): axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray') axs[i, j].axis('off') cnt += 1 fig.savefig("./images/mnist_%d.png" % epoch) plt.close()if __name__ == '__main__': dc = DCGAN() dc.init_model() dc.train(epochs=4000, batch_size=32)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN]]></title>
    <url>%2F2018%2F04%2F12%2FDeepLearning%2F%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%2F</url>
    <content type="text"><![CDATA[自动编码器定义自动编码器是一种默认数据的压缩算法，一种使用神经网络学习数据值编码的无监督方式 应用 数据去噪 进行降维可视化 自编码器可以学习到比PCA等技术更好的数据投影 原理 搭建编码器 搭建解码器 设定一个损失函数，用以衡量由于压缩而损失掉的信息 编码器和解码器一般都是参数化的方程，并关于损失函数可导，通常情况是用神经网络 类别 普通编码器 编解码网络使用全连接层 多层自编码器 卷积自编码器 编解码器使用卷积结构 正则化自编码器 降噪自编码器 流程 初始化自编码器结构 定义编码器：输出32个神经元，使用relu激活函数，（32这个值可以自己制定） 定义解码器：输出784个神经元，使用sigmoid函数，（784这个值是输出与原图片大小一致） 损失：每个像素值的交叉熵损失（输出为sigmoid值(0,1)，输入图片要进行归一化(0,1)） 训练自编码器 获取数据，并进行归一化处理以及形状修改 模型输入输出训练 指定迭代次数 指定每批次数据大小 是否打乱数据 验证集合 显示自编码前后效果比对 Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189from keras.layers import Input, Dense, Conv2D, MaxPool2D, UpSampling2Dfrom keras.models import Modelfrom keras.datasets import mnistimport numpy as npimport matplotlib.pylab as pltclass AutoEncoder(object): ''' 自动编码器 ''' def __init__(self, model_type='normal', mode=None): self.encodeing_dim = 32 self.decodeing_dim = 784 self.model_type = model_type self.model = &#123;'normal': self.auto_encoder_model(), 'depth': self.depth_auto_encoder_model(), 'conv': self.conv_auto_encoder_model()&#125; self.mode = mode def auto_encoder_model(self): ''' 普通自编码器的结构 :return: ''' # 编码器输入结构 input_img = Input(shape=(784,)) # 定义编码器：输出32个神经元，使用relu激活函数 encoder = Dense(self.encodeing_dim, activation='relu')(input_img) # 定义解码器：输出784个神经元，使用sigmoid函数(需要进行交叉熵损失计算所以用sigmoid) decoder = Dense(self.decodeing_dim, activation='sigmoid')(encoder) # 定义完整的模型逻辑 auto_encoder = Model(inputs=input_img, outputs=decoder) auto_encoder.compile(optimizer='adam', loss='binary_crossentropy') return auto_encoder def depth_auto_encoder_model(self): ''' 深度自编码器结构 :return: ''' # 编码器输入结构 input_img = Input(shape=(28, 28, 1)) # 定义编码器：输出32个神经元，使用relu激活函数 encoder = Dense(128, activation='relu')(input_img) encoder = Dense(64, activation='relu')(encoder) encoder = Dense(32, activation='relu')(encoder) # 定义解码器：输出784个神经元，使用sigmoid激活函数(仅需要在输出时使用sigmoid) decoder = Dense(64, activation='relu')(encoder) decoder = Dense(128, activation='relu')(decoder) decoder = Dense(784, activation='sigmoid')(decoder) auto_encoder = Model(input=input_img, output=decoder) auto_encoder.compile(optimizer='Adam', loss='binary_crossentropy') return auto_encoder def conv_auto_encoder_model(self): ''' 卷积自编码器结构 :return: ''' # 编码器输入结构 input_img = Input(shape=(28,28,1)) # 定义编码器：输出32个神经元，使用relu激活函数 encoder = Conv2D(32, (3, 3), activation='relu', padding='SAME')(input_img) encoder = MaxPool2D((2, 2), padding='SAME')(encoder) encoder = Conv2D(32, (3, 3), activation='relu', padding='SAME')(encoder) encoder = MaxPool2D((2, 2), padding='SAME')(encoder) print(encoder) # 定义解码器：输出784个神经元，使用sigmoid激活函数(仅需要在输出时使用sigmoid) decoder = Conv2D(32, (3, 3), activation='relu', padding='SAME')(encoder) # 上采样层 decoder = UpSampling2D((2, 2))(decoder) decoder = Conv2D(32, (3, 3), activation='relu', padding='SAME')(decoder) decoder = UpSampling2D((2, 2))(decoder) decoder = Conv2D(1, (3, 3), activation='sigmoid', padding='SAME')(decoder) print(decoder) auto_encoder = Model(inputs=input_img, output=decoder) auto_encoder.compile(optimizer='Adam', loss='binary_crossentropy') return auto_encoder def train(self): ''' 训练自编码器 :return: ''' # 读取Mnist数据，并进行归一化处理以及形状修改 # (x_train, _), (x_test, _) = mnist.load_data() f = np.load('mnist.npz') x_train, _ = f['x_train'], f['y_train'] x_test, _ = f['x_test'], f['y_test'] x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 if self.model_type == 'conv': # 由于卷积层层要求，需要将数据转换成二维的[batch, h, w, c] # [60000, 28 ,28] =&gt; [60000, 28, 28, 1] x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) else: # 由于全连接层层要求，需要将数据转换成二维的[batch, feature] # [60000, 28 ,28] =&gt; [60000, 28*28] # np.prod(x_train.shape[1:])将shape后的数字累乘法 print(x_train.shape) print(len(x_train)) x_train = np.reshape(x_train, (len(x_train), np.prod(x_train.shape[1:]))) x_test = np.reshape(x_test, (len(x_test), np.prod(x_test.shape[1:]))) x_train_original = x_train x_test_original = x_test if self.mode == 'reduce_noisy': print('reduce_noisy') # 进行噪点数据处理 x_train_noisy = x_train + np.random.normal(loc=1.0, scale=1.0, size=x_train.shape) x_test_noisy = x_test + np.random.normal(loc=1.0, scale=1.0, size=x_test.shape) # 处理成0~1之间的数据 x_train = np.clip(x_train_noisy, 0., 1.) x_test = np.clip(x_test_noisy, 0., 1.) # 模型进行fit训练 # 指定迭代次数 # 指定每批数据大小 # 是否打乱数据 # 验证集合 self.model[self.model_type].fit(x_train, x_train_original, epochs=3, batch_size=256, shuffle=True, validation_data=(x_test, x_test_original) ) def display(self): ''' 显示前后效果对比 :return: ''' f = np.load('mnist.npz') x_train, _ = f['x_train'], f['y_train'] x_test, _ = f['x_test'], f['y_test'] if self.model_type == 'conv': # 卷积自编码器 x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) else: # 普通自编码器 x_test = np.reshape(x_test, (len(x_test), np.prod(x_test.shape[1:]))) x_test_original = x_test if self.mode == 'reduce_noisy': print('show noisy') x_test = x_test + np.random.normal(loc=10.0, scale=10.0, size=x_test.shape) decode_imgs = self.model[self.model_type].predict(x_test) plt.figure(figsize=(20, 4)) # 显示5张结果 n = 5 for i in range(n): # 显示编码前结果 ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # 显示编解码后的结果 ax = plt.subplot(2, n, i + n + 1) plt.imshow(decode_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show()if __name__ == '__main__': ae = AutoEncoder(mode='reduce_noisy') ae.train() ae.display()]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Auto Encoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迁移学习]]></title>
    <url>%2F2018%2F04%2F12%2FDeepLearning%2F%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[迁移学习定义 迁移学习就是利用数据、任务或模型之间的相似性，将在旧的领域学习过或训练好的模型，应用于新的领域这样的一个过程。 两个任务的输入属于同一性质：要么同是图像、要么同是语音或其他 使用场景 当我们有海量的数据资源时，可以不需要迁移学习，机器学习系统很容易从海量数据中学习到一个鲁棒性很强的模型。但通常情况下，我们需要研究的领域可获得的数据极为有限，在少量的训练样本上精度极高，但是泛化效果极差。 训练成本，很少去从头开始训练一整个深度卷积网络，从头开始训练一个卷积网络通常需要较长时间且依赖于强大的 GPU 计算资源。 方法 最常见的称呼叫做fine tuning,即微调 已训练好的模型，称之为Pre-trained model 通常我们需要加载以训练好的模型，这些可以是一些机构或者公司在ImageNet等类似比赛上进行训练过的模型。TensorFlow同样也提供了相关模型地址：https://github.com/tensorflow/models/tree/master/research/slim 过程这里我们举一个例子，假设有两个任务A和B，任务 A 拥有海量的数据资源且已训练好，但并不是我们的目标任务，任务 B 是我们的目标任务。下面的网络模型假设是已训练好的1000个类别模型而B任务假设是某个具体场景如250个类别的食物识别，那么该怎么去做 建立自己的网络，在A的基础上，修改最后输出结构，并加载A的模型参数 根据数据大小调整 如果B任务数据量小，那么我们可以选择将A模型的所有的层进行freeze(可以通过Tensorflow的trainable=False参数实现)，而剩下的输出层部分可以选择调整参数训练 如果B任务的数据量大，那么我们可以将A中一半或者大部分的层进行freeze,而剩下部分的layer可以进行新任务数据基础上的微调]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典分类卷积网络结构]]></title>
    <url>%2F2018%2F04%2F12%2FDeepLearning%2F%E7%BB%8F%E5%85%B8%E5%88%86%E7%B1%BB%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[经典分类卷积网络结构 LeNet-5网络结构 激活层默认不画网络图当中，这个网络结构当时使用的是sigmoid和Tanh函数，还没有出现Relu函数 将卷积、激活、池化视作一层，即使池化没有参数 参数形状 &amp;nbsp shape size parameters input (32,32,3) 3072 0 Conv1(f=5,s=1) (28,28,6) 4704 450+6 Pool1 (14,14,6) 1176 0 Conv2(f=5,s=1) (10,10,16) 1600 2400+16 Pool2 (5,5,16) 400 0 FC3 (120,1) 120 48000+120 FC4 (84,1) 84 10080+84 output:softmax (10,1) 10 840+10 FC层特征变化不宜过快，容易造成过拟合 AlexNet 总参数量：60M=6000万，5层卷积+3层全连接 使用了非线性激活函数：ReLU 防止过拟合的方法：Dropout，数据扩充(Data augmentation) 批标准化层的使用(BN) 卷积网络结构的优化 NIN:引入1 * 1卷积 VGG，斩获2014年分类第二（第一是GoogLeNet），定位任务第一。 参数量巨大,140M = 1.4亿 19layers VGG 版本 VGG16 VGG19 GoogleNet，2014年比赛冠军的model，这个model证明了一件事：用更多的卷积，更深的层次可以得到更好的结构。（当然，它并没有证明浅的层次不能达到这样的效果） 500万的参数量 22layers 引入了Inception模块 Inception V1 Inception V2 Inception V3 Inception V4 Inception 结构MLP卷积(1 x 1卷积) 目的:提出了一种新的深度网络结构，称为“网络中的网络”（NIN），增强接受域内局部贴片的模型判别能力。 做法 对于传统线性卷积核：采用线性滤波器，然后采用非线性激活。 提出MLP卷积取代传统线性卷积核 作用或优点： 多个1x1的卷积核级联加上配合激活函数，将feature map由多通道的线性组合变为非线性组合（信息整合），提高特征抽象能力（Multilayer Perceptron,缩写MLP,就是一个多层神经网络） 1x1的卷积核操作还可以实现卷积核通道数的降维和升维，实现参数的减小化 1 x 1卷积可以看作是对n个通道进行了线性组合，对应通道对应权重进行卷积并求所有的加权和。通常在卷积之后会加入非线性激活函数，在这里之后加入激活函数，就可以理解成一个简单的MLP网络了。 通道数变化1x1、3x3、5x5的Filter也能对通道数进行更改，1x1的参数量较少 保持通道数不变 提升通道数 减少通道数 Inception层这个结构其实还有名字叫盗梦空间结构 目的： 代替人手工去确定到底使用1x1,3x3,5x5还是是否需要max_pooling层，由网络自动去寻找适合的结构。并且节省计算。 特点： 是每一个卷积/池化最终结果的长、宽大小一致 特殊的池化层，需要增加padding，步长为1来使得输出大小一致，并且选择32的通道数 最终结果28 x 28 x 256 使用更少的参数，达到跟AlexNet或者VGG同样类似的输出结果 Inception改进改进目的：减少计算，如5x5卷积的计算量 上面的参数：5 x 5 x 32 x 192 =153600 下面的参数：192 x 16 + 5 x 5 x 16 x 32 = 3072 + 12800 = 15872 所以上面的结构会需要大量的计算，我们把这种改进的结构称之为网络的”瓶颈”,网络缩小后扩大。这样改变并没有太大影响网络的性能和效果。GoogleNet就是如此，获得了非常好的效果。所以合理的设计网络当中的Inception结构能够减少计算，实现更好的效果。完整结构如下]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Convolution Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——基于Estimator的CNNMnist手写数字识别]]></title>
    <url>%2F2018%2F04%2F08%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8EEstimator%E7%9A%84CNNMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[项目实战——基于Estimator的CNNMnist手写数字识别流程分析 构建CNN Mnist分类器 卷积层 1：应用 32 个 5x5 过滤器（提取 5x5 像素的子区域），并应用 ReLU 激活函数 池化层 1：使用 2x2 过滤器和步长 2（指定不重叠的池化区域）执行最大池化运算 卷积层 2：应用 64 个 5x5 过滤器，并应用 ReLU 激活函数 池化层 2：同样，使用 2x2 过滤器和步长 2 执行最大池化运算 密集层 1：包含 1024 个神经元，其中丢弃正则化率为 0.4（任何指定元素在训练期间被丢弃的概率为 0.4） 密集层 2（对数层）：包含 10 个神经元，每个数字目标类别 (0–9) 对应一个神经元。 生成预测 每个样本的预测类别：一个介于 0 到 9 之间的数字。 每个样本属于每个可能的目标类别的概率：样本属于以下类别的概率：0、1、2 等。 API tf.layers 模块包含用于创建上述 3 种层的方法： conv2d()。构建一个二维卷积层。接受的参数为过滤器数量、过滤器核大小、填充和激活函数。 max_pooling2d()。构建一个使用最大池化算法的二维池化层。接受的参数为池化过滤器大小和步长。 dense()。构建密集层。接受的参数为神经元数量和激活函数。 上述这些方法都接受张量作为输入，并返回转换后的张量作为输出。这样可轻松地将一个层连接到另一个层：只需从一个层创建方法中获取输出，并将其作为输入提供给另一个层即可。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_function# Importsimport numpy as npimport tensorflow as tfimport sslfrom tensorflow.examples.tutorials.mnist import input_data# 全局取消证书验证ssl._create_default_https_context = ssl._create_unverified_contexttf.logging.set_verbosity(tf.logging.INFO)# Our application logic will be added heredef cnn_model_fn(features, labels, mode): ''' :param features:features :param labels:labels :param mode: 'train','predict','eval' :return: ''' # 输入层 # -1缺省值自适应动态计算, 输入图片形状为28*28*1 input_layer = tf.reshape(features['x'], [-1, 28, 28, 1]) # 卷积层1 conv1 = tf.layers.conv2d( inputs=input_layer, filters=32, kernel_size=[5, 5], strides=(1, 1), padding='same', # case-insensitive activation=tf.nn.relu ) # 池化层1 pool1 = tf.layers.max_pooling2d( inputs=conv1, pool_size=[2, 2], strides=2 # Can be a single integer to specify the same value for all spatial dimensions ) # 卷积层2 conv2 = tf.layers.conv2d( inputs=pool1, filters=64, kernel_size=[5, 5], padding='same', activation=tf.nn.relu ) # 池化层2 pool2 = tf.layers.max_pooling2d( inputs=conv2, pool_size=[2, 2], strides=2 ) # 密集层fc pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) dense = tf.layers.dense( inputs=pool2_flat, units=1024, activation=tf.nn.relu ) # dropout =&gt; [batch_size, 1024] dropout = tf.layers.dropout( inputs=dense, rate=0.4, # rate指定丢弃率, 0.4 即表示40%的元素会在训练期间被随机丢弃 training= mode == tf.estimator.ModeKeys.TRAIN # training传入布尔值, 即为TRAIN模式时才会执行丢弃操作 ) # 对数层 logits = tf.layers.dense( inputs=dropout, units=10 ) # 生成预测并返回EstimatorSpec对象 predictions = &#123; "classes": tf.argmax(input=logits, axis=1), "probabilities":tf.nn.softmax(logits, name="softmax_tensor") &#125; if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions) # 计算损失(类别交叉熵/负对数似然率) loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits) # 配置训练操作并返回EstimatorSpec对象 if mode == tf.estimator.ModeKeys.TRAIN: optimizer = tf.train.GradientDescentOptimizer(0.01) train_op = optimizer.minimize( loss = loss, global_step=tf.train.get_global_step() ) return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op) # 添加评估指标并返回EstimatorSpec对象 eval_metric_ops = &#123; "accuracy": tf.metrics.accuracy( labels=labels, predictions=predictions['classes'] ) &#125; return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=eval_metric_ops)def main(unused_argv): # 加载train data与eval data # mnist = tf.contrib.learn.datasets.load_dataset('mnist') mnist = input_data.read_data_sets('./data/mnist') train_data = mnist.train.images train_labels = np.asarray(mnist.train.labels, dtype=np.int32) eval_data = mnist.test.images eval_labels = np.asarray(mnist.test.labels, dtype=np.int32) # 创建评估器 mnist_classifier = tf.estimator.Estimator( model_fn=cnn_model_fn, model_dir="./model/cnnmnist" ) # 为预测设置日志记录 tensors_to_log = &#123;'probabilities':'softmax_tensor'&#125; logging_hook = tf.train.LoggingTensorHook( tensors=tensors_to_log, every_n_iter=50 ) # 训练模型 train_input_fn = tf.estimator.inputs.numpy_input_fn( x=&#123;"x": train_data&#125;, y=train_labels, batch_size=10, num_epochs=None, shuffle=True ) mnist_classifier.train( input_fn=train_input_fn, steps=20000, hooks=[logging_hook] ) # 评估模型打印结果 eval_input_fn = tf.estimator.inputs.numpy_input_fn( x=&#123;"x":eval_data&#125;, y=eval_labels, num_epochs=1, shuffle=False ) eval_results =mnist_classifier.evaluate(input_fn=eval_input_fn) print(eval_results)if __name__ == "__main__": tf.app.run()]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Convolution Neural Network</tag>
        <tag>TensorFlow</tag>
        <tag>Mnist</tag>
        <tag>TensorFlow Estimator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——CNNMnist手写数字识别]]></title>
    <url>%2F2018%2F04%2F07%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94CNNMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[项目实战——CNNMnist手写数字识别网络设计 第一层 卷积：32个filter、大小5x5、strides=1、padding=“SAME” 激活：ReLU 池化：大小2x2、strides=2 第二层 卷积：64个filter、大小5x5、strides=1、padding=“SAME” 激活：ReLU 池化：大小2x2、strides=2 全连接层 经过每一层图片数据大小的变化需要确定，Mnist输入的每批次若干图片数据大小为[None, F*F]，如果要经过卷积计算，需要变成[None, 28, 28, 1] 第一层 卷积：[None, 28, 28, 1]———&gt;[None, 28, 28, 32] 权重数量：[5, 5, 1 ,32] 偏置数量：[32] 激活：[None, 28, 28, 32]———&gt;[None, 28, 28, 32] 池化：[None, 28, 28, 32]———&gt;[None, 14, 14, 32] 第二层 卷积：[None, 14, 14, 32]———&gt;[None, 14, 14, 64] 权重数量：[5, 5, 32 ,64] 偏置数量：[64] 激活：[None, 14, 14, 64]———&gt;[None, 14, 14, 64] 池化：[None, 14, 14, 64]———&gt;[None, 7, 7, 64] 全连接层 [None, 7, 7, 64]———&gt;[None, 7 7 64] 权重数量：[7 7 64, 10]，由分类别数而定 偏置数量：[10]，由分类别数而定 流程分析 准备手写数字数据 实现网络结构建模 全连接层得到输出类别预测 计算损失值并优化 计算准确率 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datatf.app.flags.DEFINE_string('mnist_path','./data/mnist/','mnist数据路径')tf.app.flags.DEFINE_integer('max_step', '500', '最大迭代次数')tf.app.flags.DEFINE_string('model_path','./model/cnnmnist/','保存和加载的模型路径')tf.app.flags.DEFINE_string('model_name','fc_nn_model','保存和加载的模型名称')tf.app.flags.DEFINE_string('events_path', './tmp/cnnmnist/','events文件路径')FLAGS = tf.app.flags.FLAGSclass Mnist(object): def __init__(self): self.W = None self.b = None def get_mnist(self): '''获取mnist数据''' mnist = input_data.read_data_sets(FLAGS.mnist_path, one_hot=True) return mnist def weight_variable(self,shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(self,shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def inputs(self): '''准备数据''' # x [None, 28*28]; y_true [None, 10] X = tf.placeholder(tf.float32, [None, 28 * 28]) y_true = tf.placeholder(tf.float32, [None, 10]) return X, y_true def conv_model(self,x): ''' 卷积网络结构建模 :return: x, y_true, y_predict ''' # 1.conv1 with tf.variable_scope('conv1'): # 随机初始化权重[5, 5, 1, 32](其中的1为输入的通道数), 偏置[32] w_conv1 = self.weight_variable([5, 5, 1, 32]) b_conv1 = self.bias_variable([32]) # 首先进行卷积计算, K=32, F=5, S=1, P="SAME" # x [None, 784] =&gt; [None, 28, 28, 1] x_conv1 =&gt; [None, 28, 28, 32] # -1 can also be used to infer the shape x_conv1_reshape = tf.reshape(x, [-1, 28, 28, 1]) # input =&gt; 4D x_conv1 = tf.nn.conv2d(x_conv1_reshape, w_conv1, strides=[1, 1, 1, 1], padding="SAME") + b_conv1 # 进行激活函数计算 # x_relu1 =&gt; [None, 28, 28, 32] x_relu1 = tf.nn.relu(x_conv1) # 进行池化层计算 # F=2, S=2 # [None, 28, 28, 32] =&gt; [None, 14, 14, 32] x_pool1 = tf.nn.max_pool(x_relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") # 2.conv2 # input =&gt; [None, 14, 14, 32] with tf.variable_scope('conv2'): # 每个filter带32张5*5的观察权重，一共有64个filter # 随机初始化权重[5, 5, 32, 64](其中的32为输入的通道数), 偏置[64] w_conv2 = self.weight_variable([5, 5, 32, 64]) b_conv2 = self.bias_variable([64]) # 首先进行卷积计算, K=64, F=5, S=1, P="SAME" # x [None, 14, 14, 32]; x_conv2 =&gt; [None, 14, 14, 64] # input =&gt; 4D x_conv2 = tf.nn.conv2d(x_pool1, w_conv2, strides=[1, 1, 1, 1], padding="SAME") + b_conv2 # 进行激活函数计算 # x_relu2 =&gt; [None, 14, 14, 64] x_relu2 = tf.nn.relu(x_conv2) # 进行池化层计算 # F=2, S=2 # [None, 14, 14, 64] =&gt; [None, 7, 7, 64] x_pool2 = tf.nn.max_pool(x_relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") # 3.fc # 每个样本输出类别的个数10个结果 # input =&gt; x_pool2 [None, 7, 7, 64] # 矩阵运算 [None, 7*7*64] * [7*7*64, 10] + [10] = [None, 10] with tf.variable_scope('fc'): # 确定全连接权重和偏置 w_fc = self.weight_variable([7 * 7 * 64, 10]) b_fc = self.bias_variable([10]) # 对上一层的输出结果的形状进行处理成2维形状 x_fc = tf.reshape(x_pool2, [-1, 7 * 7 * 64]) # 进行全连接运算 y_predict = tf.matmul(x_fc, w_fc) + b_fc return y_predict def cross_entropy(self, y_true, y_pre): '''softmax回归以及交叉熵损失计算''' # labels=标签值; logits=样本加权之后的值 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pre)) return loss def sgd_op(self, loss): # optimizer = tf.train.AdamOptimizer(0.1).minimize(loss) optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss) return optimizer def evaluation(self, y_true, y_pre): equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pre, 1)) accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)) return accuracy def merge_summary(self, loss, accuracy): '''收集合并变量''' # 收集损失和准确率 tf.summary.scalar('loss', loss) tf.summary.scalar('acc', accuracy) # 合并所有变量op merged = tf.summary.merge_all() return merged def train(self): # 创建默认图 g = tf.get_default_graph() with g.as_default(): '''将g图作为默认图''' # 获取数据 mnist = self.get_mnist() X, y_true = self.inputs() # 全连接层神经网络计算 y_pre = self.conv_model(X) # 交叉熵损失 loss = self.cross_entropy(y_true, y_pre) # 梯度下降优化 optimizer = self.sgd_op(loss) # 评估准确率 accuracy = self.evaluation(y_true, y_pre) # 收集合并变量 merged = self.merge_summary(loss, accuracy) # 创建saver对象 saver = tf.train.Saver() # 开启会话 with tf.Session() as sess: # 初始化变量 sess.run(tf.global_variables_initializer()) # 检查模型 checkpoint = tf.train.latest_checkpoint(FLAGS.model_path) if checkpoint: print('restore', checkpoint) # 加载模型 saver.restore(sess, checkpoint) # 创建一个events文件实例 file_writer = tf.summary.FileWriter(FLAGS.events_path,graph=sess.graph) for i in range(FLAGS.max_step): batch_xs, batch_ys = mnist.train.next_batch(50) sess.run(optimizer, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;) print('训练第%d步的准确率为:%f, 损失为%f' % ( i + 1, sess.run(accuracy, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;), sess.run(loss, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;))) # 运行合并变量op，写入events文件中 summary = sess.run(merged, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;) file_writer.add_summary(summary, i) if i % 100 == 0: saver.save(sess, FLAGS.model_path+FLAGS.model_name)if __name__ == '__main__': m = Mnist() m.train()]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Convolution Neural Network</tag>
        <tag>TensorFlow</tag>
        <tag>Mnist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——卷积神经网络]]></title>
    <url>%2F2018%2F04%2F07%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——卷积神经网络卷积神经网络的结构神经网络的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层(Convolutional Layer)和池化层(pooling layer，又叫下采样层)以及激活层。 卷积层：通过在原始图像上平移来提取特征 激活层：增加非线性分割能力 池化层：减少学习的参数，降低网络的复杂度(最大池化和平均池化) 为了能够达到分类效果，还会有一个全连接层(Full Connection)也就是最后的输出层，进行损失计算并输出分类结果。 卷积层(Convolution Layer)卷积神经网络中每层卷积层由若干卷积单元(卷积核)组成，每个卷积单元的参数都是通过反向传播算法最佳化得到的。卷积运算的目的是特征提取，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。 卷积核(Filter) 卷积核大小 卷积核步长 卷积核个数 卷积核零填充大小 卷积核的大小卷积核即是通过权重与偏置将原特征进行加权运算的中间量。卷积的计算过程如下图通常卷积核大小从11、33、5*5中选取，是经过实验验证的较好的效果。 卷积核的步幅通关卷积核转化整张图片就需要平移卷积核，而每次移动的距离即是步幅(Stride) 卷积核的个数多个卷积核转化整张图片的结果也是一一对应，即可以获得多个结果。不同的卷积核的权重与偏置均不同，即随机初始化的参数。 卷积核的零填充大小Filter窗口大小和移动步幅在超过图片像素宽度时，可以通过零填充(Padding)来解决这个问题。即在图片像素外围填充一圈值为0的像素。 卷积的计算 输入体积大小$H_1W_1D_1$ 图片大小$H_1*W_1$ 卷积核数量$K$ 卷积核大小$F$ 移动步幅$S$ 零填充大小$P$ 输出体积大小$H_2W_2D_2$ $H_2=\frac{H_1-F+2P}{S}+1$ $W_2=\frac{W_1-F+2P}{S}+1$ $D_2=K$ 多通道图片的卷积如果是一张彩色图片，那么就有三种表分别为R，G，B。原本每个卷积核只需要带一组权重与偏置，现在需要带三组偏置，即权重数为原来的三倍，但任然仅是得出一张结果。 卷积层API tf.nn.conv2d(input, filter, strides=, padding=, name=None) 计算给定4-D input和filter张量的2维卷积 input：给定的输入张量，具有[batch, height, width, channel]，类型为float32,64 filter：指定的输入张量，[filter_height, filter_width, in_channels, out_channels] strides：strides = [1, stride, stride, 1]，步幅 padding：“SAME”，“VALID” TensorFlow之”SAME”填充与“VALID”填充TensorFlow的零填充方式有两种方式，SAME和VALID SAME：越过边缘取样，取样的面积和输入图像的像素宽度一致。公式：$ceil(\frac{H}{S})$ H为输入的图片的高或者宽，S为步长 无论过滤器的大小是多少，零填充的数量由API自动计算 卷积API设置”SAME”之后，如果步长为1，输出高宽与输入大小一样 VALID：不越过边缘取样，取样的面积小于输入的图像的像素宽度。不填充。 激活函数ReLUReLU函数图像如下 Relu优点 有效解决梯度爆炸问题 计算速度非常快，只需要判断输入是否大于0。SGD(批梯度下降)的求解速度速度远快于sigmoid和tanh sigmoid缺点 采用sigmoid等函数，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。在深层网络中，sigmoid函数反向传播时，很容易就会出现梯度消失的情况激活函数API tf.nn.relu(features, name=None) features：卷积后加上偏置的结果 return 结果 池化层(Pooling Layer)池化层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数数量。池化的方法有很多，通常采用最大池化 max_pooling：取池化窗口的最大值 avg_pooling：取池化窗口的平均值 池化层计算 输入图片大小$H_1*W_1$ 池化窗口大小$F$ 移动步幅$S$ 零填充大小$P$ 输出图片大小$H_2*W_2$ $H_2=\frac{H_1-F+2P}{S}+1$ $W_2=\frac{W_1-F+2P}{S}+1$ 池化计算向上取整 池化层API tf.nn.max_pool(value, ksize=, strides=, padding=, name=None) 输入上执行最大池数 value：4-D Tensor形状[batch, height, width, channels] channel：并不是原始图片的通道数，而是Filter的个数 ksize：池化窗口大小，[1, ksize, ksize, 1] strides：步长大小，[1, strides, strides, 1] padding：“SAME”，“VALID”，使用的填充算法类型，默认使用“SAME” 全连接层(Full Connection)前面的卷积和池化相当于做特征工程，最后的全连接层(Softmax)在整个卷积神经网络中起到了“分类器”的作用]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>Convolution Neural Network</tag>
        <tag>TensorFlow</tag>
        <tag>Reader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——Mnist手写数字识别]]></title>
    <url>%2F2018%2F04%2F06%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94Mnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[项目实战——Mnist手写数字识别数据集介绍文件说明： train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes) 特征值Mnist数据集可以从官网下载，网址：http://yann.lecun.com/exdb/mnist/下载下来的数据集被分成两部分：60000行的训练数据集(mnist.train)和10000行的测试数据集(mnist.test)。每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为$x_s$，把这些标签设为$y_s$。训练数据集和测试数据集都包含$x_s$和$y_s$，比如训练数据集的图片是mnist.train.images，训练数据集的标签是mnist.train.labels 其中的图片为黑白图片，每一张图片包含28像素*28像素。我们把这个数组展开成一个向量，长度是$28\times28=784$。因此在MNIST训练数据集中，mnist.train.images是一个形状为[60000, 784]的张量。 目标值MNIST中的每个图像都具有相应的标签，0到9之间的数字表示图像中绘制的数字。用的是one-hot编码，因此mnist.train.labels是一个形状为[60000, 10]的张量。 Mnist数据获取API在TensorFlow的examples.turorials.mnist目录下，TensorFlow框架提供了获取这个数据集的接口，所以不需要自行读取。 from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(path, one_hot=True) mnist.train.images/mnist.train.labels mnist.test.images/mnist.test.labels 全连接层计算 tf.matmul(a, b, name=None) + bias return 全连接结果，供交叉损失运算 tf.train.GradientDescentOptimizer(learning_rate) 梯度下降 learning_rate：学习率(需要指定) method minimize(loss)：最小优化损失 完善模型功能 准确率计算 equal_list = tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1)) accuracy = tf.reduce_mean(tf.equal_list, tf.float32) 变量tensorboard显示 模型保存加载 模型预测结果输出 Mnist手写数字识别网络设计采用单层网络，即最后一个输出层的神经网络，也称之为全连接(full connected)层神经网络 流程分析 准备数据 全连接层结果计算 损失计算与优化 完善模型功能 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datatf.app.flags.DEFINE_string('mnist_path','./data/mnist/','mnist数据路径')tf.app.flags.DEFINE_integer('max_step', '5000', '最大迭代次数')tf.app.flags.DEFINE_string('model_path','./model/mnist/','保存和加载的模型路径')tf.app.flags.DEFINE_string('model_name','fc_nn_model','保存和加载的模型名称')tf.app.flags.DEFINE_string('events_path', './tmp/mnist/','events文件路径')FLAGS = tf.app.flags.FLAGSclass Mnist(object): def __init__(self): self.W = None self.b = None def get_mnist(self): '''获取mnist数据''' mnist = input_data.read_data_sets(FLAGS.mnist_path, one_hot=True) return mnist def inputs(self): '''准备数据''' # x [None, 28*28]; y_true [None, 10] X = tf.placeholder(tf.float32, [None, 28 * 28]) y_true = tf.placeholder(tf.float32, [None, 10]) return X, y_true def inference(self, X): '''全连接层神经网络计算''' # 类别:10 =&gt; 全连接层:10个神经元 # 参数W:[28*28, 10]; b:[10] # 全连接层神经网络的计算公式:[None, 28*28] * [28*28, 10] + [10] = [None, 10] # 随机初始化权重偏置参数，这些是需要优化的参数，必须使用变量op去定义 self.W = tf.Variable(tf.random_normal([784, 10], mean=0.0, stddev=1.0)) self.b = tf.Variable(tf.random_normal([10], mean=0.0, stddev=1.0)) # fc层的计算 # y_pre [None, 10] y_pre = tf.matmul(X, self.W) + self.b return y_pre def cross_entropy(self, y_true, y_pre): '''softmax回归以及交叉熵损失计算''' # labels=标签值; logits=样本加权之后的值 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pre)) return loss def sgd_op(self, loss): optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss) return optimizer def evaluation(self, y_true, y_pre): equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pre, 1)) accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)) return accuracy def merge_summary(self, loss, accuracy): '''收集合并变量''' # 收集损失和准确率 tf.summary.scalar('loss', loss) tf.summary.scalar('acc', accuracy) # 收集权重和偏置 tf.summary.histogram('W', self.W) tf.summary.histogram('b', self.b) # 合并所有变量op merged = tf.summary.merge_all() return merged def train(self): # 创建默认图 g = tf.get_default_graph() with g.as_default(): '''将g图作为默认图''' # 获取数据 mnist = self.get_mnist() X, y_true = self.inputs() # 全连接层神经网络计算 y_pre = self.inference(X) # 交叉熵损失 loss = self.cross_entropy(y_true, y_pre) # 梯度下降优化 optimizer = self.sgd_op(loss) # 评估准确率 accuracy = self.evaluation(y_true, y_pre) # 收集合并变量 merged = self.merge_summary(loss, accuracy) # 创建saver对象 saver = tf.train.Saver() # 开启会话 with tf.Session() as sess: # 初始化变量 sess.run(tf.global_variables_initializer()) # 检查模型 checkpoint = tf.train.latest_checkpoint(FLAGS.model_path) if checkpoint: print('restore', checkpoint) # 加载模型 saver.restore(sess, checkpoint) # 创建一个events文件实例 file_writer = tf.summary.FileWriter(FLAGS.events_path, graph=sess.graph) for i in range(FLAGS.max_step): batch_xs, batch_ys = mnist.train.next_batch(50) sess.run(optimizer, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;) print('训练第%d步的准确率为:%f, 损失为%f' % ( i + 1, sess.run(accuracy, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;), sess.run(loss, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;))) # 运行合并变量op，写入events文件中 summary = sess.run(merged, feed_dict=&#123;X: batch_xs, y_true: batch_ys&#125;) file_writer.add_summary(summary, i) if i % 100 == 0: saver.save(sess, FLAGS.model_path+FLAGS.model_name)if __name__ == '__main__': m = Mnist() m.train()]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Mnist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——TFRecords]]></title>
    <url>%2F2018%2F04%2F05%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94TFRecords%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——TFRecordsTFRecords文件介绍TFRecords其实是一种二进制文件，虽然它不如其他格式好理解，但是它能更好得利用内存，更方便复制和移动，并且不需要单独的标签文件文件格式 *.tfrecords 使用步骤 获取数据 将数据填入到Example协议内存块(protocol buffer) 将协议内存块序列化为字符串，并且通过tf.python_io.TFRecordWriter写入到TFRecord文件 Example结构解析这种结构很好地实现了数据和标签(训练的类别标签)或者其他属性数据存储在同一个文件中 tf.train.Example：协议内存块(protocol buffer)(协议内存块包含了字段 Features) Features：包含了一个 Feature 字段 Feature：包含要写入的数据、并指明数据类型 这是一个样本结构，批数据需要循环存入这样的结构 TFRecords文件写入API tf.python_io.TFRecordWriter(path) 写入TFRecords文件 path：TFRecords文件的路径 return 写入器对象 writer method方法 write(record)：向文件中写入一个example close()：关闭文件写入器 tf.train.Example(features=None) 写入tfrecords文件 features：tf.train.Features类型的特征实例 return example格式协议块 tf.train.Features(feature=None) 构建每个样本的信息键值对 feature：字典数据，key为要保存的名字，value为tf.train.Feature实例 return Features类型 tf.train.Feature(options) options：例如 bytes_list = tf.train.BytesList(value=[Bytes]) int64_list = tf.train.Int64List(value=[Value]) 支持存入的类型 tf.train.Int64List(value=[Value]) tf.train.BytesList(value=[Bytes]) tf.train.FloatList(value=[Value]) 1234example = tf.train.Example(features=tf.train.Features(feature=&#123; "image": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])), "label": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])), &#125;)) TFRecords文件读取API读取TFRecords文件的过程与其他文件一样，只不过需要有个解析Example的步骤。从TFRecords文件中获取数据，可以使用tf.TFRecordReader的tf.parse_single_example解析器。这个操作可以将Example协议内存块(protocol buffer)解析为张量 tf.parse_single_example(serialized, features=None, name=None) 解析一个单一的Example原型 serialized：标量字符串Tensor，一个序列化的Example feature：dict字典数据，键为读取的名字，值为FixedLenFeature return 一个键值对组成的字典，键为读取的名字 tf.FixedLenFeature(shape, dtype) shape：输入数据的形状，一般不指定，为空列表 dtype：输入数据类型，与存储进文件的类型要一致 类型只能是float32，int64，string 1234feature = tf.parse_single_example(values, features=&#123; "image": tf.FixedLenFeature([], tf.string), "label": tf.FixedLenFeature([], tf.int64)&#125;) Demo将读取后的CIFAR10二进制数据存储为TFRecord文件以及读取TFRecord文件为例，进行流程分析 二进制文件读取及解码流程 构造文件队列 读取数据并进行解码 处理数据形状，放入批处理队列 开启会话线程运行 123456789101112131415161718192021222324def read_and_decode(file_list): ''' 二进制文件读取及解码 ''' # 1.构造文件队列 file_queue = tf.train.string_input_producer(file_list) # 2.读取数据 # 2.1.实例化读取器，tf.FixedLengthRecordReader()，传入一个样本(1+3*32*32) reader = tf.FixedLengthRecordReader(1+3*32*32) # 2.2.传入文件队列，调用read方法，返回(key, value)元组 key, value = reader.read(file_queue) # 3.解码(按行)为tf.uint8 data = tf.decode_raw(value, tf.uint8) # 4.数据处理 # 4.1.切片分离标签与特征值，tf.slice(input_, begin, size, name=None) label = tf.cast(tf.slice(data, [0], [1]), tf.int32) image = tf.slice(data, [1], [3*32*32]) # 4.2.改变图像形状 image_major = tf.reshape(image, [3,32,32]) image_trans = tf.transpose(image_major, [1, 2, 0]) # 5.批处理(传入列表) label_batch, image_batch = tf.train.batch([label_batch, image_trans], batch_size=100, num_threads=1, capacity=100) return label_batch,image_batch 写入TFRecords文件流程 构造存储实例(写入器) tf.python_io.TFRecordWriter(path) path：TFRecords文件的路径(需要指定文件名) reutrn 写入器对象 writer method 方法 write(record)：向文件中写入一个example close()：关闭文件写入器 循环将数据填入到 Eaxmpel 协议内存块(protocol buffer) 关闭文件 123456789101112131415161718192021222324252627282930def write_to_tfrecords(image_batch,label_batch): ''' 写入TFRecords文件 ''' # 进行类型转换，转成tf.uint8，节省空间 label_batch = tf.cast(label_batch, tf.uint8) image_batch = tf.cast(image_batch, tf.uint8) # 1.构造tfrecords的存储实例 writer = tf.python_io.TFRecordWriter('./data/cifar.tfrecords') # 2.循环将每个样本写入到文件中 for i in range(10): # 准备特征值，特征值必须是bytes类型，tostring() # 需要取出真正的值而不是张量，所以要在会话中运行函数 image = image_batch[i].eval().tostring() # 准备目标值，int # eval() =&gt; [6] =&gt; 6 label = label_batch[i].eval()[0] # 绑定每个样本的属性 example = tf.train.Example(features=tf.train.Features(feature=&#123; 'image':tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])), 'label':tf.train.Feature(int64_list=tf.train.Int64List(value=[label])) &#125;)) # 写入每个样本的example writer.write(example.SerializeToString()) # 关闭文件 writer.close() 读取TFRecords文件流程 构造文件队列 tf.train.string_input_producer 读取数据并进行解析 tf.TFRecordReader读取TFRecords数据 tf.parse_single_example解析 数据解码 tf.decode_raw解码 类型是bytes类型需要解码 其他类型不需要解码 处理图片数据形状以及数据类型，加入处理队列 开启会话线程运行 1234567891011121314151617181920212223242526272829303132333435363738def read_tfrecords(): ''' 读取TFRecords的数据 ''' # 1.构造文件队列 file_queue = tf.train.string_input_producer(['./data/cifar.tfrecords']) # 2.构造tfrecords读取器，读取队列 reader = tf.TFRecordReader() # 默认读取一个样本 key, value = reader.read(file_queue) # tfrecords # 解析Example feature = tf.parse_single_example(value, features=&#123; 'image':tf.FixedLenFeature([],tf.string), 'label':tf.FixedLenFeature([],tf.int64) &#125;) # 取出feature里面的特征值和目标值 image = feature['image'] label = feature['label'] # 3.解码 # 对于image是一个bytes类型，所以需要decode_raw去解码成uint8张量 # 对于label是一个int类型，不需要解码 image = tf.decode_raw(image, tf.uint8) # 从原来的[32, 32, 3]的bytes形式直接变成[32, 32, 3] # 处理image形状和类型 image_reshape = tf.reshape(image, [32, 32, 3]) # 处理label形状和类型 label = tf.cast(label, tf.int32) # 批处理 image_batch, label_batch = tf.train.batch([image_reshape, label], batch_size=10, num_threads=1, capacity=10 ) return label,image_batch]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>TFRecords</tag>
        <tag>Reader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——图片数据]]></title>
    <url>%2F2018%2F04%2F04%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——图片数据图像基本知识图像三要素组成一张图片特征值所有的像素值，有三个维度：图片长度、图片宽度、图片通道数 灰度图：单通道(1)——黑白 RGB彩色图片：三通道(3)——RGB色彩通道 PNG图片：四通道(4)——RGB色彩通道与$\alpha$透明度通道 张量形状一张图片可以被表示成一个3D张量，即其形状为[height, width, channel]，height就表示高，width表示宽，channel表示通道数 单个图片：[height, width, channel] 多个图片：[batch, height, width, channel] batch表示一个批次的张量数量 图片特征值处理在进行图像识别的时候，每个图片样本的特征数量要保持相同，所以需要将所有的图片张量大小统一转换，另一方面，如果图片的像素量太大，通过这种方式适当减少像素的数量，减少训练的计算开销 tf.image.resize_image(image, size) 缩小放大图片 images：4-D形状[batch, height, width, channels]或3-D形状的张量[height, width, channels]的图片数据 size：1-D int32张量[new_height, new_width] 图像新的尺寸 返回4-D格式或者3-D格式图片4 数据格式 存储：uint8——节约空间 矩阵计算：float32——提高精度 NHWC与NCHW在读取设置图片形状的时候有两种格式 设置为“NHWC”时，排列顺序为[batch, height, width, channels] 设置为”NCHW”时，排列顺序为[batch, channels, height, width] 其中N表示这批图像有几张，H表示图像在竖直方向有多少像素，W表示水平方向像素数，C表示通道数TensorFlow默认[height, width, channel] 若1，2，3，4-红色；5，6，7，8-绿色；9，10，11，12-蓝色 如果通道在最低维度0[channel, height, width]，RGB三颜色分成三组，在第一维度上找到三个RGB颜色 如果通道在最高维度2[height, width, channel]，在第三维度上找到RGB三个颜色 转换API tf.reshape(tensor, shape, name=None) shape：需要改变成的形状 tf.transpose(a, perm=None) 修改维度的位置 a：数据 perm：形状的维度值下标列表 转换DemoNCHW =&gt; NHWC 默认形状为[channel height width] =&gt; tf.reshape(image,[channel, height, width]) 再使用tf.transpose，将刚才的数据[channel, height, width]变成TensorFlow默认的[height, width, channel] 12345678import tensorflow as tf# 3组2*2depth_major = tf.reshape([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [3, 2, 2])depth_trans = tf.transpose(depth_major, [1, 2, 0])with tf.Session() as sess:print(depth_major.eval())print('----------------')print(depth_trans.eval()) 输出1234567891011121314[[[ 1 2][ 3 4]][[ 5 6][ 7 8]][[ 9 10][11 12]]]----------------[[[ 1 5 9][ 2 6 10]][[ 3 7 11][ 4 8 12]]] Demo以读取批量图片为例，进行读取流程分析 构造图片文件队列 读取图片数据并进行解码 处理图片数据形状，放入批处理队列 开启会话线程运行 123456789101112131415161718192021222324252627282930313233343536def picread(file_list): # 1.构造文件队列 file_queue = tf.train.string_input_producer(file_list) # 2.读取文件数据 # 2.1.实例化读取器 reader = tf.WholeFileReader() # 2.2.调用读取器的read方法，传入文件队列，返回(key, value)元组 key, value = reader.read(file_queue) # 3.对样本内容进行对应格式解码 image = tf.image.decode_jpeg(value) # 4.图片处理 # 4.1.处理图片的大小，tf.image.resize_images(image, [height, width]) image_resize = tf.image.resize_images(image, [200, 200]) # 4.2.设置固定形状，可以使用静态形状API进行修改 image_resize.set_shape([200, 200, 3]) # 4.3.批处理图片数据，每个样本的形状必须定义，传入列表 image_batch = tf.train.batch([image_resize],batch_size=100,num_threads=1,capacity=100) return image_batchif __name__ == "__main__": # 生成路径/文件名的列表 filename = os.listdir('./data/cat/') file_list = [os.path.join('./data/cat/',file) for file in filename] image_batch = picread(file_list) # 开启会话 with tf.Session() as sess: # 创建线程协调器 coord = tf.train.Coordinator() # 开启子线程读取数据，并返回子线程实例 threads = tf.train.start_queue_runners(sess=sess, coord=coord) # 获取样本数据 print(sess.run(image_batch)) # 关闭子线程，回收 coord.request_stop() coord.join(threads)]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Reader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——文件读取]]></title>
    <url>%2F2018%2F04%2F04%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——文件读取文件读取方法有三种获取数据到TensorFlow程序的方法 QueueRunner：基于队列的输入管道从TensorFlow图形开头的文件中读取数据 Feeding：运行每一步时，Python代码提供数据 预加载数据：TensorFlow图中的张量包含所有数据(对于小数据集) 文件读取流程 第一阶段 构造文件名队列 第二阶段 读取与解码 第三阶段 批处理 这些操作需要启动运行这些队列操作的线程，以便在进行文件读取的过程中能够顺利进行入队出队操作 构造文件名将需要读取的文件的文件名放入文件名队列 tf.train.string_input_producer(string_tensor, shuffle=True) string_tensor：含有文件名+路径的1阶张量 num_epochs：次数。对于一个数据集来讲，运行一个epoch就是将这个数据集中的图片全部计算一遍 shuffle：在一个epoch内文件顺序是否打乱 return 文件队列 读取与解码从队列中读取文件内容，并进行解码操作读取文件内容文本文件默认一次读取一样，图片文件默认一次读取一张图片，二进制文件一次读取指定字节数(最好是一个样本的字节数)，TFRecords默认一次读取一个example。即阅读器默认每次只读取一个样本 tf.TextLineReader 阅读文本文件逗号分隔值(CSV)格式，默认按行读取 return 读取器实例 tf.WholeFileReader 用于读取图片文件 return 读取器实例 tf.FixedLengthRecordReader(record_bytes) 用于读取二进制文件，要读取每个记录是固定数量字节的二进制文件 record_bytes：整型，指定每次读取(一个样本)的字节数 return 读取器实例 tf.TFRecordReader： 读取TFRecords文件 return 读取器实例 读取器共同读取方法 read(file_queue) return Tensors元组(key文件名字, value默认的内容(一个样本)) 由于默认只会读取一个样本，所以如果想要进行批处理，需要使用tf.train.batch或tf.train.shuffle_batch进行批处理操作，便于之后指定每批次多个样本的训练 内容解码读取不同类型的文件，也应该对读取到的不同类型的内容进行相应的解码操作，解码成同一的Tensor格式 tf.decode_csv 解码文本文件内容 tf.image.decode_jpeg(contents) 将JPEG编码的图像解码为uint8张量 return uint8张量，3-D形状[height, width, channels] tf,image.decode_png(contents) 将PNG编码的图像解码为unit8张量 return uint8张量，3-D形状[height, width, channels] tf,decode_raw 解码二进制文件内容 与tf.FixedLengthRecordReader搭配使用，二进制读取为uint8类型 解码阶段，默认所有的内容都解码成tf.uint8类型，如果之后需要转换成指定类型则可使用tf.cast()进行相应转换 批处理解码之后，可以直接获取默认的一个样本内容，但是若想获取多个样本，需要加入到新的队列进行批处理 tf.train.batch(tensros, batch_size, num_threads=1, capacity=32, name=None) 读取指定大小(个数)的张量 tensors：可以使包含张量的列表，批处理的内容放到列表中 batch_size：从队列中读取的批处理大小 num_threads：进入队列的线程数 capacity：整数，队列中元素的最大数量 return tensors tf.train.shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, name=None) 线程操作以上用的队列都是tf.train.QueueRunner对象每个QueueRunner都负责一个阶段，tf.train.start_queuq_runners函数会要求图中的每个QueueRunner启动它的运行队列操作的线程。(需要在会话中开启) tf.train.start_queue_runners(sess=None, coord=None) 收集图中所有的队列线程，默认同时启动线程 sess：所在的会话 coord：线程协调器 return 返回所有线程 tf.train.Coordinator() 线程协调员，对线程进行管理和协调 request_stop()：请求停止 should_stop()：询问是否结束 join(threads=None, stop_grace_period_secs=120)：回收线程 return 线程协调员实例]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Reader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——线性回归(OOP)]]></title>
    <url>%2F2018%2F04%2F04%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(OOP)%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——线性回归(OOP) 1 准备好数据集：y = 2.0x + 3.0 100个样本 2 建立线性模型随机初始化W1和b1y = W·X + b，目标：求出权重W和偏置b 3 确定损失函数（预测值与真实值之间的误差）-均方误差 4 梯度下降优化损失：需要指定学习率（超参数）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import tensorflow as tf# 定义命令行参数tf.app.flags.DEFINE_integer('max_step', '100', '最大迭代次数')tf.app.flags.DEFINE_string('model_path', './model/lr/', '保存和加载的模型路径')tf.app.flags.DEFINE_string('model_name', 'lr_model', '保存和加载的模型名称')FLAGS = tf.app.flags.FLAGSclass LinearRegression(): def __init__(self): self.W = None self.b = None def inputs(self): '''获取数据''' X = tf.random_normal([100, 1], mean=2, stddev=2, name='original_data_X') y_true = tf.matmul(X, [[2.0]]) + 3.0 return X, y_true def inference(self, X): # 2.建立线性模型 # y = WX + b # 3.随机初始化W1和b1 # 新增命名空间：线性模型 with tf.variable_scope('linear_model'): self.W = tf.Variable(tf.random_normal([1, 1]), name='W') self.b = tf.Variable(tf.random_normal([1, 1]), name='b') y_pre = tf.matmul(X, self.W) + self.b return y_pre def error_func(self, y_true, y_pre): '''损失''' error = tf.reduce_mean(tf.square(y_true - y_pre), name='error_op') return error def sgd_op(self, error): '''随机梯度下降''' optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(error) return optimizer def merge_summary(self, error): '''收集变量并合并''' # 收集变量 tf.summary.scalar('error', error) tf.summary.histogram('W', self.W) tf.summary.histogram('b', self.b) # 合并变量 merge = tf.summary.merge_all() return merge def train(self): # 创建默认图 g = tf.get_default_graph() with g.as_default(): '''将g图作为默认图''' # 获取数据 X, y_true = self.inputs() # 建立模型 y_pre = self.inference(X) # 损失 error = self.error_func(y_true, y_pre) # 梯度下降优化 optimizer = self.sgd_op(error) # 收集合并数据 merge = self.merge_summary(error) # 创建saver对象 saver = tf.train.Saver() # 开启会话 with tf.Session() as sess: # 初始化变量 sess.run(tf.global_variables_initializer()) print('初始化的权重为%f, 偏置为%f' % (self.W.eval(), self.b.eval())) # print(os.getcwd()) # 调用get_checkpoint_state查看模型路径和所有文件路径 checkpoint = tf.train.latest_checkpoint(FLAGS.model_path) if checkpoint: print('Restoring', checkpoint) # 加载模型 saver.restore(sess, checkpoint) # 创建事件文件 file_writer = tf.summary.FileWriter('tmp', graph=sess.graph) for i in range(FLAGS.max_step): # 使用sgd进行优化 sess.run(optimizer) print('迭代第%d次后的损为%f, 权重为%f, 偏置为%f' % (i, error.eval(), self.W.eval(), self.b.eval())) # 运行收集变量的结果 summary = sess.run(merge) file_writer.add_summary(summary, i) if i % 50 == 0: print('save') # 保存模型 saver.save(sess, FLAGS.model_path+FLAGS.model_name)def main(argv): print('这是main函数') print(FLAGS.model_path) lr = LinearRegression() lr.train()if __name__ == '__main__': # 通过tf.app.run()启动main(argv)函数 tf.app.run()]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>Linear Regression</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——二进制数据]]></title>
    <url>%2F2018%2F04%2F04%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——二进制数据CIFAR10二进制数据集介绍CIFAR-10数据集是由10个类的60000个32x32彩色图像组成，每个类有6000个图像。有50000个悬链图像和10000个测试图像。数据集分为五个训练批次和一个测试批次，每个批次有10000个图像。测试批次包含来自每个类别的恰好1000个随机选择的图像。训练批次以随机顺序包含剩余图像，但一些训练批次可能包含来自一个类别的图像比另一个更多。总体来说，五个训练集之和包含来自每个类的正好5000张图片。一下是数据集中的类，以及来自每个类的10个随机图像，这些类完全相互排至。汽车和卡车之间没有重叠。“汽车”包括轿车、SUV，这类东西。“卡车”只包括大卡车。都不包括皮卡车。https://www.cs.toronto.edu/~kriz/cifar.html 二进制版本数据文件 二进制版本包含文件data_batch_1.bin，data_batch_2.bin，…，data_batch_5.bin以及test_batch.bin。这些文件中的每一个格式如下，数据中每个样本包含了特征值和目标值 &lt;1×标签&gt; &lt;3072×像素&gt;… &lt;1×标签&gt; &lt;3072×像素&gt; 第一个字节是第一个图像的标签，它是一个0-9范围的数字。接下来的3072个字节是图像像素的值。前1024个字节是红色通道值，下1024个绿色，最后1024个蓝色。值以行优先顺序存储，因此前32个字节是图像第一行(每张图片共32行)的红色通道值。每个文件都包含10000个这样的3072字节的“行”图像，但没有任何分隔行的限制。因此每个文件应该完全是30730000字节长度。 Demo以读取CIFAR10二进制数据为例，进行读取流程分析 构造文件队列 读取数据并进行解码 处理数据形状，放入批处理队列 开启会话线程运行 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import tensorflow as tfimport os# 组织数据filename = os.listdir('./data/cifar-10-batches-bin/')file_list = [os.path.join('./data/cifar-10-batches-bin/',file) for file in filename]# 1.构造文件队列file_queue = tf.train.string_input_producer(file_list)# 2.实例化阅读器，tf.FixedLengthRecordReader(bytes)(默认必须指定读取一个样本)# 返回reader实例，传入文件队列，调用read方法，返回(key, value)元组reader = tf.FixedLengthRecordReader(1+3*32*32)key, value = reader.read(file_queue)# 3.解码data = tf.decode_raw(value, tf.uint8)# 4.数据处理# 4.1.切片处理，把标签值和特征值分开，tf.slice(input_, begin, size, name=None)label = tf.slice(data, [0], [1])image = tf.slice(data, [1], [3*32*32])print('label',label)print('image',image)# 4.2.改变图像的形状image_major = tf.reshape(image, [3, 32, 32])# 4.3.转置image_trans = tf.transpose(image_major, [1, 2, 0])print('image_trans',image_trans)# 4.4.类型转换label_cast = tf.cast(label, tf.float32)image_cast = tf.cast(image_trans, tf.float32)# 5.批处理label_batch, image_batch = tf.train.batch([label_cast, image_cast], batch_size=10, num_threads=1, capacity=10)# 开启会话with tf.Session() as sess: # 创建线程协调器 coord = tf.train.Coordinator() # 创建线程 threads = tf.train.start_queue_runners(sess=sess, coord=coord) print('label:\n',label_batch.eval().shape) print('image:\n',image_batch.eval().shape) # 回收资源 coord.request_stop() coord.join(threads) 输出1234label: (10, 1)image: (10, 32, 32, 3)]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Reader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——进阶使用]]></title>
    <url>%2F2018%2F04%2F03%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——进阶使用其他基础API tf.app这个模块相当于TensorFlow进行的脚本提供一个main函数入口，可以定义脚本运行的flags 定义命令行参数类型 tf.app.flags DEFINE_string(flag_name, default_value, docstring) DEFINE_integer(flag_name, default_value, docstring) DEFINE_boolean(flag_name, default_value, docstring) DEFINE_float(flag_name, default_value, docstring) FLAGS标志 tf.app.flag.FLAGS 通过FLAGS调用定义的flag_name 执行app tf.app.run() 启动main(argv)函数，参数argv必须 12345# 定义命令行参数类型tf.app.flags.DEFINE_integer('age',10,'年龄')# 定义获取命令行参数FLAGS = tf.app.flags.FLAGSprint('年龄',FLAGS.age) 输出1年龄 10 tf.imageTensorFlow的图像处理操作。主要是一些颜色变换、变形和图像的编码和解码 tf.gfile这个模块提供了一组文件操作函数 tf.summary用来生成TensorBoard可用的统计日志，目前Summary主要提供了4种类型：audio、image、histogram、scalar 收集变量 tf.summary.scalar(name=’’, tensor)收集对于损失函数和准确率等单值变量，name为变量的名称，tensor为值 tf.summary.histogram(name=’’, tensor)收集高纬度的变量 tf.summary.image(name=’’, tensor)收集输入的图片张量，能显示图片 合并变量并写入事件文件 merged = tf.summary.merge_all() 运行合并：summary = sess.run(merged) 每次迭代都需要运行 添加：FileWriter.add_summary(summary, i) i为迭代次数 tf.python_io用来读写TFRecords文件 tf.train这个模块提供了一些训练器，与tf.nn组合起来，实现一些网络的优化计算 梯度下降优化 tf.train.GradientDescentOptimizer(learning_rate) learning_rate：学习率 method minimize(loss) return：梯度下降op 模型的保存于加载(checkpoint文件) tf.train.Saver(var_list=None, max_to_keep=5) var_list：指定将要保存和还原的变量。它可以作为一个dict或一个列表传递 max_to_keep：指示要保留的最近检查点文件的最大数量。创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件。默认为5。 12345678910111213'''梯度下降优化'''op = tf.train.GradientDecentOptimizer(learning=0.01,name='op').minimizer(loss)'''模型的保存与加载'''# 创建实例对象saver = tf.train.Saver()# 保存saver.save(sess, path)# 加载saver.restore(sess, path)# 判断模型是否存在checkpoint = tf.train.latest_checkpoint(path)saver.restore(sess, checkpoint) tf.nn这个模块提供了一些构建神经网络的底层函数。TensorFlow构建网络的核心模块。其中包含了添加各种层的函数，比如添加卷积层、池化层等 高级API tf.kerasKeras本来是一个独立的深度学习库，tensorflow将其学习过来，增加这部分模块在于快速构建模型 tf.layers以更高的概念层来定义一个模型，类似于tf.keras tf.contribtf.contrib.layers提供计算图中的网络层、正则化、摘要操作、构建计算图的高级操作，但是tf.contrib包含不稳定性和实验代码，可能在以后APi会发生改变 tf.estimator一个Estimator相当于Model+Training+Evaluate的合体。在模块中，已经实现了几种简单的分类器和回归器，包括：Baseline，Learning和Dnn。这里的DNN网络，只是全连接网络，没有提供卷积之类。 关于TensorFlow的API图示]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Advance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——变量]]></title>
    <url>%2F2018%2F04%2F03%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——变量变量TensorFlow变量是表示程序处理的共享持久状态的最佳方法。变量通过tf.Variable OP类以及tf.get_variable()类进行操作。变量的特点 存储持久化 可修改值 可指定被训练 创建变量 tf.Variable(inital_value=None, trainable=True, collections=None, name=None) inital_value：初始化的值 trainable：是否被训练 collections：新变量将添加到列出的图的结合中collections，默认为[GraphKeys.GLOBAL_VARIABLES]，如果trainable是True变量也被添加到图像集合GraphKeys.TRAINABLE_VARIABLES 变量初始化变量需要初始化才能运行值 tf.global_variables_initializer() global_variables_initializer() to add an Op to the graph that initializesall the variables. You then run that Op after launching the graph. 此op返回variables_initializer(global_variables()) variables_initialzer()中传入list global_valables()返回ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope) 返回get_default_graph().get_collection(key, scope)获取变量 123456import tensorflow as tfvar = tf.Variable(tf.random_normal([2, 2], mean=0.0, stddev=1.0), name="var", trainable=True)init_op = tf.global_variables_initializer()with tf.Session() as sess: print('init',sess.run(init_op)) print('var',sess.run(var)) 输出123init Nonevar [[ 1.2884922 -1.2058842] [ 0.9121997 -1.4975668] 变量OP的方法 var.assign(value) 给变量赋值一个新的值 var.assign_add(delta) 给变量增加差值 123456import tensorflow as tfnew_var = var.assign([[1,2],[3,4]])var_add = var.assign_add([[1,2],[3,4]])with tf.Session() as sess: print(sess.run(new_var)) print(sess.run(var_add)) 输出1234new_var [[1. 2.] [3. 4.]]var_add [[2. 4.] [6. 8.]] 命名空间与共享变量共享变量的主要用途在一些网络中的参数共享，由于在TensorFlow当中，如果定义在OP的name参数指定一样，其实并不是同一个变量。如果想要达到重复利用变量的效果，我们就要使用tf.variable_scope()结合tf.get_variable()一起使用 定义同名变量1234var1 = tf.Variable(name=&apos;var&apos;,initial_value=[1],dtype=tf.float32)var2 = tf.Variable(name=&apos;var&apos;,initial_value=[1],dtype=tf.float32)print(var1)print(var2) 输出12&lt;tf.Variable 'var:0' shape=(1,) dtype=float32_ref&gt;&lt;tf.Variable 'var_1:0' shape=(1,) dtype=float32_ref&gt; 虽然把两个变量的name设置为相同的’var’，但是在同一命名空间中，默认情况下不能出现相同name的变量，所以当存在一个’var’时，再次定义变量且命名为’var’时，新定义的变量的name为’var_1’，从而保证唯一。 修改OP命名空间在TensorFlow中，为了区别不同的变量，会需要命名空间对不同的变量进行命名，其中常用的两个函数为 tf.variable_scope tf.name_scope tf.variable_scope用于定义创建变量(层)的操作的上下文管理器。在默认图中，这个上下文管理器验证(可选)值来自同一个图，并推送一个名称范围和一个变量范围。如果name_or_scope不是None，则按原样使用它。如果name_or_scope为None，则使用default_name。在这种情况下，如果以前在相同的范围中使用了相同的名称，那么将通过向其追加_N使其唯一。 Simple example of how to create a new variable: 1234with tf.variable_scope('foo'): with tf.variable_scope('bar'): v = tf.get_variable('v', [1]) assert v.name == 'foo/bar/v:0' tf.name_scope定义Python操作时使用的上下文管理器。这个上下文管理器验证给定的值来自相同的图，使该图成为默认图，并将名称作用域推入该图。 Simple example to define a new Python op called my_op: 1234567def my_op(a, b, c, name=None): with tf.name_scope(name, 'MyOp', [a, b, c]) as scope: a = tf.convert_to_tensor(a, name='a') b = tf.convert_to_tensor(b, name='b') c = tf.convert_to_tensor(c, name='c') # Define some comuptation that uses 'a', 'b', and 'c' return foo_op(..., name=scope) Simple example of how to reenter a premade variable scope safely: 1234567891011import tensorflow as tfwith tf.variable_scope('foo') as vs: pass# Re-enter the variable scopewith tf.variable_scope(vs,auxiliary_name_scope=False) as vs1: # Restore the original name_scope with tf.name_scope(vs1.original_name_scope): s = tf.get_variable('s',[1]) assert s.name == 'foo/s:0' c = tf.constant([1],name='c') assert c.name == 'foo/c:0' 共享变量通过tf.get_variable的初始化与Variable参数一样，但是要实现共享需要设置tf.variable_scope()中的reuse=tf.AUTO_REUSE参数 Basic example of sharing a variable AUTO_REUSE 1234567def foo(): with tf.variable_scope('foo', reuse=tf.AUTO_REUSE): v = tf.get_variable('v',[1]) return vv1 = foo() # Creates vv2 = foo() # Gets the same, existing vassert v1 == v2 Basic example of sharing a variable with reuse=True: 123456import tensorflow as tfwith tf.variable_scope('foo'): v = tf.get_variable('v', [1])with tf.variable_scope('foo', reuse=True): v1 = tf.get_variable('v',[1])assert v1 == v Sharing a variable by capturing a scope and setting reuse: 123456import tensorflow as tfwith tf.variable_scope('foo') as scope: v = tf.get_variable('v', [1]) scope.reuse_variables() v1 = tf.get_variable('v', [1])assert v1 == v To prevent accidental sharing of variables, we raise an exception when getting an existing variable in a non-reusing scope: 1234with tf.variable_scope("foo"): v = tf.get_variable("v", [1]) v1 = tf.get_variable("v", [1]) # Raises ValueError("... v already exists ..."). Similarly, we raise an exception when trying to get a variable that does not exist in reuse mode: 123with tf.variable_scope("foo", reuse=True): v = tf.get_variable("v", [1]) # Raises ValueError("... v does not exists ...").]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Variable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——张量的数学运算]]></title>
    <url>%2F2018%2F04%2F03%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——张量的数学运算张量的数学运算算术运算符x = tf.constant([a, b, c])y = tf.constant([i, j, k]) tf.add(x, y, name=None) [a+i, b+j, c+k] tf.subtract(x, y, name=None) [a-i, b-j, c-k] tf.multiply(x, y, name=None) [ai, bj, c*k] tf.scalar_mul(scalar, x) [scalara, scalarb, scalar*c] tf.div(x, y, name=None) [a/i, b/j, c/k] tf.div([1,2,1], [1,2,3]) ==&gt; [1, 1, 0] tf.divide(x, y, name=None) [a/i, b/j, c/k] tf.divide([1,2,1], [1,2,3]) ==&gt; [1. , 1. , 0.33333333] tf.truediv(x, y, name=None) [a/i, b/j, c/k] tf.truediv([1,2,1], [1,2,3]) =&gt; [1. , 1. , 0.33333333] tf.floordiv(x, y, name=None) [a/i, b/j, c/k] tf.floordiv([1,2,1], [1,2,3]) ==&gt; [1, 1, 0] tf.realdiv(x, y, name=None) [a/i, b/j, c/k] tf.realdiv([1.0,2.0,1.0], [1.0,2.0,3.0]) ==&gt; [1. , 1. , 0.33333334] the dtype of x,y must be float (tf&lt;=1.11.0) tf.truncatediv(x, y, name=None) [int(a/i), int(b/j), int(c/k)] tf.truncatediv([-10,-4,10], [6,3,6]) ==&gt; [-1, -1, 1] the dtype of x,y must be int (tf&lt;=1.11.0) tf.floordiv(x, y, name=None) [a//i, b//j, c//k] tf.truncatemod(x, y, name=None) x - tf.truncatediv(x, y,)*y tf.floormod(x, y, name=None) x - tf.floordiv(x,y) tf.mod(x, y, name=None) x - tf.floordiv(x,y) tf.cross(x, y, name=None) $[bk-cj, ak-ci, aj-bi]基本数学函数 tf.add_n(inputs, name=None) inputs must be a list tf.abs(x, name=None) tf.negative(x, name=None) tf.sign(x, name=None) [a/|a|, b/|b|, c/|c|] tf.reciprocal(x, name=None) [1/a, 1/b, 1/c] the dtype of x must be float (tf&lt;=1.11.0) tf.square(x, name=None) [a^2, b^2, c^2] tf.round(x, name=None) [round(a), round(b), round(c)] tf.sqrt(x, name=None) [sqrt(a), sqrt(b), sqrt(c)] tf.rsqrt(x, name=None) [1/sqrt(a), 1/sqrt(b), 1/sqrt(c)] tf.pow(x, y, name=None) [a^i ,b^j , c^k] tf.exp(x, name=None) [exp(a), exp(b), exp(c)] tf.expm1(x, name=None) [exp(a)-1, exp(b)-1, exp(c)-1] tf.log(x, name=None) [log(a), log(b), log(c)] base ‘e’ tf.log1p(x, name=None) [log(1+a), log(1+b), log(1+c)] tf.ceil(x, name=None) [ceil(a), ceil(b), ceil(c)] tf.ceil([1.1,-1.1,-1.9]) ==&gt; [ 2., -1., -1.] tf.floor(x, name=None) [floor(a), floor(b), floor(c)] tf.floor([1.9,-1.1,-1.9]) ==&gt; [ 1., -2., -2.] tf.maximum(x, y, name=None) [max(a,i), max(b,j), max(c,k)] tf.minimum(x, y, name=None) [min(a,i), min(b,j), min(c,k)] tf.cos(x, name=None) [cos(a), cos(b), cos(c)] tf.sin(x, name=None) [sin(a), sin(b), sin(c)] tf.lbeta(x, name=’lbeta’) log(Beta(x)) tf.tan(x, name=None) [tan(a), tan(b), tan(c)] tf.acos(x, name=None) [arccos(a), arccos(b), arccos(c)] the dtype of x must be float (tf&lt;=1.11.0) tf.asin(x, name=None) [arcsin(a), arcsin(b), arcsin(c)] the dtype of x must be float (tf&lt;=1.11.0) tf.atan(x, name=None) [arctan(a), arctan(b), arctan(c)] the dtype of x must be float (tf&lt;=1.11.0) tf.cosh(x, name=None) [(exp(a)+exp(-a))/2.0, (exp(b)+exp(-b))/2.0, (exp(c)+exp(-c))/2.0] the dtype of x must be float (tf&lt;=1.11.0) tf.sinh(x, name=None) [(exp(a)-exp(-a))/2.0, (exp(b)-exp(-b))/2.0, (exp(c)-exp(-c))/2.0] the dtype of x must be float (tf&lt;=1.11.0) tf.tanh(x, name=None) [sinh(a)/cosh(a), sinh(b)/cosh(b), sinh(c)/cosh(c)] tf.asinh(x, name=None) [log(a+sqrt(a^2+1)), log(b+sqrt(b^2+1)), log(c+sqrt(c^2+1))] tf.acosh(x, name=None) [log(a+sqrt(a^2-1)), log(b+sqrt(b^2+1)), log(c+sqrt(c^2+1))] tf.atanh(x, name=None) [log((1+a)/(1-a))/2, log((1+b)/(1-b))/2, log((1+c)/(1-c))/2] tf.lgamma(x, name=None) tf.digamma(x, name=None) tf.erf(x, name=None) tf.erfc(x, name=None) tf.squared_difference(x, y, name=None) tf.igamma(a, x, name=None) tf.igammac(a, x, name=None) tf.zeta(x, q, name=None) tf.polygamma(a, x, name=None) tf.betainc(a, b, x, name=None) tf.rint(x, name=None) rint(-1.5) ==&gt; -2.0 rint(0.5000001) ==&gt; 1.0 rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==&gt; [-2., -2., -0., 0., 2., 2., 2.]矩阵运算 tf.diag(diagonal, name=None) ‘diagonal’ is [1, 2, 3, 4] tf.diag(diagonal) ==&gt; [[1, 0, 0, 0],[0, 2, 0, 0],[0, 0, 3, 0],[0, 0, 0, 4]] tf.diag_part(input, name=None) ‘input’ is [[1, 0, 0, 0],[0, 2, 0, 0],[0, 0, 3, 0],[0, 0, 0, 4]] tf.diag_part(input) ==&gt; [1, 2, 3, 4] tf.trace(x, name=None) tf.trace([[1, 2, 3],[4, 5, 6],[7, 8, 9]]) ==&gt; 1+5+9=15 tf.transpose(a, perm=None, name=’transpose’, conjugate=False) tf.eye(num_rows, num_columns=None, batch_shape=None, dtype=tf.float32, name=None) tf.matrix_diag(diagonal, name=None) tf.matrix_diag_part(input, name=None) tf.matrix_band_part(input, num_lower, num_upper, name=None) tf.matrix_set_diag(input, diagonal, name=None) tf.matrix_transpose(a, name=’matrix_transpose’, conjugate=False) tf.matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) tf.norm(tensor, ord=’euclidean’, axis=None, keepdims=None, name=None, keep_dims=None) tf.matrix_determinant(input, name=None) tf.matrix_inverse(input, adjoint=False, name=None) tf.cholesky(input, name=None) tf.choleskey_solve(chol, rhs, name=None) tf.matrix_solve(matrix, rhs, adjoint=False, name=None) tf.matrix_triangular_solve(matrix, rhs, lower=True, adjoint=False, name=None) tf.matrix_solve_ls(matrix, rhs, l2_regularizer=0.0, fast=True, name=None) tf.qr(input, full_matrices=False, name=None) tf.self_adjoint_elg(tensor, name=None) tf.self_adjoint_eigvals(tensor, name=None) tf.svd(tensor, full_matrices=False, compute_uv=True, name=None) reduce操作 tf.reduce_sum(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_prod(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_min(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_max(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_all(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_any(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_logsumexp(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) tf.reduce_nonzero(input_tensor, axis=None, keepdims=None, dtype=tf.int64, name=None, reduction_indices=None, keep_dims=None) tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None) tf.einsum(equation, *inputs, **kwargs) 序列索引操作 tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None) tf.cumprod(x, axis=0, exclusive=False, reverse=False, name=None)]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Tensor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——张量]]></title>
    <url>%2F2018%2F04%2F03%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%BC%A0%E9%87%8F%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——张量s张量的属性TensorFlow的张量就是一个n维数组，类型为tf.TensorTensor具有以下两个重要属性 type：数据类型 shape：形状(阶) 张量的类型 数据类型 Python类型 描述 DT_FLOAT tf.float32 32位浮点数 DT_DOUBLE tf.float64 64位浮点数 DT_INT64 tf.int64 64位有符号整型 DT_INT32 tf.int32 32位有符号整型 DT_INT16 tf.int16 16位有符号整型 DT_INT8 tf.int8 8位有符号整型 DT_UINT8 tf.uint8 8位无符号整型 DT_STRING tf.string 可变长度的字节数组 每一个张量元素都是一个字节数组 DT_BOOL tf.bool 布尔型 DT_COMPLEX64 tf.complex64 由两个32位浮点数组成的复数：实数和虚数 DT_QINT32 tf.qint32 用于量化Ops的32位有符号整型 DT_QINT8 tf.qint8 用于量化Ops的8位有符号整型 DT_QUINT8 tf.quint8 用于量化Ops的8位无符号整型 张量的阶 阶 数学实例 Python 例子 0 纯量 (只有大小) s = 123 1 向量 (大小和方向) v = [1,2,3] 2 矩阵 (数据表) m = [[1,2,3],[4,5,6],[7,8,9]] 3 3阶张量 (数据立体) t = [[[1],[2],[3]],[[4],[5],[6]],[[7],[8],[9]]] n n阶 …… …… 张量的创建固定值张量 tf.zeros(shape, dtype=tf.float, name=None) 创建所有元素设置为0的张量，此操作返回一个dtype具有形状shape和所有元素设置为0的类型的张量 tf.zeros_like(tensor, dtype=None, name=None) 给tensor定单张量，此操作返回tensor与所有元素设置为0相同的类型和形状的张量 tf.ones(shpe, dtype=tf.float32, name=None) 创建一个所有元素设置为1的张量，此操作返回一个类型的张量，dtype形状shape和所有元素设置为1 tf.cones_like(tensor, dtype=None, name=None) 给tensor定单张量，此操作返回tensor与所有元素设置为1相同类型和形状的张量 tf.fill(dims, value, name=None) 创建一个填充了标量值的张量。此操作创建一个张量的形状dims并填充它value tf.constant(value, dtype=None, shape=None, name=’Const’) 创建一个常数张量 123456789import tensorflow as tfa = tf.constant([1,2])sess = tf.Session()print('zeros',sess.run(tf.zeros(10)))print('zeros_like',sess.run(tf.zeros_like(a)))print('ones',sess.run(tf.ones(10)))print('ones_like',sess.run(tf.ones_like(a)))print('fill',sess.run(tf.fill([2,10],2)))print('constant',sess.run(tf.constant(10))) 输出1234567zeros [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]zeros_like [0 0]ones [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]ones_like [1 1]fill [[2 2 2 2 2 2 2 2 2 2] [2 2 2 2 2 2 2 2 2 2]]constant 10 随机值张量 tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 从截断的正态分布中输出随机值，和tf.random_normal()一样，但是所有数字都不超过两个标准差 tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 从正态分布中输出随机值，由随机正态分布的数字组成的矩阵 123with tf.Session() as sess:print('truncated_normal',sess.run(tf.truncated_normal([1,3],mean=0,stddev=2)))print('random_normal',sess.run(tf.random_normal([1,3],mean=0,stddev=2))) 输出12truncated_normal [[-1.4836491 1.4532584 -1.5331081]]random_normal [[ 1.4573846 1.2889411 -0.46845075]] 其他特殊的创建张量的op tf.Variable tf.placeholder 张量的变换类型变换 tf.string_to_number(string_tensor, out_type=None, name=None) tf.as_string(input, precision=-1, scientific=False, shortest=False, width=-1, fill=’’, name=None) tf.to_double(x, name=’ToDouble’) tf.to_float(x, name=’ToFloat’) tf.to_bfloat16(x, name=’ToBFloat16’) tf.to_int32(x, name=’ToInt32’) tf.to_int64(x, name=’ToInt64’) tf.cast(x, dtype, name=None) 12345t = tf.constant([[1,2,3]])s =tf.as_string(a)print(sess.run(s))print(sess.run(tf.string_to_number(s)))print(sess.run(tf.cast(a,dtype=tf.int32))) 输出123[b'1' b'3'][1. 3.][1 3] 形状变换TensorFlow的张量具有两种形状变换，动态形状和静态形状 tf.reshape tf.set_shape 关于动态形状和静态形状必须符合以下规则 静态形状 转换静态形状的时候，1-D到1-D，2-D到2-D，不能跨阶数改变相撞 对于已经固定的张量的静态形状的张量，不能再次设置静态状态 shape=(m, n) 若m，n已知，不能改变 若shape中有None，则可以修改None，且使用.set_shape()在原来的tensor进行修改 动态形状 tf.reshape()动态创建新张量时，张量的元素个数必须匹配 修改前后总数一样 动态修改可以跨阶，静态不可以实现跨阶 1234567891011# 能用set_shape就优先使用，不能用的话，使用reshapea = tf.constant([1,2,3,4])ph = tf.placeholder(tf.float32,[None,4])sess = tf.Session()# ph.set_shape([4,3])报错ph.set_shape([3,4])new_ph = tf.reshape(ph,[4,3])# a.set_shape([2,2])报错tf.reshape(a,[2,2])]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Tensor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——会话Session]]></title>
    <url>%2F2018%2F04%2F02%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E4%BC%9A%E8%AF%9DSession%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——会话Session会话一个运行TensorFlow operation的类。会话包含以下两种开启方式 tf.Session：用于完整的程序当中 tf.InteractiveSession：用于交互式上下文中的TensorFlow，例如shell 1.TensorFlow使用tf.Session类来表示客户端程序(通常为Python程序，但也提供了使用其他语言的类似接口)与C++运行时之间的链接2.tf.Session对象使用分布式，TensorFlow运行时提供对本地计算机中的设备和远程设备的访问权限 会话可能拥有的资源，如tf.Variable，tf.QueueBase和tf.ReaderBase。当这些资源不再需要时，释放这些资源非常重要。因此，需要调用tf.Session.close会话中的方法，或将会话用作上下文管理器。1234567891011121314151617import tensorflow as tf# 创建图a = tf.constant(1)b = tf.constant(2)c = tf.add(a,b)# 不使用上下文管理器# 1.创建会话对象sess = tf.Session()# 2.执行操作cprint('不使用上下文管理器:\n',sess.run(c))# 3.关闭会话sess.close()# 使用上下文管理器with tf.Session() as sess: print('使用上下文管理器:\n',sess.run(c)) 输出1234不使用上下文管理器: 3使用上下文管理器: 3 tf.Session.__init__(target=’’, graph=None, config=None) target：如果将此参数留空(默认设置)，会话将仅使用本地计算机中的设备。可以指定grpc://网址，以便指定TensorFlow服务器的地址，这使得会话可以访问该服务器控制的计算机上的所有设备 graph：默认情况下，新的tf.Session将绑定到当前的默认图 config：此参数允许指定一个tf.ConfigProto以便控制会话的行为。例如，ConfigProto协议用于打印设备使用信息 在pycharm中输入123456789101112import tensorflow as tf# 创建图a = tf.constant(1)b = tf.constant(2)c = tf.add(a,b)# 运行会话并打印设备信息sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))# 2.执行操作cprint('不使用上下文管理器:\n',sess.run(c))# 3.关闭会话sess.close() 输出12345678910112018-04-02 22:04:28.244854: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-04-02 22:04:28.245870: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:Device mapping: no known devices.2018-04-02 22:04:28.252338: I tensorflow/core/common_runtime/placer.cc:922] Add: (Add)/job:localhost/replica:0/task:0/device:CPU:02018-04-02 22:04:28.252353: I tensorflow/core/common_runtime/placer.cc:922] Const: (Const)/job:localhost/replica:0/task:0/device:CPU:02018-04-02 22:04:28.252360: I tensorflow/core/common_runtime/placer.cc:922] Const_1: (Const)/job:localhost/replica:0/task:0/device:CPU:0Add: (Add): /job:localhost/replica:0/task:0/device:CPU:0Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0Const_1: (Const): /job:localhost/replica:0/task:0/device:CPU:0不使用上下文管理器: 3 会话可以分配不同的资源在不同的设备上运行12# device_type：类型设备（例如CPU，GPU，TPU）/job:worker/replica:0/task:0/device:CPU:0 会话的run() run(fetches, feed_dict=None, options=None, run_metadata=None) 通过使用sess.run()来运行operation fetches：单一的operation，或者列表、元组(其他不属于tensorflow的类型不行) feed_dict：参数允许调用者覆盖途中张量的值，运行时赋值 与tf.placeholder搭配使用，则会检查值的形状是否与占位符兼容 使用tf.operation.eval()也可运行operation，但需要在会话中运行 12345678910111213import tensorflow as tfa = tf.constant(2)b = tf.constant(3)c = a * b# 创建会话sess = tf.Session()# 执行cprint(sess.run(c))print(c.eval(session=sess))sess.close() 输出1266 feed操作 placeholder提供占位符，run时候通过feed_dict指定参数 123456789101112131415161718import tensorflow as tfa = tf.constant(2)b = tf.constant(3)c = a * b# 定义placeholder(dtype,shape)ph = tf.placeholder(dtype=tf.int32,shape=[None, 4])sess = tf.Session()print('c.eval',c.eval(session=sess))a,b,c = sess.run([a,b,c])print('a',a)print('b',b)print('c',c)# 运行placeholderprint('placeholder',sess.run(ph,feed_dict=&#123;ph:[[1,2,3,4],[5,6,7,8]]&#125;))print('ph_eval',ph.eval(session=sess,feed_dict=&#123;ph:[[1,2,3,4],[5,6,7,8]]&#125;))# 关闭会话sess.close() 常见错误信息 RuntimeError：如果此Session时无效状态(例如已关闭) TypeError：如果fetches或者feed_dict键的类型不合适 ValueError：如果fetches或feed_dict键无效或引用Tensor不存在的键]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——OP]]></title>
    <url>%2F2018%2F04%2F02%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94OP%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——OP常见OP 类型 实例 标量运算 add,sub,mul,div,exp,log,greater,less,equal 向量运算 concat,slice,splot,constant,rank,shape,shuffle 矩阵运算 matmul,matrixnverse,matrixdateminant 带状态的运算 variable,assgin,assginadd 神将网络组件 softmax,sigmoid,relu,convolution,max_pool 存储,回复 Save,Restore 队列及同步运算 Enqueue,Dequeue,MutexAcquire,MutexRelease 控制流 Merge,Switch,Enter,Leave,Nextlteration 一个操作对象(Operation，OP)是tensorFlow途中的一个节点，可以接收0个或者多个输入Tensor，并且可以输出0个或者多个Tensor，Operation对象是通过op构造函数(如tf.matmul())创建的。 例如：c = tf.matmul(a,b)创建了一个Operation对象，类型为MatMul类型，它将张量a，b作为输入，c作为输出，并且输出数据，打印的时候也是打印的数据。其中tf.matmul()是函数，在执行matmul函数的过程中会通过MatMul类创建一个与之对应的对象 1234567import tensorflow as tfa = tf.constant(1.0)b = tf.constant(2.0)c = tf.add(a,b)print(a)print(b)print(c) 输出123Tensor("Const_2:0", shape=(), dtype=float32)Tensor("Const_3:0", shape=(), dtype=float32)Tensor("Add:0", shape=(), dtype=float32) 打印出来的是张量值，可以理解成OP当中包含了这个值。并且每一个OP指令都对应一个唯一的名称，如上面的Const:0，这个在TensorBoard中也可以显示。tf.Tensor对象以输出该张量的tf.Operation明确命名。张量名称形式为&amp;ltOP_NAME&amp;gt:&amp;lti&amp;gt &amp;ltOP_NAME&amp;gt：生成该张量的指令名称 &amp;lti&amp;gt：整数，表示该张量在指令的输出中的索引 指令名称tf.Graph对象为奇奇包含的tf.Operation对象定义的一个命名空间TensorFlow会自动为图中的每个指令选择一个唯一名称，用户也可以指定描述名称，使程序阅读起来更轻松。我们可以通过以下方式改写指令名称 每个创建新的tf.Operation或返回新的tf.Tensor的API函数可以接受可选的name参数 例如，tf.constant(1.0, name=’test’)创建了一个名为’test’的新tf.Operation并返回一个名为’test:0’的tf.Tensor。如果默认图包含名为’test’的指令，则TensorFlow会在名称上附加’1’，‘2’等字符，以便让名称具有唯一性12345import tensorflow as tfa = tf.constant(1.0,name='test')b = tf.constant(2.0,name='test')print(a)print(b) 输出12Tensor("test:0", shape=(), dtype=float32)Tensor("test_1:0", shape=(), dtype=float32) 当修改好之后，Tensorboard显示的名字也会被修改]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Operation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——TensorFlow数据流图]]></title>
    <url>%2F2018%2F04%2F02%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94TensorFlow%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——TensorFlow数据流图TensorFlow结构分析TensorFlow程序通常被组织成一个构建图阶段和一个执行图阶段，即使用静态图计算构建阶段，数据与操作的执行步骤被描述成一个图在执行阶段，使用会话执行构建好的图中的操作 图和会话 图：TensorFlow将计算表示为指令之间的依赖关系的一种表示法 会话：TensorFlow跨一个或多个本地或远程设备运行数据流图的机制 张量：TensorFlow中的基本数据对象 节点：提供图当中执行的操作 动态计算动态计算意味着程序将按照我们编写命令的顺序进行执行。这种机制将使得调试更加容易，并且也使得我们将大脑中的想法转化为实际代码变得更加容易 静态计算静态计算意味着程序在编译执行时将先生成神经网络的结果，让后再执行响应操作 从理论上讲，静态计算这样的机制允许编译器进行更大程度的优化，但是这也意味着你所期望的程序与编译器实际执行之间存在着更多的代沟。这也意味着，代码中的错误将更加难以发现(比如，如果计算图的结果出现问题，你可能只有在代码执行到相应操作的时候才能发现它)。尽管理论上而言，静态计算图比动态计算图具有更好的性能，但是在实践中我们经常发现不是这样的。(MxNet 和最新的 TensorFlow同时具有动态计算和静态计算两种机制) 数据流图介绍TensorFlow是采用数据流图(data flow graphs)，用于数值计算的开源框架节点(Operation)在途中表示数学操作，线(edges)则表示在节点间相互联系的多维数据数组，即张量(tensor)。 图结构介绍图包含了一组tf.Operatio代表的计算单元对象和tf.Tensor代表的计算单元之间流动的数据 图相关操作默认图Graph默认注册，并可通过调用tf.get_default_graph()访问，要讲操作添加到默认图形中，直接创建OP即可12345678910111213141516171819import tensorflow as tfa = tf.constant(1)b = tf.constant(2)c = tf.add(a,b)print('a',a)print('b',b)print('c',c)# 图：打印出来是一个分配内存的地址# 所有的张量、op、会话默认都在一张图中print('获取默认图',tf.get_default_graph())print('a的图属性为',a.graph)print('b的图属性为',b.graph)print('c的图属性为',c.graph)# 会话，默认智能运行默认的图，不能运行其他的(可通过参数解决)with tf.Session() as sess: print('a',sess.run(a)) print('b',sess.run(b)) print('c',sess.run(c)) print('会话的图属性',sess.graph) 输出1234567891011a Tensor("Const_34:0", shape=(), dtype=int32)b Tensor("Const_35:0", shape=(), dtype=int32)c Tensor("Add_5:0", shape=(), dtype=int32)获取默认图 &lt;tensorflow.python.framework.ops.Graph object at 0xb32263550&gt;a的图属性为 &lt;tensorflow.python.framework.ops.Graph object at 0xb32263550&gt;b的图属性为 &lt;tensorflow.python.framework.ops.Graph object at 0xb32263550&gt;c的图属性为 &lt;tensorflow.python.framework.ops.Graph object at 0xb32263550&gt;a 1b 2c 3会话的图属性 &lt;tensorflow.python.framework.ops.Graph object at 0xb32263550&gt; 创建图可以通过tf.Graph()创建图，如果要在这张图中创建OP，典型用法是使用tf.Graph.as_default()上下文管理器123456789g = tf.Graph()with g.as_default(): # 在g图中定义了一个operation a = tf.constant(1) print('a的图属性为',a.graph)with tf.Session(graph=g) as sess: print('a的值为',sess.run(a)) print('会话的图属性为',sess.graph) 输出123a的图属性为 &lt;tensorflow.python.framework.ops.Graph object at 0xb32251c88&gt;a的值为 1会话的图属性为 &lt;tensorflow.python.framework.ops.Graph object at 0xb32251c88&gt; TensorBoard:可视化学习TensorFlow可用于训练大规模深度学习神经网络所需的计算，使用该工具涉及的计算往往复杂而深奥。为了更方便TensorFlow程序的理解、调试与优化，TensorFlow提供了TensorBoard可视化工具。 数据序列化-events文件TensorBoard通过读取TensorFlow的事件文件来运行，需要将数据生成一个序列化的Summary protobuf对象123# 返回filewriter，写入事件文件到指定目录，以提供给tensorboard使用# 此处将event文件存入当前文件夹下的tmp文件夹下tf.summary.FileWriter('./tmp',graph=sess.graph) 这将在指定目录中生成一个event文件，其名称改格式如下1events.out.tfevents.&#123;timestamp&#125;.&#123;hostname&#125; 启动TensorBoard使用terminal进入存储event文件的文件夹，并在terminal中输入如下代码1tensorboard --logdir='./' 在浏览器中打开TensorBoard的图页面localhost:6006 ，会看到与以下图形类似的图,在GRAPHS模块我们可以看到以下图结构]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Data Flow Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——深度学习主流框架]]></title>
    <url>%2F2018%2F04%2F02%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——深度学习主流框架 框架名 主语言 从语言 开发者 介绍 Theano python C++/cuda 蒙特利尔理工学院 深度学习开源工具的鼻祖，由蒙特利尔理工学院于2008年将其开源 Torch lua C/cuda Facebook FaceBook和Twitter主推的一款开源深度学习框架，Google和多个大学研究机构也是用Torch。出于性能的考虑，Torch使用一种比较小众的语言(lua)作为其开发语言，目前在音频、图像及视频处理方面有着大量的应用。大名鼎鼎的AlphaGo便是基于Torch开发的，只不过在Google开源Tensorflow之后，AlphaGo迁移到Tensorflow上 PyTorch python C/C++ FaceBook Pytorch是Torch的python版本，是由FaceBook开源的神经网络框架，使用动态计算图，可以根据计算需要实时改变计算图 TensorFlow C++ cuda/python Google Google开源的一款深度学习工具，使用C++语言开发，上层提供Python API，在开源之后，在工业界和学术界引起了极大的震动，因为TensorFlow曾经是著名的Google Brain计划中的一部分 Caffe C++ cuda/python/Matlab 贾杨清 Caffe是加州大学伯克利分校视觉与学习中心(Berkeley Vision and Learning Center·BVLC)贡献出来的一套深度学习工具，使用C/C++开发，上层提供Python API MXNet C++ cuda/R/julia 李沐和陈天奇 Apache MXNet是一个深度学习框架，旨在提高效率和灵活性。它允许混合符号和命令式编程，以最大限度地提高效率和生产力。MXNet的核心是一个动态依赖调度程序，可以动态地自动并行化符号和命令操作。最重要的图形优化层使用符号执行更快，内存效率更高 CNTK C++ cuda/python Microsoft CNTK(Computational Network Toolkit)是微软开源的深度学习工具，现已经更名为Microsoft Cognitive Toolkit，同样也是用C++语言开发并提供Python API，目前在计算机视觉、金融领域及自然处理领域已经被广泛使用 最常用的框架当数TensorFlow和Pytorch, 而 Caffe 和 Caffe2 次之。 PyTorch 和 Torch 更适用于学术研究（research）；TensorFlow，Caffe，Caffe2 更适用于工业界的生产环境部署（industrial production） Caffe 适用于处理静态图像（static graph）；Torch 和 PyTorch 更适用于动态图像（dynamic graph）；TensorFlow 在两种情况下都很实用。 Tensorflow 和 Caffe2 可在移动端使用。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记——TensorFlow介绍]]></title>
    <url>%2F2018%2F04%2F02%2FTensorFlow%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94TensorFlow%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[TensorFlow学习笔记——TensorFlow介绍TensorFlow工作原理TensorFlow在不同设备(如CPU和GPU)熵执行代码的能力是其特定结构的结果：TensorFlow将计算定义为图形，并且这些都是用操作(也称”ops”)进行的。所以，当我们使用TensorFlow时，它就像在Graph中定义一些列操作一样。要讲这些操作作为计算来执行，我们必须将图形启动到会话中。会话会将表示到图表中的操作转换并传递到要执行它们的设备，无论是GPU还是CPU。 TensorFlow优点 高灵活度(Deep Flexibility) 它不仅考验用来做神经网络算法研究，也可以用来做普通的机器学习算法，甚至是只要把计算表示成数据流图，都可以用TensorFlow 语言多样(Language Options) TensorFlow使用C++实现的，然后用Python封装。谷歌号召社区通过SWIG开发更多的语言接口来支持TensorFlow 设备支持 TensorFlow可以运行在各种硬件上，同时根据计算的需要，合理将运算分配到相应的设备，比如卷积就分配到GPU上，也允许在CPU和GPU上的计算分布，甚至支持使用gRPC进行水平扩展 Tensorboard可视化 TensorBoard是TensorFlow的一组Web应用，用来监控TensorFlow运行过程，或可视化Computation Graph。TensorBoard目前支持5种可视化 标量(scalars) 图片(images) 音频(audio) 直方图(histograms) 计算图(Computation Graph) TensorBoard的Events Dashboard可以用来持续地监控运行时的关键指标，比如loss、学习速率(learning rate)或是验证集熵的准确率(accuracy) TensorFlow的安装 CPU安装 1pip install tensorflow GPU安装http://www.tensorfly.cn/tfdoc/get_started/os_setup.html CPU与GPU的对比 CPU：核芯的数量更少 但是每一个核芯的速度更快，性能更强 更适用于处理连续性（sequential）任务 GPU：核芯的数量更多 但是每一个核芯的处理速度较慢 更适用于并行（parallel）任务]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——K均值算法(K-means)]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95(K-means)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——K均值算法(K-means)无监督学习 无监督学习，即从无标签的数据开始学习 包含算法 聚类 K均值聚类(K-means) 降维 PCA K-means原理 聚类效果图如下 优点：采用迭代式算法，直观易懂且非常实用 缺点：容易收敛到局部最优解(多次聚类) 聚类一般做在分类之前 K-means聚类步骤 1.随机设置K个特征空间内的点作为初始的聚类中心 2.对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 3.接着对标记的聚类中心之后，重新计算出每个聚类的新中心点(平均值) 4.如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程 具体步骤如下图 K-meansAPI sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’) k-means聚类 n_clusters：开始的聚类中心数量 init：初始化方法，默认为’k-means++’ labels_：默认标记的类型，可以和真实值比较(不是值比较) 1234cust = datakm = KMeans(n_clusters=4)km.fit(cust)pre = km.predict(cust) K-means性能评估指标轮廓系数$$SC_i=\frac{b_i-a_i}{max(b_i,a_i)}$$对于每个点$i$为已聚类数据中的样本，$b_i$为$i$到其他簇的所有样本的距离最小值，$a_i$为$i$到本身簇的距离平均值。最终计算出所有样本点的轮廓系数平均值 轮廓系数数值分析 分析过程(以蓝点为例) 1.计算出蓝1离本身簇所有点的距离平均值$a_i$ 2.蓝1到其他两个簇的距离计算出平均值红平均，绿平均，取最小的那个距离作为$b_i$ 根据公式进行极端值考虑，如果$b_i&gt;&gt;a_i$，那个公式结果趋近于1；如果$a_i&gt;&gt;b_i$，那么公式结果趋近于-1。若$b_i&gt;&gt;a_i$，即公式结果趋近于1，则效果越好；若$a_i&gt;&gt;b_i$，即公式结果趋近于-1，则效果越差。轮廓系数的值时介于[-1,1]，越趋近于1代表内聚度和分离度相对较优 轮廓系数API sklearn.metrics.sihouette_score(X, labels) 计算所有样本的平均轮廓系数 X：特征值 labels：被聚类标记的目标值 1silhourtte_score(cust,pre)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——模型保存和加载]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——模型保存和加载sklearn模型的保存和加载API from sklearn.externals import joblib 保存：joblib.dump(estimator, ‘test.pkl’) 加载：estimator = joblib.load(‘test.pkl’) 线性回归的模型保存加载案例 保存 1234567# 使用线性模型进行预测# 使用正规方程求解estimator = LinearRegression()# 训练模型estimator.fit(x_train, y_train)# 保存模型joblib.dump(estimator,'test.pkl') 加载 12model = joblib.load('test.pkl')y_predict = model.predict(x_test)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——分类的评估方法]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB%E7%9A%84%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——分类的评估方法精确率与召回率混淆矩阵 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类) 精确率(Precision)与召回率(Recall) 精确率(查准率)预测结果为正例样本中真实为正例的比例 $$P=\frac{真正例}{预测为真}=\frac{TP}{TP+FP}$$ 召回率(查全率)：真实为正例的样本中预测结果为正例的比例 $$R=\frac{真正例}{实际为真}=\frac{TP}{TP+FN}$$ F1值 F1-score，反映模型的稳健性，F1值与精确率(P)、召回率(R)之间的关系为 $$\begin{split}\frac{2}{F1}&amp;=&amp;\frac{1}{P}+\frac{1}{R}\\F1&amp;=&amp;\frac{2TP}{2TP+FN+FP}\end{split}$$ 分类评估报告API sklearn.metrics.classification_report(y_true, y_pred, labels=[]. target_names=None) y_true：真实目标值 y_pred：估计器预测目标值 labels：指定类别对应的数字 target_names：目标类别名称 return：每个类别精确率与召回率 ROC曲线与AUC指标真正类率TPR与假正类率FPR 真正类率(true positive rate ,TPR) $$TPR=\frac{TP}{TP+FN}$$ 假正类率(false positive rate, FPR) $$FPR=\frac{FP}{TP+FN}$$ ROC曲线 受试者工作特征曲线(receiver operating characteristic curve，简称ROC曲线)，又称为感受性曲线(sensitivity curve)，ROC曲线的横轴就是FP-Rate，纵轴就是TP-Rate，当二者相等时，表示的意义为：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5 AUC指标 ROC曲线下面积(area under curve，简称AUC指标) AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率 AUC的最小值为0.5，最大值为1，取值越高越好 AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5&lt;AUC&lt;1，优于随机猜测。这个分类器(模型)妥善设定阈值的话，能有预测价值。 AUC只能用来评价二分类 AUC非常适合评价样本不平衡中的分类器性能 最终AUC的范围在[0.5, 1]之间，并且越接近1越好 AUC计算API- sklearn.metrics.roc_auc_score(y_true, y_score) - 计算ROC曲线面积，即AUC值 - y_true：每个样本的真实类别，必须为0(反例)，1(正例)标记 - y_score：预测得分，可以使正类的估计概率、置信值或者分类器方法的返回值]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——逻辑回归(LR)]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(LR)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——逻辑回归(LR)逻辑回归的原理逻辑回归(Logistic Regression)是机器学习中的一种分类模型，逻辑回归是一种分类算法。常用于解决解决二分类问题。 输入$$h_{\theta}(x)=\theta_0x_0+\theta_1x_1+theta_2x_2+theta_3x_3+\cdots = \Theta^TX$$ 激活函数 sigmoid函数$$g(z) = \frac{1}{1+e^z}$$其中$z=\Theta^TX$，且输出结果在[0,1]中的一个概率值，默认0.5为阈值(即当输入$z=0$时)12345678910111213141516# sigmoid函数展示import matplotlib.pylab as pltimport numpy as npdef sigmoid(z): return 1/(1+np.exp(-z))x = np.linspace(-10,10,100)y = sigmoid(x)plt.plot(x,y)ax = plt.gca()ax.spines['right'].set_color('none') ax.spines['top'].set_color('none') # 将右边 上边的两条边颜色设置为空 其实就相当于抹掉这两条边ax.xaxis.set_ticks_position('bottom') ax.yaxis.set_ticks_position('left') # 指定下边的边作为 x 轴 指定左边的边为 y 轴ax.spines['bottom'].set_position(('data', 0)) #指定 data 设置的bottom(也就是指定的x轴)绑定到y轴的0这个点上ax.spines['left'].set_position(('data', 0))plt.show() 输入如下sigmoid函数：由图像可知：$$g(z) \begin{cases}&amp;&gt;0.5&amp; z&gt;0\\&amp;=0.5&amp; z=0\\&amp;&lt;0.5&amp; z&lt;0\end{cases}$$ 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例)，另外的一个类别会标记为0(反例)。 逻辑回归中，我们预测$$y = \begin{cases}&amp;1&amp; h_{\theta}(x)\geq0.5\\&amp;0&amp;h_{\theta}(x)&lt;0.5\end{cases}$$又因为$z=\Theta^TX$，可以得到$$y = \begin{cases}&amp;1&amp; \Theta^TX\geq0\\&amp;0&amp; \Theta^TX&lt;0\end{cases}$$ 损失函数逻辑回归的损失，称为对数似然损失，公式如下$$cost(h_{\theta}(x))=\begin{cases}&amp;-log(h_{\theta}(x))&amp;if&amp;y = 1 \\&amp;-log(1-h_{\theta}(x))&amp;if&amp;y = 0\end{cases}$$ 可以化简为$$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)})$$ 优化损失函数同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。 逻辑回归API sklearn.linear_model.LogisticRegression(solver=’liblinear’,penalty=’l2’,C=1.0) solver：优化求解方式(默认通过开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数) sag：根据数据集自动选择，随机平均梯度下降 penalty：正则化的种类 C：正则化力度 默认将类别数量少的当做正例 LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Logistic Regression</tag>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——波士顿房价预测]]></title>
    <url>%2F2018%2F03%2F23%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[项目实战——波士顿房价预测数据介绍 字段名 描述 CRIM 城镇人均犯罪率 ZN 占地面积超过2.5万平方英尺的住宅用地比例 INDUS 城镇非零售业务地区的比例 CHAS 查尔斯河虚拟变量(=1 如果土地在河边;否则是0) NOX 一氧化氮浓度(每1000万份) RM 平均每居民房数 AGE 在1940年之前建成的所有者占用单位的比例 DIS 与五个波士顿就业中心的加权距离 RAD 辐射状公路的可达性指数 TAX 每10000美元的全额物业税率 PTRATIO 城镇师生比例 B 1000(Bk - 0.63)^2 LASTAT 人口中低位较低人群的百分数 MEDV 以1000美元计算的自有住房的中位数(预测值) 需求分析回归中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。 数据分割与标准化 回归预测 线性回归的算法效果评估 线性回归使用正规方程优化线性回归123456789101112131415161718192021222324252627import pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error# 1.获取数据集boston = load_boston()print('boston:\n',boston.DESCR)# 2.划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.traget, random_state=8)# 3.特征工程# 3.1.实例化转化器transfer = StandardScaler()# 3.2.转化数据x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.模型训练# 4.1.实例化估计器estimator = LinearRegression()# 4.2.传入训练数据 进行模型训练estimator.fit(x_train, y_train)# 4.3.模型结果print('正规方程优化的模型结果系数为:\n',estimator.coef_)print('正规方程优化的模型结果偏置为:\n',estimator.intercept_)# 5.模型评估y_predict = estimator.predict(x_test)error = mean_squared_error(y_test, y_predict)print('正规方程优化的结果均方误差为:\n',error) 输出：12345678梯度下降优化的模型结果系数为: [-0.74192506 0.73579142 -0.32384815 0.7751081 -0.75627116 3.20090862 -0.28381128 -2.00606851 0.62594045 -0.42608003 -1.79599478 0.86938128 -3.65789337]梯度下降优化的模型结果偏置为: [22.03114]梯度下降优化的结果均方误差为: 24.267303702468947 使用梯度下降优化线性回归123456789101112131415161718192021# 1.获取数据boston = load_boston()# 2.分割数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=8)# 3.特征工程：标准化# 3.1.实例化转化器transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.模型训练# 4.1.实例化估计器estimator = SGDRegressor()# 4.2.传入训练数据 进行模型训练estimator.fit(x_train, y_train)# 4.3.模型结果print('梯度下降优化的模型结果系数为:\n',estimator.coef_)print('梯度下降优化的模型结果偏置为:\n',estimator.intercept_)# 5.模型评估y_predict = estimator.predict(x_test)error = mean_squared_error(y_test, y_predict)print('梯度嘉奖优化的结果均方误差为:\n',error) 输出：12345678梯度下降优化的模型结果系数为: [-0.74999984 0.53962994 -0.3141274 0.79470893 -0.6952651 3.16754672 -0.13407217 -1.75681033 0.76326425 -0.43241248 -1.81617333 0.85367247 -3.72958928]梯度下降优化的模型结果偏置为: [22.03435752]梯度下降优化的结果均方误差为: 24.164326494635407 岭回归123456789101112131415161718192021# 1.获取数据集boston = load_boston()# 2.划分数据集x_train,x_test,y_train,y_test = train_test_split(boston.data, boston.target,random_state=8)# 3.特征工程：标准化# 3.1.实例化转化器transfer = StandardScaler()# 3.2.传入数据 进行标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.实例化估计器estimator = Ridge()estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print('预测值为:\n',y_predict)# 5.得出模型print('回归系数:\n',estimator.coef_)print('偏置:\n',estimator.intercept_)# 6.模型评估——均方误差error = mean_squared_error(y_test,y_predict)print('均方误差:\n',error) 输出：12345678910111213141516171819202122232425262728293031预测值为: [19.29249509 10.9044467 38.6033501 26.96819247 41.18226428 27.45946881 10.19715176 36.31848668 29.09661004 35.05929037 13.21177162 7.38752095 15.49082238 24.31068901 16.14135614 29.0269088 23.08671884 22.41480902 20.72699958 6.97252278 20.85834773 25.35109948 30.46228136 33.31940905 28.52422234 35.42941678 4.88670106 14.52403709 25.57897217 23.47617658 34.5486346 18.50767426 19.57369094 22.86517531 25.57180901 27.12713609 32.33832511 25.71132131 14.31961378 14.02131841 21.59103876 21.98840868 36.11575743 35.07330795 23.34357369 19.4875536 19.99391045 21.12832977 25.44572146 24.25852502 19.99810516 16.53912451 31.93307255 16.29585862 22.33019461 24.59180264 20.81018269 4.97997805 20.02669797 28.45661718 24.31984154 21.82956466 20.08062459 9.06176246 14.44128436 17.45246292 12.75087695 20.26358367 35.21892937 35.90539814 13.72890447 28.87158618 30.52360529 31.87582711 21.91264297 28.48043189 29.38052967 17.15354022 13.75751895 18.40478712 13.92300614 5.7029655 16.19939976 22.84322293 20.63795823 31.04957851 14.79821092 15.86515596 26.71807603 31.5844528 21.00528122 21.07860099 29.34781794 35.60194418 19.07216227 28.1273874 23.21044017 24.81655864 23.1514886 13.49429145 2.54518224 27.68752227 0.29009813 41.18846989 32.71620063 28.95862556 16.88687507 22.80113434 25.33344261 31.18299913 18.48851751 20.24384335 39.95151299 12.68377544 20.0579597 20.57578142 39.810066 15.81067337 25.53594481 21.70233939 28.65676966 28.58793363 14.92303049 25.0807655 25.38638199 15.52952136 24.05308496]回归系数: [-0.97246454 1.14327275 0.15848304 0.65305661 -1.45541569 2.68212945 -0.17139627 -2.97390427 2.22587256 -1.76604839 -1.91302371 0.8558563 -4.03757414]偏置: 22.52163588390508均方误差: 22.69682807165292]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Linear Regression</tag>
        <tag>Sklearn</tag>
        <tag>Ridge Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——岭回归(RR)]]></title>
    <url>%2F2018%2F03%2F22%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%B2%AD%E5%9B%9E%E5%BD%92(RR)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——岭回归(RR)岭回归的定义岭回归，即在线性回归的基础上加上了L2正则化的限制，从而达到降低过拟合的效果。正则化时，一般不对$\theta_0$进行惩罚，正则化的本质即是减少$\theta_j$的平方范数，从而降低模型复杂度。 岭回归API sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver=’auto’,normalizer=False) 具有L2正则化的线性回归 alpha：正则化参数，$\lambda$ $\lambda$取值：0~1 1~10 solver：会根据数据自动选择优化方法 sag：如果数据集、特征都比较大，选择该随机梯度下降优化 normalize：数据是否进行标准化 normalize=False：可以在fit之前调用preprocessing.StandardScaler标准化数据 Ridge.coef_：回归权重 Ridge.intercept_：回归偏置 Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG) SAG()Stochastic average gradient)，即随机平均梯度下降，在SGD中，由于收敛的速度太慢，所以后面就有人提出SAG基于梯度下降的算法。SAG中的S是随机（Stochastic），A是平均（average），G是梯度（gradient）的意思。可以看到SAG是一种加速版本的SGD。直观上看，利用的信息量大了，收敛速度就应该比单纯用一个样本估计梯度值的SGD要快。在一定条件下SAG线性收敛，与全梯度下降（FGD）收敛速度一样。但是SAG带来的问题就是需要内存来维护（保存）每一个旧梯度值。即用空间换时间。 sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin) 具有L2正则化的线性回归，可以进行交叉验证 coef_：回归系数 12345class _BaseRidgeCV(LinearModel): def __init__(self, alpha=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False,scoring=None, cv=None, gcv_mode=None, store_cv_values=False):]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Ridge Regression</tag>
        <tag>SAG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——线性回归(LR)]]></title>
    <url>%2F2018%2F03%2F22%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(LR)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——线性回归(LR)线性回归的原理目标值是连续变量同时目标值(因变量)与特征值(自变量)呈线性关系 线性回归的定义“回归”即拟合。线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。 只有一个自变量的情况称为单变量回归，多个自变量的情况称为多变量回归 线性回归的公式可以表示为$$h_{\theta}(x)=\theta_0x_0+\theta_1x_1+theta_2x_2+theta_3x_3+\cdots = \Theta^TX$$其中$\Theta = [\theta_0\ \theta_1\ \theta_2\cdots]^T\ \ X=[x_0\ x_1\ x_2\cdots]^T$，$x_0=1$，即$\Theta$与$X$均为列向量，且维度相同。 单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系，多特征即在更高维度进行扩展。 损失函数使用最小二乘法定义损失函数，用来衡量预测值与期望值之间的误差损失函数的公式可以表示为$$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$$ 优化算法线性回归中常用的优化算法 正规方程$$\Theta = (X^TX)^{-1}X^Ty$$其中，$X$为特征矩阵，y为目标值矩阵 优点：直接求解，且误差较小 缺点：当特征过多且过于复杂(维度&gt;10000)时，求解速度太慢且得不到结果 梯度下降$$\theta_1 := \theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta)$$$$\theta_0 := \theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta)$$其中，$\alpha$为学习速率，需要手动指定(超参数)，偏导部分表示梯度下降方向，直到得到最小的$J(\theta)$ 优点：面对大量特征也你能求得结果，且能够找到较好的结果 缺点：速度相对较慢，需要多次迭代，且需要调整学习率$\alpha$，当学习率调整不当时，可能会出现函数不收敛的情况 线性回归API正规方程线性回归((Normal Equation)) sklearn.linear_model.LinearRegression(fit_intercept=True) 通过正规方程优化 fit_intercept：是否计算偏置 LinearRegression.coef_：回归系数 LinearRegression.intercept_：偏置 随机梯度下降线性回归(Stochastic Gradient Descent,SGD) sklearn.linear_model.SGDRegressor(loss=’squared_loss’,fit_intercept=True,learning_rate=’invscaling’,eta0=0.01) SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型 loss：损失类型 loss=’squared_loss’：普通最小二乘法 fit_intercept：是否计算偏置 learning_rate：string，optional 学习率填充 ‘constant’：eta = eta0 ‘optimal’：eta = 1.0 / (alpha * (t + t0)) [default] ‘invscaling’：eta = eta0 / pow(t, power_t) power_t = 0.25：存在父类当中 对于一个常数值的学习率来说，可以使用learning_rate=’constant’，并使用eta0来指定学习率 SGDRegressor.coef_：回归系数 SGDRegressor.intercept_：偏置]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Linear Regression</tag>
        <tag>Sklearn</tag>
        <tag>Normal Equation</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——泰坦尼克号乘客生存预测]]></title>
    <url>%2F2018%2F03%2F21%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[项目实战——泰坦尼克号乘客生存预测使用决策树12345678910111213141516171819202122232425262728293031323334import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier# 获取数据tanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')# 数据预处理# 缺失值处理tanic['age'].fillna(tanic['age'].mean(),inplace=True)# 提取特征值与目标值# 注意x的tanic内部的columns为listx = tanic[['pclass','age','sex']]y = tanic['survived']# 特征工程 字典特征提取# 实例化转化器transfer = DictVectorizer(sparse=False)# x.to_dict(orient='records') 按行转字典x = transfer.fit_transform(x.to_dict(orient='records'))# 数据集划分x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)# 模型训练与评估# 实例化估计器estimator = DecisionTreeClassifier()estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测值为',res[res==True].size/res.size)# 方法2 计算模型准确值print('模型的准确值为:\n',estimator.score(x_test,y_test)) 输出1230.7918781725888325模型准确值为: 0.7918781725888325 使用随机森林1234567891011121314151617181920from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCV# 实例化估计器estimator = RandomForestClassifier()# 网格搜索优化随机森林模型param_dict = &#123;'n_estimators':[120,200,300,500,800,1200],'max_depth':[5,8,15,25,30]&#125;estimator = GridSearchCV(estimator,param_grid=param_dict,cv=5)estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_test# print('预测值为:\n',y_predict)# print('比较真实值与预测值结果为:\n',res)print(res[res==True].size/res.size)# 方法2 计算模型准确值print('模型准确值为:\n',estimator.score(x_test,y_test))print('在交叉验证中最好的结果:\n',estimator.best_score_)print('最好的参数模型:\n',estimator.best_estimator_)# print('每次交叉验证后的结果准确率为:\n',estimator.cv_results_) 输出123456789101112130.817258883248731模型准确值为: 0.817258883248731在交叉验证中最好的结果: 0.8389553862894451最好的参数模型: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Random Forest</tag>
        <tag>Decision Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——集成学习方法之随机森林(RF)]]></title>
    <url>%2F2018%2F03%2F21%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97(RF)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——集成学习方法之随机森林(RF)集成学习方法的定义集成学习通过建立几个模型组合地来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的作出预测。 随机森林的定义在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。 随机森林原理过程学习算法根据下列算法而建造每棵树： 用N来表示训练用例(样本)的个数，M表示特征数目 1.一次随机选出一个样本，重复N次(有可能出现重复的样本) 2.随机去选出m个特征，m&lt;&lt;M，建立决策树 采用bootstrap抽样 即随机有放回抽样 随机，如果不进行随机抽样，每棵树的训练集都相同，从而导致最终训练出的树分类结果也完全相同 有放回，如果不是有放回的抽样，每棵树的训练样本都不同，都是没有交集的，这样每棵树可能是”有偏的”，也可能是绝对”片面的”，即每棵树训练出来都是有很大的差异；而随机森林最后分类取决于多棵树(弱分类器)的投票表决。 随机森林API sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2) 随机森林分类器 n_estimators：integer，optional(default=10)森林里的树木数量 criteria：string，可选(default=’gini’) 分割特征的测量方法 max_depth：integer或None，可选(默认=无) 树的最大深度 max_features=’auto’：每个决策树的最大特征数量 If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features 超参数：n_estimator, max_depth, min_samples_split, min_samples_leaf 实例化随机森林估计器12from sklearn.ensemble import RandomForestClassifier()estimator = RandomForestClassifier() 定义超参数的选择列表1param_dict = &#123;"n_estimators":[120,200,300,500,800,1200],"max_depth":[5,8,15,25,30]&#125; 使用GridSearchCV进行网格搜索1234# 超参数调优 并设置2折验证estimator = GridSeachCV(estimator,param_grid=param_dict,cv=2)estimator.fit(x_train,y_train)print('随机森林的预测准确率为:\n',estimator.score(x_test,y_test)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Random Forest</tag>
        <tag>Decision Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——决策树(DT)]]></title>
    <url>%2F2018%2F03%2F21%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91(DT)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——决策树(DT)原理信息熵、信息增益等 优点 简单的理解和解释，树木可视化 缺点 容易创建出泛化能力较差且过于复杂的模型，称之为过拟合 改进 剪枝cart算法(决策树API中已实现)企业重要决策，由于决策树很好的分析能力，在决策过程应用较多， 可以选择特征信息熵的定义在信息论与概率统计中，熵是表示随机变量不确定性的度量，熵的公式定义为：$$H(X)=-\sum{P(x_i)logP(x_i)}$$其中，对数一般以2为底或以e为底(自然对数)，这时熵的单位分别称作比特(bit)或纳特(nat)。由定义可知，熵只依赖于X的分布，而与X的取值无关。 信息增益当上和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵(empirical entropy)和经验条件熵(conditional entropy)。此时，如果有0概率，令0log0=0。特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。$$g(D,A)=H(D)-H(D|A)$$信息增益表示得知特征X的信息而息的不确定性减少的程度使得类Y的信息熵减少的程度。经验熵的计算：$$H(D)=-\sum\limits_{|k=1|}^{|K|}\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$$其中，$C_k$表示属于某个类别的样本数。经验条件熵的计算：$$H(D|A)=\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}\sum\limits_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}$$决策树的原理还有其他方法 ID3 信息增益 最大的准则 C4.5 信息增益比 最大的准则 CART 分类树：基尼系数 最小的准则 在sklearn中可以选择划分的默认原则 优势：划分更加细致 决策树API sklearn.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=None,random_state=None) 决策树分类器 criterion：默认是’gini’系数，也可以选择信息增益的熵’entropy’ max_depth：树的深度大小 random_state：随机数种子 决策树可视化保存树的结构到dot文件 sklearn.tree.export_graphviz(estimator,out_file=’tree.dot’,feature_names=[‘’,’’]) 该函数能够导出DOT格式 网站显示结构 http://webgraphviz.com/ 输入DOT格式内容 Demo以鸢尾花数据集的DT为例：1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier# 获取数据集与分割数据集iris = load_iris()x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 模型训练与评估# 实例化一个估计器estimator = DecisionTreeClassifier()# 传入训练数据集，进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测值为:\n',y_predict)print('比较真实值与预测值结果为:\n',res)print(res[res==True].size/res.size)# 方法2 计算模型准确值score = estimator.score(x_test,y_test)print('模型准确值为:\n',score) 输出：1234567891011预测值为: [0 0 0 2 1 0 0 2 2 1 1 0 1 1 1 2 2 2 2 2 1 0 1 1 1 0 2 0 0 2 0 0 0 2 1 1 1 1 0 1 1 0 1 1 2]比较真实值与预测值结果为: [ True True True True True True True True True True True True False True False True True True False True True True True True True True True True True True True True True True True True False True True True True True True True True]0.9111111111111111模型准确值为: 0.9111111111111111]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Decision Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——20类新闻分类]]></title>
    <url>%2F2018%2F03%2F20%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%9420%E7%B1%BB%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[项目实战——20类新闻分类流程分析 获取数据 划分数据集 特征抽取 Tf-idf 朴素贝叶斯 朴素贝叶斯新闻文章分类12345678910111213141516171819202122232425262728from sklearn.datasets import fetch_20newsgroupsfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizer# 1.获取数据news = fetch_20newsgroups()# 2.划分数据集x_train,x_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.3)# 3.特征抽取 Tf-idf# 3.1.实例化转化器transfer = TfidfVectorizer()# 3.2.转化数据x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.朴素贝叶斯# 4.1.实例化估计器estimator = MultinomialNB()estimator.fit(x_train,y_train)# 4.2.模型评估# 方法1 比较真实值与预测值y_predcit = estimator.predict(x_test)res = y_predict == y_testprint('预测结果为:\n',y_predict)print('比对真实值和预测值:\n',res)print(res[res==True].size/res.size)# 方法2 直接计算准确率score = estimator.score(x_test,y_test)print('准确率为:\n',score) 输出：123456预测值为: [ 4 14 13 ... 14 10 9]比较真实值与预测值结果为: [ True True True ... True True True]模型准确率为: 0.8176730486008836]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——朴素贝叶斯(NB)]]></title>
    <url>%2F2018%2F03%2F20%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF(NB)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——朴素贝叶斯(NB)概率的定义 概率定义为一件事情发生的可能性 $P(X)$：取值在[0, 1] 联合概率、条件概率与相互独立 联合概率：包含多个条件，且所有条件同事成立的概率 记作：$P(A, B)$ 条件概率：就是事件A在另一个事件B已经发生条件下的发生概率 记作：$P(A|B)$ 相互独立：如果$P(A, B)=P(A)P(B)$，则称事件A与事件B相互对立 朴素贝叶斯的定义朴素贝叶斯(naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方法。 优点 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率 对缺失数据不太敏感，算法也比较简单，常用语文本分类 分类准确度高，速度快 缺点 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好 贝叶斯公式公式$$P(C|W) = \frac{P(W|C)P(C)}{P(W)}$$其中W为给定文档的特征值(频数统计，预测文档提供)，C为文档的类别 朴素贝叶斯公式可以理解为$$\begin{split}P(C|F1,F2,\cdots)&amp;=\frac{P(F1,F2,\cdots|C)P(C)}{P(F1,F2,\cdots)}\\&amp;=\frac{P(F1|C)P(F2|C),\cdots{P(C)}}{P(F1)P(F2)\cdots}\end{split}$$且对于任意C=c其分母都一致，则在分类时仅需要比较分子大小 贝叶斯估计若任意一个$P(Fi|C)=0$即后验概率为0，则会导致分类产生偏差，此时采用贝叶斯估计 公式$$P(Fi|C)=\frac{N_i+\alpha}{N+\alpha{m}}$$常取$\alpha=1$，此时称为拉普拉斯平滑(Laplace smoothing)。 API sklearn.native_bayes.MultinomialNB(alpha = 1.0) 朴素贝叶斯分类 alpha：拉普拉斯平滑系数]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——预测FaceBook签到位置]]></title>
    <url>%2F2018%2F03%2F20%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8BFaceBook%E7%AD%BE%E5%88%B0%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[项目实战——预测FaceBook签到位置train and testrow_id：登记时间的IDx y：坐标准确性：定位准确性时间：时间戳place_id：业务的ID(预测目标) 流程分析 数据预处理 缩小数据集范围 时间特征提取 将签到位置少于n个用户的删除 数据集划分 特征工程：标准化 KNN算法 GSCV优化 模型优化 代码实现 获取数据集 123456# 项目实战——预测facebook签到位置import pandas as pdfrom sklearn.model_selection import train_test_split,GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfacebook = pd.read_csv('FBlocation/train.csv') 缩小数据的范围 选择有用的时间特征和取出标签较少的地点 12345678910111213141516171819# 数据预处理# 缩小数据集 去1.25&lt;x&lt;1.5 and 2.25&lt;y&lt;2.5facebook = facebook.query('x&lt;1.5&amp;x&gt;1.25&amp;y&gt;2.25&amp;y&lt;2.5')# 时间特征提取# pd.to_datetime(column,unit='s') 把对应列的数据按单位秒转化为时间time_value = pd.to_datetime(facebook['time'],unit='s')# pd.DatetimeIndex 将字符类型日期转化为时间戳索引time_value = pd.DatetimeIndex(time_value)# 将时间戳索引添加到facebook数据中facebook['day'] = time_value.dayfacebook['hour'] = time_value.hourfacebook['weekday'] = time_value.weekday# # 删除用户分享小于3的地点# 聚合统计place_count = facebook.groupby(['place_id']).count()# 查询分享大于3的所有地点place_count = place_count.query('row_id&gt;3')# 筛选facebook中的数据facebook = facebook[facebook['place_id'].isin(place_count.index)] 取出数据的特征值和目标值 1234# 数据集划分# 选择多列时需传入列表x = facebook[['x','y','accuracy','day','hour','weekday']]y = facebook['place_id'] 划分训练集与测试集 12# 划分训练集与测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=8) 标准化处理 12345# 特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)# 此处x_test在标准化时需使用x_train的均值与方差x_test = transfer.transform(x_test) K近邻算法模型进行预测 12345678910111213141516171819202122# 模型训练与评估# 实例化估计器estimator = KNeighborsClassifier()# 准备超参数param_dict = &#123;'n_neighbors':[3,5,7,9]&#125;# 运用网格搜索参数优化KNN算法# cv=5 为设置S折验证中的S为5estimator = GridSearchCV(estimator,param_grid=param_dict,cv=5)# 传入训练数据，进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测值为:\n',y_predict)# print('比较真实值与预测值的结果为:\n',res)print(res[res==True].size/res.size)# 方法2 计算模型的准确值print('模型的准确率为:\n',estimator.score(x_test,y_test))print('在交叉验证中最好的结果:\n',estimator.best_score_)print('最好的参数模型:\n',estimator.best_estimator)print('每次交叉验证后的结果准确率为:\n',estimator.cv_results_) 输出：12345678910111213141516预测值为: [4760271365 4760271365 9841447845 ... 9841447845 1226687693 7536975002]0.46121318168562264模型准确率为: 0.46121318168562264在交叉验证中最好的结果: 0.4498468845697144最好的参数模型: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=9, p=2, weights='uniform')每次交叉验证后的结果准确率为: &#123;'mean_fit_time': array([0.00774164, 0.00718284, 0.00750437, 0.00720239]), 'std_fit_time': array([0.00104153, 0.00050873, 0.00082317, 0.00051814]), 'mean_score_time': array([0.06678772, 0.07659383, 0.08839059, 0.09959669]), 'std_score_time': array([0.0063074 , 0.00286544, 0.00336546, 0.00472586]), 'param_n_neighbors': masked_array(data=[3, 5, 7, 9], mask=[False, False, False, False], fill_value='?', dtype=object), 'params': [&#123;'n_neighbors': 3&#125;, &#123;'n_neighbors': 5&#125;, &#123;'n_neighbors': 7&#125;, &#123;'n_neighbors': 9&#125;], 'split0_test_score': array([0.40610826, 0.43241609, 0.43664953, 0.42999698]), 'split1_test_score': array([0.4165132 , 0.4407612 , 0.44229589, 0.4490485 ]), 'split2_test_score': array([0.41081417, 0.43629584, 0.44655065, 0.45090118]), 'split3_test_score': array([0.42793509, 0.45020681, 0.45911549, 0.45752466]), 'split4_test_score': array([0.4295935 , 0.45528455, 0.46178862, 0.46308943]), 'mean_test_score': array([0.41797388, 0.44278483, 0.44903444, 0.44984688]), 'std_test_score': array([0.00923467, 0.00850695, 0.00966491, 0.01127239]), 'rank_test_score': array([4, 3, 2, 1], dtype=int32), 'split0_train_score': array([0.64778636, 0.59476918, 0.57176619, 0.54970852]), 'split1_train_score': array([0.65141646, 0.59664129, 0.56846896, 0.5480656 ]), 'split2_train_score': array([0.64710944, 0.59524368, 0.56841117, 0.54994915]), 'split3_train_score': array([0.64644579, 0.59254939, 0.56159589, 0.54168611]), 'split4_train_score': array([0.64474702, 0.59306823, 0.5649853 , 0.54239517]), 'mean_train_score': array([0.64750102, 0.59445436, 0.5670455 , 0.54636091]), 'std_train_score': array([0.00220288, 0.00148695, 0.00346753, 0.00359357])&#125;]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——K-近邻算法(KNN)]]></title>
    <url>%2F2018%2F03%2F19%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95(KNN)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——K-近邻算法(KNN)定义如果一个样本在特征空间中的k各最相似(即特征空间中最邻近)的样本中的大多数属于另一个类别，则该样本也属于这个类别。 优点： 简单，易于理解，易于实现，无需训练 缺点： 懒惰算法，对测试样本分类时的计算量大，内存开销大 必须指定K值，K值选择不当则分类精度不能保证 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试 K-近邻算法API sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’) n_neighbors:：int，可选(默认5)，k_neighbors查询默认使用的邻居数 algorithm：{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}，可选用于最近邻居的算法 ‘ball_tree’将会使用BallTree ‘kdtree’将会使用KDTree ‘auto’将尝试根据传递给fit方法的值来决定何时的算法。 不同实现方式影响效率 Demo以鸢尾花数据集的KNN为例： 12345678910111213141516171819202122232425262728293031# 鸢尾花种类预测from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier'''KNN算法对鸢尾花数据集分类'''# 1.获取数据集和分割数据集iris = load_iris()x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 2.特征工程，标准化# 2.1.实例化转化器transfer = StandardScaler()# 2.2.对训练集和测试集进行标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 3.模型训练和评估# 3.1.实例化估计器estimator = KNeighborsClassifier()# 3.2.传入训练数据集，进行机器学习estimator.fit(x_train, y_train)# 3.3.模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测结果为:\n', y_predict)print('比对真实值和预测值L\n', res)print(res[res==True].size/res.size)# 方法2 直接计算准确率score = estimator.score(x_test, y_test)print('准确率为:\n', score) 输出：1234567891011预测结果为: [0 0 0 2 1 0 0 2 2 1 1 0 1 1 1 2 2 2 1 2 1 0 1 1 1 0 2 0 0 2 0 0 0 2 1 1 1 1 0 1 1 0 1 1 2]比对真实值和预测值L [ True True True True True True True True True True True True False True False True True True True True True True True True True True True True True True True True True True True True False True True True True True True True True]0.9333333333333333准确率为: 0.9333333333333333]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>k-Nearest Neighbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——模型选择与调优]]></title>
    <url>%2F2018%2F03%2F19%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——模型选择与调优交叉验证交叉验证目的：为了让被评估的模型更加准确可信 超参数搜索-网格搜索(Grid Search)在模型选择与调优的过程中，需要手动指定的参数(如k-邻近算法中的K值)叫做超参数。但是手动过程繁杂，需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。 模型选择与调优API sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None) 对估计器的指定参数值进行详尽搜索 estimator：估计器对象 param_grid：估计器参数(dict)(‘n_neighbors’:[1,3,5]) cv：指定S折交叉验证的S fit：输入训练数据 score：准确率 结果分析： bestscore：在交叉验证中验证的最好结果 bestestimator：最好的参数模型 cvresults：每次交叉验证后的验证集准确率结果和训练集准确率结果 Demo以鸢尾花数据集的KNN为例：1234567891011121314151617181920212223242526272829303132333435363738# 鸢尾花种类预测from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import GridSearchCV'''KNN算法对鸢尾花数据集分类'''# 1.获取数据集和分割数据集iris = load_iris()x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 2.特征工程，标准化# 2.1.实例化转化器transfer = StandardScaler()# 2.2.对训练集和测试集进行标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 3.模型训练和评估# 3.1.实例化估计器estimator = KNeighborsClassifier()# 3.2.网络搜索和交叉验证# 3.2.1.准备超参数param_dict = &#123;'n_neighbors':[1,3,5,7,9]&#125;estimator = GridSearchCV(estimator, param_grid=param_dict, cv=6)# 3.2.传入训练数据集，进行机器学习estimator.fit(x_train, y_train)# 3.3.模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测结果为:\n', y_predict)print('比对真实值和预测值L\n',res)print(res[res==True].size/res.size)# 方法2 直接计算准确率score = estimator.score(x_test, y_test)print('准确率为:\n', score)print("在交叉验证中验证的最好结果：\n", estimator.best_score_)print("最好的参数模型：\n", estimator.best_estimator_)print("每次交叉验证后的准确率结果：\n", estimator.cv_results_) 输出：12345678910111213141516171819202122预测结果为: [0 0 0 2 1 0 0 2 2 1 1 0 1 1 1 2 2 2 2 2 1 0 1 1 1 0 2 0 0 2 0 0 0 2 1 1 1 1 0 1 1 0 1 1 2]比对真实值和预测值L [ True True True True True True True True True True True True False True False True True True False True True True True True True True True True True True True True True True True True False True True True True True True True True]0.9111111111111111准确率为: 0.9111111111111111在交叉验证中验证的最好结果： 0.9714285714285714最好的参数模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=1, p=2, weights='uniform')每次交叉验证后的准确率结果： &#123;'mean_fit_time': array([0.00038306, 0.00037567, 0.00055607]), 'std_fit_time': array([1.02261530e-04, 4.53648737e-05, 2.36380916e-04]), 'mean_score_time': array([0.00077566, 0.00076612, 0.00082179]), 'std_score_time': array([0.00018651, 0.00024699, 0.00017762]), 'param_n_neighbors': masked_array(data=[1, 3, 5], mask=[False, False, False], fill_value='?', dtype=object), 'params': [&#123;'n_neighbors': 1&#125;, &#123;'n_neighbors': 3&#125;, &#123;'n_neighbors': 5&#125;], 'split0_test_score': array([1. , 0.88888889, 0.88888889]), 'split1_test_score': array([0.94444444, 0.94444444, 0.94444444]), 'split2_test_score': array([1., 1., 1.]), 'split3_test_score': array([1., 1., 1.]), 'split4_test_score': array([1., 1., 1.]), 'split5_test_score': array([0.875, 0.875, 0.875]), 'mean_test_score': array([0.97142857, 0.95238095, 0.95238095]), 'std_test_score': array([0.04575725, 0.05252505, 0.05252505]), 'rank_test_score': array([1, 2, 2], dtype=int32), 'split0_train_score': array([1. , 0.97701149, 0.96551724]), 'split1_train_score': array([1. , 0.98850575, 0.96551724]), 'split2_train_score': array([1. , 0.95402299, 0.95402299]), 'split3_train_score': array([1. , 0.97701149, 0.96551724]), 'split4_train_score': array([1. , 0.97727273, 0.95454545]), 'split5_train_score': array([1., 1., 1.]), 'mean_train_score': array([1. , 0.97897074, 0.96752003]), 'std_train_score': array([0. , 0.01394093, 0.01537038])&#125;]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——转换器和估计器]]></title>
    <url>%2F2018%2F03%2F19%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——转换器和估计器转换器即特征工程的实现(面向对象)步骤 实例化转换器 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用) fit_transform 先进行fit，再进行transform fit 保存相关参数，如均值、方差、标准差等 transform 使用转化器中已保存的相关参数进行转化 估计器即sklearn机器学习算法的实现(面向对象)步骤 实例化估计器 用于分类的估计器： sklearn.neighbors k-近邻算法 sklearn.naive_bayes 贝叶斯 sklearn.linear_model.LogisticRegression 逻辑回归 sklearn.tree 决策树与随机森林 用于回归的估计器 sklearn.linear_model.LinearRegression 线性回归 sklearn.linear_model.Ridge 岭回归 用于无监督学习的估计器 sklearn.cluster.KMeans 聚类 传入训练数据集，进行机器训练 模型估计 比较真实值与预测值 y_predict = estimator.predict(x_test) 计算模型准确率 estimator.score(x_test,y_test)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Transfer</tag>
        <tag>Estimator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征降维]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征降维降维的定义降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程 降低随机变量的个数 相关特征(correlated feature) 由于在进行训练的时使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大。 降维的原因 使得数据集更易使用 降低很多算法的计算开销 去除噪声 使得结果易懂 降维的几种方法 1.特征选择(Feature Selection) 2.主成分分析(Principal Component Analysis,PCA) 3.因子分析(Factor Analysis) 4.独立成分分析(Independent Component Analysis,ICA)特征选择sklearn.feature_selection定义数据中包含冗余或无关变量(或称特征、属性、指标等)，旨在从原有特征中找出主要特征方法 Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联 方差选择法：低方差特征过滤 相关系数 Embedded(嵌入式)：算法自动选择特征(特征与目标值之间的关联) 决策树：信息熵、信息增益 正则化：L1、L2 深度学习：卷积等 过滤式低方差特征过滤删除低方差的一些特征 特征方差小：某个特征大多样本的值比较相近 特征方差大：某个特征很多样本的值都有差别 API sklearn.feature_selection.VarianceThreshold(threshold=0.0) 删除所有低方差特征 Variance.fit_transform(X) X：numpy array格式的数据[n_samples,n_features] 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同的特征。123456789101112# 删除低方差特征值from sklearn.feature_selection import VarianceThresholdimport pandas as pd# 获取数据data = pd.read_csv('data.csv')# 查看列print(data.columns)print(data[data.columns[1:-2]].shape)# 实例化转换器，同时设置阈值transfer = VarianceThreshold(threshold=3)# 转换数据，选择有效特征列data = transfer.fit_transform(data[data.columns[1:-2]]) 相关系数 皮尔逊相关系数(Pearson Correlation Coefficient) 反应变量之间相关关系密切程度的统计指标 公式$$r = \frac{n\sum{xy}-\sum{x}\sum{y}}{\sqrt{n\sum{x^2}-(\sum{x})^2\sqrt{n\sum{y^2}-(\sum{y})^2}}}$$ 特点 相关系数的值介于-1与+1之间，即$-1\leq{r}\leq{+1}$ 当$r&gt;0$时，表示量变量正相关，$r&lt;0$时，两变量为负相关 当$|r|=1$时，表示量变量为完全相关，当$r=0$时，表示量变量无相关关系 当$0&lt;|r|&lt;1$时，表示量变量存在一定程度的相关。且$|r|$越接近1，量变量间线性关系越密切；$|r|$越接近于0，表示量变量的新型相关越弱 一般可以按三级划分：$|r|&lt;0.4$为低度相关;$0.4\geq{|r|}\geq{0.7}$为显著相关；$0.7\geq{|r|}\geq{1}$为高度线性相关 API from scipy.stats import pearsonr x : (N,) array_like y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value 1234567data = pd.read_csv('data.csv')factor = data.columns[1:-2]for i in range(len(factor)): for j in range(i,len(factor)-1): # 错位比较 x = pearsonr(data[factor[i]],data[factor[j+1]]) print(factor[i],factor[j+1],x) 画图观察 123456import pandas as pdimport matplotlib.pylab as pltdata = pd.read_csv('data.csv')# 选择所有有效特征列pd.scatter_matrix(data[data.columns[1:-2]],figsize=(20,8))plt.show() PCAPCA定义高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量 PCA作用数据维数压缩，尽可能降低原数据的维数(复杂度)，损失少量信息 优点：降低数据的复杂性，识别最重要的多个特性 缺点：不一定需要，且可能损失有用信息 适用数据类型：数值型数据 PCA应用应用于回归分析或者聚类分析中 API sklearn.decomposition.PCA(n_components=None) 将数据分解为较低维数空间 n_components: 小数：表示保留百分之多少信息 整数：减少到多少个特征 PCA.fit_transform(X) X：numpy array格式的数据[n_samples,n_features] 返回值：转换后指定维度的array 12345678from sklearn.decomposition import PCA# 获取数据data = [[2,8,4,5],[6,3,0,8],[5,4,9,1]]# 实例化一个转换器(保留3个特征)transfer = PCA(n_components=3)# 传入数据进行转化data = transfer.fit_transform(data)print(data) 输出：123[[ 1.22879107e-15 3.82970843e+00 2.65047672e-17] [ 5.74456265e+00 -1.91485422e+00 2.65047672e-17] [-5.74456265e+00 -1.91485422e+00 2.65047672e-17]]]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征预处理]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征预处理特征预处理定义通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程 特征预处理内容： 数据型数据的无量纲化 归一化 标准化 特征预处理的意义 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征 特征预处理sklearn.preprocessing 归一化定义通过对原始数据进行变换把数据映射到(feature_range，默认为[0,1])之间。由于最大值最小值是变化的，并且最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。 公式$$X’=\frac{x-min}{max-min}&emsp;X’’=X’*(max-min)+min$$ 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0 API sklearn.preprocessing.MinMaxScaler(feature_range(0,1)…) MinMaxScalar.fit_transform(X) X : array-like, shape (n_samples, n_features)The data. feature_range : tuple (min, max), default=(0, 1)Desired range of transformed data. axis : int (0 by default)axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample. copy : boolean, optional, default is TrueSet to False to perform inplace scaling and avoid a copy (if the input is already a numpy array). 返回值：转换后的形状相同的array 1234567891011import pandas as pdfrom sklearn.preprocessing import MinMaxScaler# 1.获取数据data = pd.read_csv("data.txt")# 2.实例化一个转换器类# feature_range 期望的转换数据范围，即输出在2到3之间transfer = MinMaxScaler(feature_range=(2, 3))# 3.调用fit_transformdata = transfer.fit_transform(data[columns])# 4.输出print(data) 标准化定义通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 公式$$X’=\frac{x-mean}{\sigma}$$ 作用于每一列，mean为平均值，σ为标准差 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。 API sklearn.preprocessing.StandardScaler() 处理之后每列来说所有数据都聚集在均值0附近标准差差为1 StandardScaler.fit_transform(X) X ： numpy array格式的数据[n_samples,n_features] 返回值：转换后的形状相同的array 1234567891011121314import pandas as pdfrom sklearn.preprocessing import StandardScaler# 1.获取数据data = pd.read_csv("data.txt")# 2.实例化一个转换器类# feature_range 期望的转换数据范围，即输出在2到3之间transfer = StandardScaler(feature_range=(2, 3))# 3.调用fit_transformdata = transfer.fit_transform(data[columns])# 4.输出print(data)# 可以使用transfer.mean_查看平均值，transfer.var_查看方差print('均值',transfer.mean_)print('方差',transfer.var_)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征抽取]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征抽取特征抽取 sklearn.feature_extraction 将任意数据（如文本或图像）转换为可用于机器学习的数字特征，即特征值化。特征值化是为了计算机更好的去理解数据。 字典特征提取 文本特征提取 图像特征提取 字典特征提取对字典数据进行特征值化 sklearn.feature_extraction.DictVectorizer(sparse=True….) DictVectorizer.fit_transform(X) X：字典或者包含字典的迭代器返回值，返回sparse矩阵，使用sparse矩阵可以节省内存，效率高。 DictVectorizer.inverse_transform(X) X：array数组或者sparse矩阵，返回转换之前数据格式 DictVectorizer.get_feature_names() 返回类别名称 12345678910111213141516# 字典特征提取from sklearn.feature_extraction import DictVectorizer# 1.获取数据data = [&#123;'course': '语文','score':100&#125;,&#123;'course': '数学','score':60&#125;,&#123;'course': '英语','score':30&#125;]# 2.实例化一个转换器# 此处可以传入sparse=False直接返回arraytransfer = DictVectorizer()# 3.对数据进行转换data = transfer.fit_transform(data)# 4.输出结果print('特征名称\n',transfer.get_feature_names())# 返回sparse矩阵 节省内存 效率高print('data\n',data)print('type\n',type(data)) 输出：1234567891011特征名称 ['course=数学', 'course=英语', 'course=语文', 'score']data (0, 2) 1.0 (0, 3) 100.0 (1, 0) 1.0 (1, 3) 60.0 (2, 1) 1.0 (2, 3) 30.0type &lt;class 'scipy.sparse.csr.csr_matrix'&gt; 文本特征提取对文本数据进行特征值化 sklearn.feature_extracion.text.CountVectorizer(stop_words=[]) 返回词频矩阵，忽略了单个字母单词和符号作为特征值 stop_words：iter,限制某些单词作为特征值英文文本特征提取1234567891011121314151617# 文本特征提取from sklearn.feature_extraction.text import CountVectorizer'''英文文本特征提取'''# 获取数据data = ["life is short,i like python","life is too long,i dislike python"]# 实例化一个转换器# stop_words iter,限制某些单词作为特征值# api忽略了单个字母单词和符号作为特征值transfer = CountVectorizer(stop_words=['is'])# 传入数据进行转换data = transfer.fit_transform(data)print('特征名称\n',transfer.get_feature_names())# 输出是特征出现频率print('data\n',data)print('type\n',type(data))# toarray()转ndarray对象print(data.toarray()) 输出：12345678910111213141516特征名称 ['dislike', 'life', 'like', 'long', 'python', 'short', 'too']data (0, 4) 1 (0, 2) 1 (0, 5) 1 (0, 1) 1 (1, 0) 1 (1, 3) 1 (1, 6) 1 (1, 4) 1 (1, 1) 1type &lt;class 'scipy.sparse.csr.csr_matrix'&gt;[[0 1 1 0 1 1 0] [1 1 0 1 1 0 1]] 中文文本特征提取先使用jieba分词进行分词123456789import jiebafrom sklearn.feature_extraction.text import CountVectorizer# jieba分词 中文文本特征提取'''中文文本特征提取演示'''def cut_word(data): text = ' '.join(list(jieba.cut(data))) return texttext = "人生苦短，我喜欢Python；生活太长久，我不喜欢Python"cut_word(text) 再进行特征提取123456789101112text_list = []data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。", "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。", "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]for s in data: text_list.append(cut_word(s))print(text_list)# 实例化一个转换器transfer = CountVectorizer()data = transfer.fit_transform(text_list)print('特征值名称\n',transfer.get_feature_names())print(data.toarray()) 输出：123456789['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']特征值名称 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样'][[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0] [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1] [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]] TF-IDF文本特征提取 TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 TF-IDF是分类机器学习算法进行文章分类中前期数据处理方式公式 词频(term frequency, tf)指的是某一个给定的词语在该文件中出现的频率 逆向文档频率(inverse document frequency,idf)是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到$$tfidf_{i,j}=tf_{ij}\times{idf_i}$$最终得出结果可以理解为重要程度 注：假如一篇文件的总词语数是100个，而词语”非常”出现了5次，那么”非常”一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现”非常”一词的文件数。所以，如果”非常”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后”非常”对于这篇文档的tf-idf的分数为0.05 * 3=0.15 12345678910111213141516171819202122import jiebafrom sklearn.feature_extraction.text import TfidfVectorizer# jieba分词 中文文本特征提取'''中文文本tf-idf提取演示'''def cut_word(data): text = ' '.join(list(jieba.cut(data))) return texttext_list = []data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。", "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。", "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]for s in data: text_list.append(cut_word(s))print(text_list)# 实例化一个转换器transfer = TfidfVectorizer()data = transfer.fit_transform(text_list)print('特征值名称\n',transfer.get_feature_names())print(data.toarray())x = data.toarray()# 打印得分最高的特征索引print('max_index',np.where( x== np.max(x))) 输出：12345678910111213141516171819202122232425['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']特征值名称 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样'][[0.30847454 0. 0.20280347 0. 0. 0. 0.40560694 0. 0. 0. 0. 0. 0.20280347 0. 0.20280347 0. 0. 0. 0. 0.20280347 0.20280347 0. 0.40560694 0. 0.20280347 0. 0.40560694 0.20280347 0. 0. 0. 0.20280347 0.20280347 0. 0. 0.20280347 0. ] [0. 0. 0. 0.2410822 0. 0. 0. 0.2410822 0.2410822 0.2410822 0. 0. 0. 0. 0. 0. 0. 0.2410822 0.55004769 0. 0. 0. 0. 0.2410822 0. 0. 0. 0. 0.48216441 0. 0. 0. 0. 0. 0.2410822 0. 0.2410822 ] [0.12001469 0.15780489 0. 0. 0.63121956 0.47341467 0. 0. 0. 0. 0.15780489 0.15780489 0. 0.15780489 0. 0.15780489 0.15780489 0. 0.12001469 0. 0. 0.15780489 0. 0. 0. 0.15780489 0. 0. 0. 0.31560978 0.15780489 0. 0. 0.15780489 0. 0. 0. ]]max_index (array([2]), array([4]))]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征工程]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征工程特征工程的定义(Feature Engineering)特征工程是使用专业背景只是和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。即直接影响机器学习的效果。 特征工程的作用 Andrew Ng said；”Coming up with features is difficult, time-consuming, requires expert knowledge.’Applied machine learning’ is basically feature engineering.”注：业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 特征工程的位置与数据处理的比较 pandas：一个数据读取非常方便以及基本的处理格式的工具 sklearn：对于特征的处理提供了强大的接口 特征工程包含内容 特征抽取 特征预处理 特征降维]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——scikit-learn介绍]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94scikit-learn%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——scikit-learn介绍可用数据集Kaggle网址：https://www.kaggle.com/datasetsUCI数据集网址： http://archive.ics.uci.edu/ml/scikit-learn网址：http://scikit-learn.org/stable/datasets/index.html#datasets scikit-learn介绍及其特点 Python语言的机器学习工具 Scikit-learn包括许多知名的机器学习算法的实现 Scikit-learn文档完善，容易上手，丰富的API 目前稳定版本0.19.1scikit-learn安装1pip3 install Scikit-learn==0.19.1 scikit-learn数据集 sklearn.datasets 加载获取流行数据集 datasets.load_*() 获取小规模数据集，数据包含在datasets里 datasets.fetch_*(data_home=None) 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集瞎子啊的目录，默认是~/scikit_learn_data/ sklearn小数据集加载并返回鸢尾花数据集 sklearn.datasets.load_iris()加载并返回波士顿房价数据集 sklearn.datasets.load_boston() sklearn大数据集 sklearn.datasets.fetch_20newsgroups(data_home=None,subset=’train’) subset：’train’ or ‘test’，’all’，可选，选择要加载的数据集。 训练集的”训练”，测试集的”测试”，两者的”全部” sklearn数据集返回值 load和fetch返回的数据类型datasets.base.Bunch(字典格式) data：特征数据数组，是[n_samples * n_features]的二维numpy.ndarray数组 target：标签数组，是n_samples的一维numpy.ndarray数组 DESCR：数据描述 feature_names：特征名，新闻数据，手写数字、回归数据集没有 target_names：标签名 数据集的划分机器学习一般数据集会划分为两个部分 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 划分比例 训练集：70% 80% 75% 测试集：30% 20% 30% 数据划分 sklearn.model_selection.train_test_split(arrays,*options) x数据集的特征值 y数据集的标签值 test_size测试集的大小，一般为float，默认0.25 random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。 returnx_train, x_test, y_train, y_test (默认随机取) 1x_train,x_test,y_train,y_test = train_test_split(data,target,test_size=0.2,random_state=8)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[精通正则表达式]]></title>
    <url>%2F2018%2F01%2F29%2FBooks%2F%E7%B2%BE%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[精通正则表达式 精通正则表达式]]></content>
      <categories>
        <category>Books</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闭包]]></title>
    <url>%2F2017%2F11%2F03%2FPython%2F%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[闭包实现闭包的基石闭包的创建通常是利用嵌套函数来完成的。在PyCodeObject中，与嵌套函数相关的属性是co_cellvars和co_freevars。具体两者的的含义如下：co_cellvars：tuple，保存嵌套的作用域中使用的变量名集合co_freevars：tuple，保存使用了的外层作用域中的变量名集合closure.py会编译出3个PyCodeObject，其中有两个，一个与函数get_func对应，一个与函数inner_func对应，那么，与get_func对应的PyCodeObject对象中的co_cellvars就应该包含外层函数内的变量，即字符串”value”，因为其嵌套作用域(inner_func的作用域)中可以使用这个变量；同理，与函数inner_func对应的PyCodeObject对象中的co_freevars中应该也有该变量。在PyFrameObject对象中，也有一个属性与闭包的实现相关，这个属性就是f_localsplus，在PyFrame_New中的extras正是f_localsplus指向的那边内存的大小。1extras = code-&gt;co_stacksize + code-&gt;co_nlocals + ncells + nfrees; f_localsplus的完整内存布局：运行时栈(co_stacksize)、局部变量(co_nlocals)、cell对象(对应co_cellvars)、free对象(对应co_freevars) 闭包的实现创建 closure在python虚拟机执行CALL_FUNCTION指令时，会进入fast_function函数。而在fast_function函数中，由于当前的PyCodeObject为get_func对应之PyCodeObject，其中的co_flags为3(CO_OPTIMIZED|CO_NEWLOCALS)，所以最终不符合进入快速通道的条件，而会进入PyEval_EvalCodeEx。在PyEval_EvalCodeEx中，Python虚拟机会如同处理默认参数一样，将co_cellvars中的东西拷贝到新创建的PyFrameObject的f_localsplus中。嵌套函数有时候会很复杂，比如内层嵌套函数引用的不是外层嵌套函数的局部变量，而是外层嵌套函数的一个拥有默认值的参数。Python虚拟机会获得被内层嵌套函数引用的符号名，即字符串”value”，即获得被嵌套函数共享的符号名cellname，通过判断标识位found来处理被嵌套函数共享外层函数的默认参数，若found标识位为0时，Python虚拟机会创建一个cell对象——PyCellObject。cell对象仅维护一个ob_ref，指向一个Python中的对象。一开始cell对象维护的ob_ref指向了NULL，但当外层局部变量被创建时，即value=”inner”这个赋值语句执行的时候，这个cell对象会被拷贝到新创建的PyFrameObject对象的f_localsplus中，且这个对象呗拷贝到的位置是co-&gt;co_nlocals + i，说明在f_localsplus中，cell对象的位置是在局部变量之后的。 PyEval_CodeEx中的found标志位，指的是被内层嵌套函数引用的符号是否已经与某个值绑定的标识，或者说与某个对象建立了约束关系。只有在内层嵌套函数引用的是外层函数的一个有默认值的参数值时，这个标识才可能为1。 在处理co\cellvars即cell对象时，之前获得的cellname会被忽略，因为在get_func函数执行的过程中，对value这个cell变量的访问将通过基于索引访问f_localsplus完成，因而完全不需要再知道cellname了。这个cellname实际上是在处理内层嵌套函数引用外层函数的默认参数时产生的。在处理了cell对象之后，Python虚拟机将进入PyEval_EvalFrameEx，从而正是开始对函数get_func的调用过程。首先将PyStringObject对象(即外层函数的)压入到运行时栈，然后Python虚拟机开始执行STORE_DEREF。从运行时栈弹出的是PyStringObject对象”inner”，而从f_localsplus中取得的是PyCellObject对象，通过PyCell_Set来设置PyCellObject对象中的ob_ref。从而，f_localsplus就发生了变化。设置cell对象之后的get_func函数的PyFrameObject对象，如图： 在get_func的环境中，value符号对应着一个PyStringObject对象，但是closure的作用是将这个约束进行冻结，使得在嵌套函数inner_func被调用时还能使用这个约束。在执行”def inner_func()”表达式时，Python虚拟机就会将(value,”inner”)这个约束塞到PyFunctionObject中。首先将刚刚放置好的PyCellObject对象取出，并压入运行时栈，接着将PyCellObject对象打包进一个tuple中，tuple中可以放置多个PyCellObject。随后将inner_func对应的PyCodeObject对象也压入到运行时栈中，接着完成约束与PyCodeObject的绑定。表达式”def inner_func()”对应的将新创建的PyFunctionObject对象放置到了f_localsplus中，从而使f_localsplus发生了变化。设置function对象之后的get_func函数的PyFrameObject对象，如图：在get_func的最后，新建的PyFunctionObject对象作为返回值给了上一个栈帧，并被压入到该栈帧的运行时栈中。 使用 closureclosure是在get_func中被创建的，而对closure的使用，则是在inner_inner中。在执行”show_value()”对应的CALL_FUNCTION指令时，和inner_func对应的PyCodeObject中co_flags里包含了CO_NESTED，所以在fast_function中不能通过快速通道的验证，从而智能进入PyEval_EvalCodeEx。inner_func对应的PyCodeObject中的co_freevars里有引用的外层作用域中的富豪命，在PyEval_EvalCodeEx中，就会对这个co_freevars进行处理。其中的closure变量是作为最后一个函数参数传递进来的，即在PyFunctionObject对象中与PycodeObject对象绑定的装满PyCellObject对象的tuple。因此在PyEval_EvalCodeEx中，进行的动作就是讲这个PyCellObject对象一个一个放入到f_loaclsplus中相应的位置。在处理完closure之后，inner_func对应的PyFrameObject中的f_loaclsplus再次发生变化。设置cell对象之后的inner_func函数的PyFrameObject对象，如图：这里的动作与调用get_func是一致的，在inner_func调用的过程中，当引用外层作用域的符号时，其实是到f_localsplus中的free变量区域中获得符号对应的值。其实这就是inner_func函数中”print(value)”表达式对应的第一条字节码的意义。 闭包代码1234567def get_func():value = "inner"def inner_func():print(value) return inner_funcshow_value = get_func()show_value()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python源码剖析]]></title>
    <url>%2F2017%2F11%2F03%2FBooks%2FPython%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Python源码剖析 Python源码剖析]]></content>
      <categories>
        <category>Books</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV官方教程中文版]]></title>
    <url>%2F2017%2F03%2F20%2FBooks%2FOpenCV%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B%E4%B8%AD%E6%96%87%E7%89%88%2F</url>
    <content type="text"><![CDATA[OpenCV官方教程中文版 OpenCV官方教程中文版For Python]]></content>
      <categories>
        <category>Books</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习笔记——基本使用]]></title>
    <url>%2F2017%2F03%2F18%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[OpenCV学习笔记——基本使用pip安装1pip install opencv-python 导入OpenCV1import cv2 # 即OpenCV 原图展示 文件读取与展示OpenCV是以BGR的顺序读取图像，而matplotlib则是以RGB的顺序读取图像，于是蓝色和红色的通道颠倒了。1234import cv2import matplotlib.pylab as pltinput_image = cv2.imread('test.jpeg')plt.imshow(input_image) 输出图像： 图像的大小、尺寸和类型参数123print('size',input_image.size)print('shape',input_image.shape)print('dtype',input_image.dtype) 输出：123size 1195350shape (613, 650, 3)dtype uint8 注意 最后一个（数据类型）是用Python工作的棘手问题之一。由于它不是强类型化的，Python将允许您具有不同类型的数组，但大小相同，并且一些函数将返回可能不需要的类型数组。能够像这样检查和检查数据类型是非常有用的，也是我经常在调试中发现的事情之一。 颜色之间的转换、分离与合并为了调整回正确的颜色，首先要分离图像三个颜色通道。 注意：一般计算机中显示图像是由蓝色、绿色和红色三个色层叠加而成的，这也使得我们可以分离三个颜色通道，即在分离时，使用b,g,r的顺序获取。 在OpenCV中，可以使用 split 函数分离，使用 merge 合并。 分离1234# 分离b,g,r=cv2.split(input_image)# 展示其中一种颜色 (此处展示了红色，你可以用b来展示蓝色)plt.imshow(r, cmap='gray') 输出图像： 合并1234# 合并merged=cv2.merge([r,g,b])# 合并采用单通道矩阵数组plt.imshow(merged) 输出图像：除了以上合并方法，OpenCV还有专门解决这一问题的功能，即 CORLYBBGR2RGB 。于是我们不用通过手动拆分合并就能显示正确的颜色，它的使用方式如下：12opencv_merged=cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)plt.imshow(opencv_merged) 输出图像： 获取图像数据和设置图像数据Python OpenCV中的图像是以Numpy数组的形式保存。类似于Numpy中的数组索引，我们也可以提取和更改每个像素点的数值。12pixel = input_image[100,100]print(pixel) 输出：1[ 26 84 243] 123input_image[100,100] = [0,0,0]pixelnew = input_image[100,100]print(pixelnew) 输出：1[0 0 0] 获取和设置图像区域我们可以得到或设置单个像素，同样地，我们可以获取或设置图像的区域。这类似于Numpy中的数组切片：12center = opencv_merged[201:501, 202:502]plt.imshow(center) 输出图像： 注意：图像切片选取的方向与Numpy数组索引的方向也是一致的，在上述代码中，“201:501”表示竖直向下201至501的像素行，“202:502”表示水平向右202至502列的像素。 同样，我们也可以对选中的区域进行修改：1234fresh_image=opencv_merged.copy() # 复制一张图像fresh_image[20:20+center.shape[0], 30:30+center.shape[1]]=center # 修改选中区域print(center.shape)plt.imshow(fresh_image) 输出图像： 注意：在对选中区域进行修改时，选中的区域必须与传入的图像大小一致，故可以在切片时使用[a:a+test.shape[0],b:b+test.shape[1]]来确保选中区域的大小与传入图像大小一致。 矩阵切片在OpenCV Python风格中，正如我所提到的，图像是Numpy数组。在Numpy教程中有一些很棒的数组操作：如果你以前没有做过，那么这是一个很好的介绍。上面的区域的获取和设置使用切片，尽管如此，我还是想把这本书的内容更详细地讲完。123freshim2 = opencv_merged.copy()crop = freshim2[100:200, 130:400] plt.imshow(crop) 输出图像： 切片可以由以下几种方式：[top_y:bottom_y, left_x:right_x]或者[y:y+height, x:x+width]同时，您也可以在切片时，直接提取颜色[y:y+height, x:x+width, channel]其中channel你感兴趣的颜色。这里定义 0=蓝色，1=绿色，2=红色。下面我们采用第三种方式获取切片，并转换到HSV的例子。 HSV（色相，饱和度，明度）为另一种色彩空间，有兴趣可以了解色彩空间理论！123hsvim=cv2.cvtColor(freshim2,cv2.COLOR_BGR2HSV)bcrop =hsvim[100:200, 130:400, 1]plt.imshow(bcrop, cmap="gray") 输出图像：]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——高级处理]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%AB%98%E7%BA%A7%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——高级处理1.缺失值处理处理nan或者默认标记在pandas中我们处理起来非常容易 判断数据是否为NaN pd.isnull(df), pd.notnull(df)处理方式： 存在缺失值nan, 并且是np.nan: 1 删除存在缺失值的：dropna(axis=’rows’) 注：不会修改原数据，需要接受返回值 2 替换缺失值：fillna(value, inplace=True) value：替换成的值 inplace： True：会修改原数据 False：不替换修改原数据，生成新的对象 不是缺失值nan，有默认标记的，如问号 先替换’?’为np.nan，再进行缺失值的处理 df.replace(to_replace=, value=) to_replace:替换前的值 value:替换后的值 2.数据离散化连续属性的离散化就是将连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数 值代表落在每个子区间中的属性值。 读取数据 数据分组 使用的工具： pd.qcut(data, bins)： 对数据进行分组将数据分组 一般会与value_counts搭配使用，统计每组的个数 series.value_counts()：统计分组次数 pd.cut(data, bins)：自定义区间分组 pandas.get_dummies(data, prefix=None):获取one-hot编码矩阵 data:array-like, Series, or DataFrame prefix:分组名字 3.合并3.1.pd.concat实现合并 pd.concat([data1, data2], axis=1) 按照行或列进行合并,axis=0为列索引，axis=1为行索引3.2.pd.merge实现合并 pd.merge(left, right, how=’inner’, on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=(‘_x’, ‘_y’), copy=True, indicator=False,validate=None) 可以指定按照两组数据的共同键值对合并或者左右各自 left: A DataFrame object right: Another DataFrame object on: Columns (names) to join on. Must be found in both the left and right DataFrame objects. left_on=None, right_on=None：指定左右键 Merge method SQL Join Name Description left LEFT OUTER JOIN Use keys from left frame only right RIGHT OUTER JOIN Use keys from right frame only outer FULL OUTER JOIN Use union of keys from both frames inner INNER JOIN Use intersection of keys from both frames 4.交叉表与透视表4.1.crosstab(交叉表)实现交叉表：交叉表用于计算一列数据对于另外一列数据的分组个数(寻找两个列之间的关系) pd.crosstab(value1, value2)4.2.pivot_table(透视表)实现 pd.pivot_table([], index=[]) 5.分组与聚合 DataFrame.groupby(key, as_index=False) key:分组的列数据，可以多个]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——文件读取与存储]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——文件读取与存储文件读取与存储1.csv1.1.读取csv文件-read_csv() pandas.read_csv(filepath_or_buffer, sep =’,’ , delimiter = None) filepath_or_buffer:文件路径 usecols:指定读取的列名，列表形式1.2.写入csv文件-to_csv() DataFrame.to_csv(path_or_buf=None, sep=’, ’, columns=None, header=True, index=True, index_label=None, mode=’w’, encoding=None) path_or_buf :string or file handle, default None sep :character, default ‘,’ columns :sequence, optional mode:’w’：重写； ‘a’ 追加，若使用mode=’a’，需要同时指定header=False，否侧会吧column名一起追加入数据中 index:是否写进行索引 header :boolean or list of string, default True,是否写进列索引值 Series.to_csv(path=None, index=True, sep=’, ‘, na_rep=’’, float_format=None, header=False, index_label=None, mode=’w’, encoding=None, compression=None, date_format=None, decimal=’.’)Write Series to a comma-separated values (csv) file2.hdf5优先选择使用HDF5文件存储 HDF5在存储的是支持压缩，使用的方式是blosc，这个是速度最快的也是pandas默认支持的 使用压缩可以提磁盘利用率，节省空间 HDF5还是跨平台的，可以轻松迁移到hadoop 上面2.1.read_hdf()与to_hdf()HDF5文件的读取和存储需要指定一个键，值为要存储的DataFrame pandas.read_hdf(path_or_buf，key =None，** kwargs)从h5文件当中读取数据 path_or_buffer:文件路径 key:读取的键 mode:打开文件的模式 return:Theselected object DataFrame.to_hdf(path_or_buf, key, \kwargs) 需要安装安装tables模块避免不能读取HDF5文件1pip install tables ps:可以直接使用pandas.read_table()读取table 3.json3.1.read_json() pandas.read_json(path_or_buf=None, orient=None, typ=’frame’, lines=False) 将JSON格式准换成默认的Pandas DataFrame格式 orient : string,Indication of expected JSON string format. ‘split’ : dict like {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]} ‘records’ : list like [{column -&gt; value}, … , {column -&gt; value}] ‘index’ : dict like {index -&gt; {column -&gt; value}} ‘columns’ : dict like {column -&gt; {index -&gt; value}},默认该格式 ‘values’ : just the values array lines : boolean, default False 按照每行读取json对象 typ : default ‘frame’， 指定转换成的对象类型series或者dataframe 3.2.to_json() DataFrame.to_json(path_or_buf=None, orient=None, lines=False) 将Pandas 对象存储为json格式 path_or_buf=None：文件地址 orient:存储的json形式，{‘split’,’records’,’index’,’columns’,’values’} lines:一个对象存储为一行]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——Pandas画图]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Pandas%E7%94%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——Pandas画图Pandas画图1.pandas.DataFrame.plot DataFrame.plot(x=None, y=None, kind=’line’) x : label or position, default None y : label, position or list of label, positions, default None Allows plotting of one column versus another kind : str ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘pie’ : pie plot ‘scatter’ : scatter plot2.pandas.Series.plot]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——DataFrame运算]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94DataFrame%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——DataFrame运算DataFrame运算1.算术运算 add(other) 比如进行数学运算加上具体一个数字 也可以加上某一列 sub(other) 比如进行数学运算加上具体一个数字 也可以减去某一列2.逻辑运算2.1.逻辑运算符号&lt;、&gt;、|、&amp;、== ==等于 大于 &lt;小于 |或 &amp;且 2.2.逻辑运算函数 query(expr) expr:查询字符串(查询条件，如’column01 &gt; 1 &amp; column02 &lt; 2’) isin(values) values:tuple,list,ndarray,series 3.统计运算3.1.describe()综合分析: 能够直接得出很多统计结果,count, mean, std, min, max 等12# 计算平均值、标准差、最大值、最小值df.describe 3.2.统计函数 count Number of non-NA observations sum Sum of values mean Mean of values median Arithmetic median of values min Minimum max Maximum mode Mode abs Absolute Value prod Product of values std Bessel-corrected sample standard deviation var Unbiased variance idxmax compute the index labels with the maximum idxmin compute the index labels with the minimum nunique repeat for all columns 对于单个函数去进行统计的时候，坐标轴还是按照这些默认为“columns” (axis=0, default)，如果要对行“index” 需要指定(axis=1)。例如1234567# 对列求结果df.min(0)# 对行求结果df.max(0)# 对于一位数组或者列表，unique函数去重，并按元素由大到小返回一个新的无重复的元组或者列表pd.unique(df[column])df.nunique()[column] 4.累计统计函数 函数 作用 cumsum 计算前1/2/3/…/n个数的和 cummax 计算前1/2/3/…/n个数的最大值 cummin 计算前1/2/3/…/n个数的最小值 cumprod 计算前1/2/3/…/n个数的积 12345678910111213&gt;&gt;&gt; df.cumsum() month year sale0 1 2012 551 2 4026 952 3 6039 1793 4 8053 210&gt;&gt;&gt; df['year'].cumsum()0 20121 40262 60393 8053Name: year, dtype: int64 5.自定义运算 apply(func,axis=0) func:自定义函数 axis=0:默认是列，axis=1为行进行运算 123456789101112&gt;&gt;&gt; df month year sale test0 1 2012 55 20141 1 2014 40 20142 1 2013 84 20143 1 2014 31 2014# 指定列 最大值-最小值&gt;&gt;&gt; df[['year','test']].apply(lambda x:x.max()-x.min(), axis=0)year 2test 0dtype: int64]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——基本数据操作]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——基本数据操作基本数据操作1.索引操作Numpy当中我们已经讲过使用索引选取序列和切片选择，pandas也支持类似的操作，也可以直接使用列名、行名称，甚至组合使用。 1.1.直接使用行列索引(先列后行)1234567891011121314151617&gt;&gt;&gt; df = pd.DataFrame(&#123;'month': [1, 4, 7, 10],'year': [2012, 2014, 2013, 2014],'sale':[55, 40, 84, 31]&#125;)&gt;&gt;&gt; df month year sale0 1 2012 551 4 2014 402 7 2013 843 10 2014 31&gt;&gt;&gt; df['month']0 11 42 73 10Name: month, dtype: int64&gt;&gt;&gt; df['month'][1]4 1.2.结合loc或者iloc使用索引 loc 12345# 使用loc:只能指定行列索引的名字&gt;&gt;&gt; df.loc['month':'sales']Empty DataFrameColumns: [month, year, sale]Index: [] iloc 123456# 使用iloc可以通过索引的下标去获取&gt;&gt;&gt; df.iloc[0:3, 0:2] month year0 1 20121 4 20142 7 2013 1.3.使用ix组合索引 ix123456789101112131415161718192021222324252627282930&gt;&gt;&gt; df.ix[0:4,['month']] month0 11 42 73 10&gt;&gt;&gt; df.ix[0:4,['month','year']] month year0 1 20121 4 20142 7 20133 10 2014# 使用loc与iloc也可以完成组合索引# loc&gt;&gt;&gt; df.loc[df.index[0:4], ['month','year']] month year0 1 20121 4 20142 7 20133 10 2014# iloc&gt;&gt;&gt; df.iloc[0:4, df.columns.get_indexer(['month','year'])] month year0 1 20121 4 20142 7 20133 10 2014 2.赋值操作 与字典操作类似，可以新增也可以修改1234567891011121314151617181920212223242526272829303132333435# 新增&gt;&gt;&gt; df['test'] = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 4 2014 40 12 7 2013 84 13 10 2014 31 1&gt;&gt;&gt; df.test = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 4 2014 40 12 7 2013 84 13 10 2014 31 1df.test = 1# 修改&gt;&gt;&gt; df['month'] = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 1 2014 40 12 1 2013 84 13 1 2014 31 1&gt;&gt;&gt; df.month = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 1 2014 40 12 1 2013 84 13 1 2014 31 1 3.排序排序有两种形式，一种对内容进行排序，一种对索引进行排序DataFrame 使用df.sort_values(by=column01, ascending=)对内容进行排序 单个键或者多个键进行排序,默认升序 ascending=False:降序 ascending=True:升序 使用df.sort_index对索引进行排序Series 使用series.sort_values(ascending=True)对内容进行排序 series排序时，只有一列，不需要参数 使用series.sort_index()对索引进行排序]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——MultiIndex、Panel与Series]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94MultiIndex%E3%80%81Panel%E4%B8%8ESeries%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——MultiIndex、Panel与Series1234567891011121314151617181920&gt;&gt;&gt; df = pd.DataFrame(&#123;'month': [1, 4, 7, 10],'year': [2012, 2014, 2013, 2014],'sale':[55, 40, 84, 31]&#125;)&gt;&gt;&gt; df month year sale0 1 2012 551 4 2014 402 7 2013 843 10 2014 31&gt;&gt;&gt; df = df.set_index(['year', 'month'])&gt;&gt;&gt; df saleyear month 2012 1 552014 4 402013 7 842014 10 31&gt;&gt;&gt; df.indexMultiIndex(levels=[[2012, 2013, 2014], [1, 4, 7, 10]], labels=[[0, 2, 1, 2], [0, 1, 2, 3]], names=['year', 'month']) 1.MultiIndex多级或分层索引对象。 index属性 names：levels的名称 levels：每个level的元组值12345&gt;&gt;&gt; df.index.namesFrozenList(['year', 'month'])&gt;&gt;&gt; df.index.levelsFrozenList([[2012, 2013, 2014], [1, 4, 7, 10]]) 2.Pannel class pandas.Panel(data=None, items=None, major_axis=None, minor_axis=None, copy=False, dtype=None) 存储3维数组的Panel结构1234567&gt;&gt;&gt; p = pd.Panel(np.arange(24).reshape(4,3,2),items=list('ABCD'),major_axis=pd.date_range('20130101', periods=3),minor_axis=['first', 'second'])&gt;&gt;&gt; p&lt;class 'pandas.core.panel.Panel'&gt;Dimensions: 4 (items) x 3 (major_axis) x 2 (minor_axis)Items axis: A to DMajor_axis axis: 2013-01-01 00:00:00 to 2013-01-03 00:00:00Minor_axis axis: first to second items - axis 0，每个项目对应于内部包含的数据帧(DataFrame)。 major_axis - axis 1，它是每个数据帧(DataFrame)的索引(行)。 minor_axis - axis 2，它是每个数据帧(DataFrame)的列。 3.Series series结构只有行索引3.1.创建Series通过已有数据创建 指定内容，默认索引 123456789101112&gt;&gt;&gt; pd.Series(np.arange(10))0 01 12 23 34 45 56 67 78 89 9dtype: int64 指定索引 1234567&gt;&gt;&gt; pd.Series(['a', 'b', 'c', 'd', 'e'], index=[1, 2, 3, 4, 5])1 a2 b3 c4 d5 edtype: object 通过字典数据创建123456&gt;&gt;&gt; pd.Series(&#123;'a':'1', 'b':2, 'c': 3, 'd':4&#125;)a 1b 2c 3d 4dtype: object 3.2.Series获取索引和值 index values]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——DataFrame]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94DataFrame%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——DataFrame1.DataFrame123456789101112131415&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; a = np.random.normal(0,1,10).reshape(2,5)&gt;&gt;&gt; aarray([[ 0.27370548, -0.33756615, 1.56610377, 1.16995613, -0.13634255], [-0.15440603, -0.87418791, -0.93990267, 0.81535022, 0.25877249]]) &gt;&gt;&gt; b = pd.DataFrame(a)&gt;&gt;&gt; type(b)&lt;class 'pandas.core.frame.DataFrame'&gt;&gt;&gt;&gt; b 0 1 2 3 40 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772 1.1.DataFrame结构DataFrame对象既有行索引，又有列索引 行索引，表明不同行，横向索引，叫index 列索引，表名不同列，纵向索引，叫columns1.2.DataFrame的属性与方法常用属性 shape 12&gt;&gt;&gt; b.shape(2, 5) index 12&gt;&gt;&gt; b.indexRangeIndex(start=0, stop=2, step=1) columns 12&gt;&gt;&gt; b.columnsRangeIndex(start=0, stop=5, step=1) values 123&gt;&gt;&gt; b.valuesarray([[ 0.27370548, -0.33756615, 1.56610377, 1.16995613, -0.13634255], [-0.15440603, -0.87418791, -0.93990267, 0.81535022, 0.25877249]]) T 1234567&gt;&gt;&gt; b.T 0 10 0.273705 -0.1544061 -0.337566 -0.8741882 1.566104 -0.9399033 1.169956 0.8153504 -0.136343 0.258772 常用方法 head(5):显示前5行内容 如果不补充参数，默认5行。填入参数N则显示前N行123&gt;&gt;&gt; b.head(1) 0 1 2 3 40 0.273705 -0.337566 1.566104 1.169956 -0.136343 tail(5):显示后5行内容 如果不补充参数，默认5行。填入参数N则显示后N行123&gt;&gt;&gt; b.tail(1) 0 1 2 3 41 -0.154406 -0.874188 -0.939903 0.81535 0.258772 DataFrame索引的设置 设置或修改行索引值 必须整体设置或修改(使用列表或元组)1234567891011121314# 修改index&gt;&gt;&gt; i = ['test1','test2']&gt;&gt;&gt; b.index = i&gt;&gt;&gt; b 0 1 2 3 4test1 0.273705 -0.337566 1.566104 1.169956 -0.136343test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772# 直接设置index索引&gt;&gt;&gt; b = pd.DataFrame(a,index=i)&gt;&gt;&gt; b 0 1 2 3 4test1 0.273705 -0.337566 1.566104 1.169956 -0.136343test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772 设置或修改列索引值 必须整体设置或修改(使用列表或元组)12345678910111213&gt;&gt;&gt; c = ['test1','test2','test3','test4','test5']&gt;&gt;&gt; b.columns = c&gt;&gt;&gt; b test1 test2 test3 test4 test50 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772# 直接设置columns索引&gt;&gt;&gt; b = pd.DataFrame(a,columns=c)&gt;&gt;&gt; b test1 test2 test3 test4 test50 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772 重设索引 reset_index(drop=False) 设置新的下标索引 drop:默认为False，不删除原来索引，如果为True,删除原来的索引值1234567891011121314&gt;&gt;&gt; b test1 test2 test3 test4 test5test1 0.273705 -0.337566 1.566104 1.169956 -0.136343test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772&gt;&gt;&gt; b.reset_index() index test1 test2 test3 test4 test50 test1 0.273705 -0.337566 1.566104 1.169956 -0.1363431 test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772&gt;&gt;&gt; b.reset_index(drop=True) test1 test2 test3 test4 test50 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772 以某列值设置为新的索引 set_index(keys, drop=True) keys : 列索引名成或者列索引名称的列表 drop : boolean, default True.当做新的索引，删除原来的列1234567891011121314151617181920212223&gt;&gt;&gt; df = pd.DataFrame(&#123;'month': [1, 4, 7, 10],'year': [2012, 2014, 2013, 2014],'sale':[55, 40, 84, 31]&#125;)&gt;&gt;&gt; df month year sale0 1 2012 551 4 2014 402 7 2013 843 10 2014 31&gt;&gt;&gt; df.set_index('month') year salemonth 1 2012 554 2014 407 2013 8410 2014 31&gt;&gt;&gt; df.set_index(['year', 'month']) saleyear month 2012 1 552014 4 402013 7 842014 10 31]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——数组间运算]]></title>
    <url>%2F2017%2F03%2F12%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84%E9%97%B4%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——数组间运算1.数组与数的运算123456789101112&gt;&gt;&gt; A = np.array([[1,2,3,4,5],[6,7,8,9,10]])&gt;&gt;&gt; Aarray([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10]]) &gt;&gt;&gt; A + 1array([[ 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11]]) &gt;&gt;&gt; A / 2array([[0.5, 1. , 1.5, 2. , 2.5], [3. , 3.5, 4. , 4.5, 5. ]]) 2.数组与数组的运算2.1.广播机制数组与数组的运算涉及到numpy的广播机制，执行 broadcast 的前提在于，两个 ndarray 执行的是 element-wise的运算，Broadcast机制的功能是为了方便不同形状的ndarray（numpy库的核心数据结构）进行数学运算。当操作两个数组时，numpy会逐个比较它们的shape（构成的元组tuple），只有在下述情况下，两个数组才能够进行数组与数组的运算。 维度相等 shape（其中相对应的一个地方为1）1234567891011121314&gt;&gt;&gt; a = np.array([1,2,3])# 查看a的维度&gt;&gt;&gt; a.shape(3,)&gt;&gt;&gt; b = np.array([[1,],[2,],[3]])# 查看b的维度&gt;&gt;&gt; b.shape(3, 1)&gt;&gt;&gt; b - aarray([[ 0, -1, -2], [ 1, 0, -1], [ 2, 1, 0]]) 根据广播原则，b满足其中一方轴长度为1，那么广播会沿着长度为1的轴，及axis=1进行，对数组b沿着axis=1即水平方向进行复制，相当于b变成一个shape为(3,3)且各列均为[1,2,3]的数组，一个维度为(3,3)的数组减去一个维度为(3,)的数组，满足后缘维度轴长度相等,数组a沿着axis=0即竖直方向进行广播，相当远a变成一个shape为(3,3)且个行均为[1,2,3]的数组。相减的时候，b被广播成为$$\begin{bmatrix}1&amp;1&amp;1\\2&amp;2&amp;2\\3&amp;3&amp;3\end{bmatrix}$$a被广播为$$\begin{bmatrix}1&amp;2&amp;3\\1&amp;2&amp;3\\1&amp;2&amp;3\end{bmatrix}$$结果为$$\begin{bmatrix}0&amp;-1&amp;-2\\1&amp;0&amp;-1\\2&amp;1&amp;0\end{bmatrix}$$ 2.2.乘法运算 a * b 对ndarray对象进行对应位置相乘 对matrix对象进行矩阵乘法 np.multiply(a ,b) 对ndarray对象和matrix对象都进行对应位置相乘 np.matmul 对ndarray对象和matrix对象 np.dot 对ndarray对象和matrix对象 a.dot(b) 对ndarray对象和matrix对象 a @ b 对ndarray对象和matrix对象 ndarray对象123456789101112131415161718192021222324&gt;&gt;&gt; a = np.array([1,2,3])&gt;&gt;&gt; b = np.array([[1,],[2,],[3]])# a,b矩阵由于广播机制，已被广播为3x3的数组&gt;&gt;&gt; a * barray([[1, 2, 3], [2, 4, 6], [3, 6, 9]]) &gt;&gt;&gt; np.multiply(a, b)array([[1, 2, 3], [2, 4, 6], [3, 6, 9]]) &gt;&gt;&gt; np.matmul(a, b)array([14])&gt;&gt;&gt; np.dot(a, b)array([14])&gt;&gt;&gt; a.dot(b)array([14])&gt;&gt;&gt; a @ barray([14]) matrix对象123456789101112131415161718192021&gt;&gt;&gt; A = np.matrix([1,2,3])&gt;&gt;&gt; B = np.matrix([[1,],[2,],[3]])&gt;&gt;&gt; A*Bmatrix([[14]])&gt;&gt;&gt; np.multiply(A, B)matrix([[1, 2, 3], [2, 4, 6], [3, 6, 9]]) &gt;&gt;&gt; np.matmul(A ,B)matrix([[14]])&gt;&gt;&gt; np.dot(A, B)matrix([[14]])&gt;&gt;&gt; A.dot(B)matrix([[14]])&gt;&gt;&gt; A @ Bmatrix([[14]]) 3.合并与分割3.1.合并 numpy.concatenate((a1, a2, …), axis=0) numpy.hstack(tup) Stack arrays in sequence horizontally (column wise). numpy.vstack(tup) Stack arrays in sequence vertically (row wise). np.concatenate() axis=0以列方式进行合并 axis=1以行方式进行合并12345678910&gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])&gt;&gt;&gt; b = np.array([[5, 6]])&gt;&gt;&gt; np.concatenate((a, b), axis=0)array([[1, 2], [3, 4], [5, 6]]) &gt;&gt;&gt; np.concatenate((a, b.T), axis=1)array([[1, 2, 5], [3, 4, 6]]) np.hstack() 以行方式进行合并1234567891011&gt;&gt;&gt; a = np.array((1,2,3))&gt;&gt;&gt; b = np.array((2,3,4))&gt;&gt;&gt; np.hstack((a, b))array([1, 2, 3, 2, 3, 4])&gt;&gt;&gt; a = np.array([[1],[2],[3]])&gt;&gt;&gt; b = np.array([[2],[3],[4]])&gt;&gt;&gt; np.hstack((a, b))array([[1, 2], [2, 3], [3, 4]]) np.vstack() 以列方式进行合并 123456789101112131415&gt;&gt;&gt; a = np.array([1, 2, 3])&gt;&gt;&gt; b = np.array([2, 3, 4])&gt;&gt;&gt; np.vstack((a, b))array([[1, 2, 3], [2, 3, 4]])&gt;&gt;&gt; a = np.array([[1], [2], [3]])&gt;&gt;&gt; b = np.array([[2], [3], [4]])&gt;&gt;&gt; np.vstack((a, b))array([[1], [2], [3], [2], [3], [4]]) 3.2.分割 numpy.split(ary, indices_or_sections, axis=0) Split an array into multiple sub-arrays. 1234567891011&gt;&gt;&gt; a = np.linspace(1,10,10)&gt;&gt;&gt; aarray([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])# np.split(a, int) int必须是len(a)的因数&gt;&gt;&gt; np.split(a, 2)[array([1., 2., 3., 4., 5.]), array([ 6., 7., 8., 9., 10.])]# np.split(a, [index_0,index_1,index_2...])&gt;&gt;&gt; np.split(a,[1,2,5])[array([[1, 2, 3, 4]]), array([[5, 6, 7, 8]]), array([], shape=(0, 4), dtype=int64), array([], shape=(0, 4), dtype=int64)]]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——ndarray运算]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94ndarray%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——ndarray运算1.逻辑运算1234567891011121314# 生成数据&gt;&gt;&gt; A = np.random.normal(0,1,(2,2))&gt;&gt;&gt; Aarray([[-0.7572373 , 2.13520462], [-0.72749164, -1.20845879]])# 逻辑判断，如果数据大于0.5就标记为True，否则为False&gt;&gt;&gt; A &gt; 0.5array([[False, True], [False, False]])# Bool赋值，将满足条件的设置为指定的值-布尔索引&gt;&gt;&gt; A[A &gt; 0.5] = 1&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]]) 2.通用判断函数 np.all() 123456&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]])# 判断A中所有元素是否全是大于0&gt;&gt;&gt; np.all(A &gt; 0)False np.any() 123456&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]])# 判断A中是否有元素大于0&gt;&gt;&gt; np.any(A &gt; 0)True 3.三元运算符 np.where1234567&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]])# 判断A中的元素 小于-1的置为1，否则置为0&gt;&gt;&gt; np.where(A &lt; -1, 1, 0 )array([[0, 0], [0, 1]]) 4.统计运算 np.min(a[, axis, out, keepdims]) Return the minimum of an array or minimum along an axis. np.max(a[, axis, out, keepdims]) Return the maximum of an array or maximum along an axis. np.median(a[, axis, out, overwrite_input, keepdims]) Compute the median along the specified axis. np.mean(a[, axis, dtype, out, keepdims]) Compute the arithmetic mean along the specified axis. np.std(a[, axis, dtype, out, ddof, keepdims]) Compute the standard deviation along the specified axis. np.var(a[, axis, dtype, out, ddof, keepdims]) Compute the variance along the specified axis.]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——基本操作]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——基本操作1.生成0和1的数组 empty(shape[, dtype, order]) empty_like(a[, dtype, order, subok])eye(N[, M, k, dtype, order]) identity(n[, dtype]) ones(shape[, dtype, order]) ones_like(a[, dtype, order, subok]) zeros(shape[, dtype, order]) zeros_like(a[, dtype, order, subok])full(shape, fill_value[, dtype, order]) full_like(a, fill_value[, dtype, order, subok])1234&gt;&gt;&gt; zero = np.zeros([3, 4])array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) 2.从现有数组生成 array(object[, dtype, copy, order, subok, ndmin]) asarray(a[, dtype, order]) asanyarray(a[, dtype, order]) ascontiguousarray(a[, dtype]) asmatrix(data[, dtype]) copy(a[, order])12345&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6]])# 从现有的数组当中创建&gt;&gt;&gt; a1 = np.array(a)# 相当于索引的形式，并没有真正的创建一个新的&gt;&gt;&gt; a2 = np.asarray(a) 关于array和asarray的不同12345678910111213141516171819202122&gt;&gt;&gt; data = np.ones([3,4])&gt;&gt;&gt; dataarray([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]])&gt;&gt;&gt; data1 = np.array(data)&gt;&gt;&gt; data2 = np.asarray(data)&gt;&gt;&gt; data1,data2(array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]), array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]))&gt;&gt;&gt; data[1] = 2&gt;&gt;&gt; data2array([[1., 1., 1., 1.], [2., 2., 2., 2.], [1., 1., 1., 1.]])&gt;&gt;&gt; data1array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) 3.生成固定范围的数组 np.linspace (start, stop, num, endpoint, retstep, dtype) 生成等间隔的序列 start 序列的起始值stop 序列的终止值，如果endpoint为true，该值包含于序列中num 要生成的等间隔样例数量，默认为50endpoint 序列中是否包含stop值，默认为tureretstep 如果为true，返回样例，以及连续数字之间的步长dtype 输出ndarray的数据类型 12生成等间隔的数组np.linspace(0, 100, 10) 返回结果：123array([ 0. , 11.11111111, 22.22222222, 33.33333333, 44.44444444, 55.55555556, 66.66666667, 77.77777778, 88.88888889, 100. ]) 其他的还有 numpy.arange(start,stop, step, dtype) numpy.logspace(start,stop, num, endpoint, base, dtype) 1np.arange(10, 50, 2) 返回结果：1array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42,44, 46, 48]) 4.生成随机数组 np.random模块 均匀分布 np.random.rand(d0, d1, …, dn)返回[0.0，1.0)内的一组均匀分布的数。 np.random.uniform(low=0.0, high=1.0, size=None)功能：从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high.参数介绍:low: 采样下界，float类型，默认值为0；high: 采样上界，float类型，默认值为1；size: 输出样本数目，为int或元组(tuple)类型，例如，size=(m,n,k), 则输出mnk个样本，缺省时输出1个值。返回值：ndarray类型，其形状和参数size中描述一致。 np.random.randint(low, high=None, size=None, dtype=’l’)从一个均匀分布中随机采样，生成一个整数或N维整数数组，取数范围：若high不为None时，取[low,high)之间随机整数，否则取值[0,low)之间随机整数。 正态分布 np.random.randn(d0, d1, …, dn)功能：从标准正态分布中返回一个或多个样本值 np.random.normal(loc=0.0, scale=1.0, size=None)loc：float此概率分布的均值（对应着整个分布的中心centre）scale：float​ 此概率分布的标准差（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高）size：int or tuple of ints输出的shape，默认为None，只输出一个值 np.random.standard_normal(size=None)返回指定形状的标准正态分布的数组。 均匀分布均匀分布（Uniform Distribution）是概率统计中的重要分布之一。顾名思义，均匀，表示可能性相等的含义。均匀分布在自然情况下极为罕见，而人工栽培的有一定株行距的植物群落即是均匀分布。12345# 生成均匀分布的随机数&gt;&gt;&gt; X = np.random.uniform(-1, 1, 100000000)&gt;&gt;&gt; Xarray([ 0.22411206, 0.31414671, 0.85655613, ..., -0.92972446,0.95985223, 0.23197723]) 正态分布 1.什么是正态分布正态分布是一种概率分布。正态分布是具有两个参数$\mu$和$\sigma$的连续型随机变量的分布，第一参数$\mu$是服从正态分布的随机变量的均值，第二个参数$\sigma$是此随机变量的方差，所以正态分布记作$N(\mu, \sigma )$。 2.正态分布的应用生活、生产与科学实验中很多随机变量的概率分布都可以近似地用正态分布来描述。 3.正态分布的特点$\mu$决定了其位置，其标准差$sigma$。决定了分布的幅度。当$\mu = 0$，$\sigma = 1$时的正态分布是标准正态分布。$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ 3.1.方差和标准差在概率论和统计学中衡量一组数据离散程度的度量方差：$$\sigma^2=\frac{1}{N}\sum\limits_{i=1}{N}(x_i-\mu)^2$$标准差：$$\sigma^2=\sqrt{\frac{1}{N}\sum\limits_{i=1}{N}(x_i-\mu)^2}$$ 3.2.方差和标准差的意义可以理解成数据的一个离散程度的衡量 5.数组的索引、切片1234567891011# 三维数组&gt;&gt;&gt; A = np.array([ [[1,2,3],[4,5,6]], [[12,3,34],[5,6,7]]])&gt;&gt;&gt; Aarray([[[ 1, 2, 3], [ 4, 5, 6]], [[12, 3, 34], [ 5, 6, 7]]])# 索引、切片&gt;&gt;&gt; A[0, 0, 1]2 6.形状修改 ndarray.reshape(shape[, order]) Returns an array containing the same data with a new shape. ndarray.T 数组的转置 ndarray.resize(new_shape[, refcheck]) Change shape and size of array in-place. 7.类型修改 ndarray.astype(type) ndarray.tostring([order])或者ndarray.tobytes([order]) Construct Python bytes containing the raw data bytes in the array. 如果遇到 123456&gt; IOPub data rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_data_rate_limit`.&gt; 这个问题是在jupyer当中对输出的字节数有限制，需要去修改配置文件创建配置文件 123&gt; jupyter notebook --generate-configvi ~/.jupyter/jupyter_notebook_config.py&gt; 取消注释,多增加123&gt; ## (bytes/sec) Maximum rate at which messages can be sent on iopub before they are limited.c.NotebookApp.iopub_data_rate_limit = 10000000&gt; 但是不建议这样去修改，jupyter输出太大会崩溃 8.数组的去重 ndarray.unique123temp = np.array([[1, 2, 3, 4],[3, 4, 5, 6]])&gt;&gt;&gt; np.unique(temp)array([1, 2, 3, 4, 5, 6])]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——ndarray属性和类型]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94ndarray%E5%B1%9E%E6%80%A7%E5%92%8C%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——ndarray属性和类型ndarray属性 属性名字 属性解释 返回类型 ndarray.shape 数组维度 tuple ndarray.ndim 数组维度(空间) int ndarray.size 数组中的元素数量 int ndarray.itemsize 一个数组元素的长度(字节) int ndarray.dtype 数组元素的类型 dtype object ndarray类型 名称 描述 简写 np.bool 用一个字节存储的布尔类型（True或False） ‘b’ np.int8 一个字节大小，-128 至 127 ‘i’ np.int16 整数，-32768 至 32767 ‘i2’ np.int32 整数，-2 31 至 2 32 -1 ‘i4’ np.int64 整数，-2 63 至 2 63 - 1 ‘i8’ np.uint8 无符号整数，0 至 255 ‘u’ np.uint16 无符号整数，0 至 65535 ‘u2’ np.uint32 无符号整数，0 至 2 ** 32 - 1 ‘u4’ np.uint64 无符号整数，0 至 2 ** 64 - 1 ‘u8’ np.float16 半精度浮点数：16位，正负号1位，指数5位，精度10位 ‘f2’ np.float32 单精度浮点数：32位，正负号1位，指数8位，精度23位 ‘f4’ np.float64 双精度浮点数：64位，正负号1位，指数11位，精度52位 ‘f8’ np.complex64 复数，分别用两个32位浮点数表示实部和虚部 ‘c8’ np.complex128 复数，分别用两个64位浮点数表示实部和虚部 ‘c16’ np.object_ python对象 ‘O’ np.string_ 字符串 ‘S’ np.unicode_ unicode类型 ‘U’]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——Numpy介绍]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Numpy%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——Numpy介绍Numpy介绍Numpy（Numerical Python）是一个开源的Python科学计算库，用于快速处理任意维度的数组。Numpy支持常见的数组和矩阵操作。对于同样的数值计算任务，使用Numpy比直接使用Python要简洁的多。Numpy使用ndarray对象来处理多维数组，该对象是一个快速而灵活的大数据容器。 ndarray介绍1234import numpy as np# 创建ndarraya = np.array([[1, 2], [3, 4]]) 返回结果为ndarray对象12array([[1, 2], [3, 4]]) ndarray的优势1.内存块风格ndarray在存储数据的时候，数据与数据的地址都是连续的，这样就给使得批量操作数组元素时速度更快。ndarray中的所有元素的类型都是相同的，而Python列表中的元素类型是任意的，所以ndarray在存储元素时内存可以连续，而python原生lis就t只能通过寻址方式找到下一个元素，这虽然也导致了在通用性能方面Numpy的ndarray不及Python原生list，但在科学计算中，Numpy的ndarray就可以省掉很多循环语句，代码使用方面比Python原生list简单的多。 2.支持并行化运算ndarray支持并行化运算，即支持向量化运算。 3.底层运算Numpy底层使用C语言编写，内部解除了GIL(全局解释器锁)，其对数组的操作速度不受Python解释器的限制，效率远高于纯Python代码。]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——饼图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F08%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%A5%BC%E5%9B%BE%E7%BB%98%E5%88%B6(pie)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——饼图绘制(pie)饼图广泛得应用在各个领域，用于表示不同分类的占比情况，通过弧度大小来对比各种分类。饼图通过将一个圆饼按照分类的占比划分成多个区块，整个圆饼代表数据的总量，每个区块（圆弧）表示该分类占总体的比例大小，所有区块（圆弧）的加和等于 100%。 1.饼图绘制与显示 注意显示的百分比的位数 plt.pie(x, labels=,autopct=,colors) x:数量，自动算百分比 labels:每部分名称 autopct:占比显示指定%1.2f%% colors:每部分颜色123456789101112import matplotlib.pylab as plt# 1.准备数据x = ['A', 'B', 'C', 'D', 'E']y = [1984, 3514, 4566, 7812, 1392]# 2.创建画布plt.figure(figsize=(8, 6), dpi=100)# 3.绘制饼图plt.pie(y, labels=x, autopct="%1.2f%%", colors=['b','r','g','y','c'])# 显示图例plt.legend()# 4.显示图像plt.show() 2.圆形饼图在plt.show()前添加代码1plt.axis('equal') 3.饼图应用场景 分类的占比情况（不超过9个分类）例如：班级男女分布占比，公司销售额占比]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——直方图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%BB%98%E5%88%B6(histogram)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——直方图绘制(histogram)直方图，形状类似柱状图却有着与柱状图完全不同的含义。直方图涉及统计学概念，首先要对数据进行分组，然后统计每个分组内数据元的数量。在坐标系中，横轴标出每个组的端点，纵轴表示频数，每个矩形的高代表对应的频数，称这样的统计图为频数分布直方图。 1.相关概念 组数：在统计数据时，我们把数据按照不同的范围分成几个组，分成的组的个数称为组数 组距：每一组两个端点的差 高度：表示频数 面积：表示数量 2.直方图与柱状图的对比 柱状图是以矩形的长度表示每一组的频数或数量，其宽度(表示类别)则是固定的，利于较小的数据集分析。 直方图是以矩形的长度表示每一组的频数或数量，宽度则表示各组的组距，因此其高度与宽度均有意义，利于展示大量数据集的统计结果。 由于分组数据具有连续性，直方图的各矩形通常是连续排列，而柱状图则是分开排列。 3.直方图绘制与显示 matplotlib.pyplot.hist(x, bins=None, normed=None, **kwargs) Parameters:x : (n,) array or sequence of (n,) arrays bins : integer or sequence or ‘auto’, optional 设置组距 设置组数（通常对于数据较少的情况，分为5~12组，数据较多，更换图形显示方式） 通常设置组数会有相应公式：组数 = 极差/组距= (max-min)/distance1234567891011121314151617# 1.准备数据x = [1, 2, 1, 2, 3, 4, 5, 7, 7, 8, 3, 4, 5, 2, 4, 7, 8, 9, 0 ,9, 8, 0, 8, 8, 0, 7, 4, 8, 8, 9, 9, 9, 7, 8, 9, 5, 3]# 2.创建画布plt.figure(figsize=(8, 6), dpi=100)# 3.绘制直方图# 设置组距distance = 1# 计算组数bins = int((max(x) - min(x)) / distance)# 绘制直方图plt.hist(x, bins=bins)# 修改x轴刻度显示plt.xticks(range(min(x), max(x))[::1])# 添加网格显示plt.grid(linestyle=&quot;--&quot;, alpha=0.5)# 4.显示图像plt.show() 4.直方图的应用场景 用于表示分布情况 通过直方图还可以观察和估计哪些数据比较集中，异常或者孤立的数据分布在何处例如：用户年龄分布，商品价格分布]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——柱状图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9F%B1%E7%8A%B6%E5%9B%BE%E7%BB%98%E5%88%B6(bar)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——柱状图绘制(bar)1.柱状图绘制与显示 plt.bar(x, width, align=’center’, **kwargs) Parameters:x : sequence of scalars. width : scalar or array-like, optional柱状图的宽度 align : {‘center’, ‘edge’}, optional, default: ‘center’Alignment of the bars to the x coordinates:‘center’: Center the base on the x positions.‘edge’: Align the left edges of the bars with the x positions.每个柱状图的位置对齐方式 **kwargs :color:选择柱状图的颜色 Returns:.BarContainerContainer with all the bars and optionally errorbars.123456789101112import matplotlib.pylab as plt# 1.准备数据x = ['A', 'B', 'C', 'D', 'E']y = [1984, 3514, 4566, 7812, 1392]# 2.创建画布plt.figure(figsize=(8,6),dpi=100)# 3.绘制柱状图plt.bar(x, y, width=0.5, color=['b','r','g','y','c'])# 添加网格显示plt.grid(linestyle="--", alpha=0.5)# 4.显示图像plt.show() 2.柱状图对比1234567891011121314151617181920import matplotlib.pylab as plt# 1.准备数据A = ['A', 'B', 'C', 'D', 'E']x = range(len(A))y = [1984, 3514, 4566, 7812, 1392]x_ = [i+0.2 for i in x]y_ = [3154, 1571, 4566, 9858, 2689]# 2.创建画布plt.figure(figsize=(8,6),dpi=100)# 3.绘制柱状图plt.bar(x, y, width=0.2, label='1')plt.bar(x_,y_, width=0.2, label='2')# 显示图例plt.legend()# 修改x轴刻度显示plt.xticks([i+0.1 for i in x], A)# 添加网格显示plt.grid(linestyle="--", alpha=0.5)# 4.显示图像plt.show() 柱状图应用场景适合用在分类数据对比场景上 数量统计 用户数量对比分析]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——散点图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%95%A3%E7%82%B9%E5%9B%BE%E7%BB%98%E5%88%B6(scatter)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——散点图绘制(scatter)1.散点图绘制与显示12345678910import matplotlib.pylab as plt# 1.准备数据x = [6.1101,5.5277,8.5186,7.0032,5.8598,8.3829,7.4764,8.5781,6.4862,5.0546,5.7107,14.164,]y = [17.592,9.1302,13.662,11.854,6.8233,11.886,4.3483,12,6.5987,3.8166,3.2522,15.505]# 2.创建画布plt.figure(figsize=(8, 6), dpi=100)# 3.绘制散点图plt.scatter(x, y)# 4.显示图像plt.show() 2.散点形状修改代码12# marker:str,可以更改散点的形状plt.scatter(x, y, marker='x') 3.应用场景 探究不同变量之间的内在关系]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——折线图绘制(plot)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%8A%98%E7%BA%BF%E5%9B%BE%E7%BB%98%E5%88%B6%E4%B8%8E%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87(plot)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——折线图绘制(plot)1.matplotlib.pyplot模块matplotlib.pytplot包含了一系列类似于matlab的画图函数。 它的函数作用于当前图形(figure)的当前坐标系(axes)。1import matplotlib.pyplot as plt 2.折线图绘制与显示12345678910111213# 1.创建画布(容器层)# figsize:指定图的长宽# dpi:图像的清晰度# plt.figure()返回fig对象plt.figure(figsize=(6, 4), dpi=100)# 2.绘制折线图(图像层)# 需要保证x,y维度一致# label:str,标签名x = [1, 2, 3, 4, 5, 6, 7]y = [4, 5, 6, 8, 9, 2, 3]plt.plot(x, y, label='A')# 3.显示图像plt.show() 3.图片保存1234# 1.创建画布，并设置画布属性plt.figure(figsize=(20, 8), dpi=80)# 2.保存图片到指定路径plt.savefig(path) 注意:plt.show()会释放figure资源，如果在显示图像之后保存图片将只能保存空图片。 4.添加自定义x,y刻度 plt.xticks(x, **kwargs)x:要显示的刻度值 plt.yticks(y, **kwargs)y:要显示的刻度值 在plt.show()前添加代码1234567# 构造x轴刻度标签x_ticks = range(10)# 构造y轴刻度y_ticks = range(10)# 修改x,y轴坐标的刻度显示plt.xticks(x_ticks[::2])plt.yticks(y_ticks[::2]) 5.添加网格显示在plt.show()前添加代码1234# True:显示网格# linestyle:str,网格线条形状# alpha:int,0到1,透明度plt.grid(True, linestyle='--', alpha=0.5) 6.添加描述信息添加x轴、y轴描述信息及标题在plt.show()前添加代码123456# 设置x轴描述信息plt.xlabel("x")# 设置y轴描述信息plt.ylabel("y")# 设置z轴描述信息plt.title("title") 注意:若使用中文需根据各操作系统添加中文支持 7.多次plot在plt.show()前添加代码123x_ = [1, 2, 3, 4, 5, 6, 7]y_ = [2, 3, 4, 2, 1, 0, 9]plt.plot(x_, y_, label="B") 8.设置图形风格 颜色字符 风格字符 r 红色 - 实线 g 绿色 - - 虚线 b 蓝色 -. 点划线 w 白色 : 点虚线 c 青色 ‘ ‘ 留空、空格 m 洋红 y 黄色 k 黑色 修改代码1plt.plot(x_, y_, 'r', linestyle='--', label="B") 9.显示图例在plt.show()前添加代码使用Location String1plt.legend(loc="best") 或者使用Location Code1plt.legend(loc=0) 颜色字符 风格字符 ‘best’ 0 ‘upper right’ 1 ‘upper left’ 2 ‘lower left’ 3 ‘lower right’ 4 ‘right’ 5 ‘center left’ 6 ‘center right’ 7 ‘lower center’ 8 ‘upper center’ 9 ‘center ‘ 10 折线图的应用场景 呈现公司产品(不同区域)每天活跃用户数 呈现app每天下载数量 呈现产品新功能上线后,用户点击次数随时间的变化 拓展：画各种数学函数图像 注意：plt.plot()除了可以画折线图，也可以用于画各种数学函数图像]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——Matplotlib介绍]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Matplotlib%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——Matplotlib介绍Matplotlib基本介绍 专门用于开发2D图表(包括3D图表) 使用起来及其简单 以渐进、交互式方式实现数据可视化 Matplotlib作用可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。 能将数据进行可视化,更直观的呈现 使数据更加客观、更具说服力 Matplotlib图像结构 Matplotlib三层结构1.容器层容器层主要由Canvas、Figure、Axes组成。Canvas是位于最底层的系统层，在绘图的过程中充当画板的角色，即放置画布(Figure)的工具。Figure是Canvas上方的第一层，也是需要用户来操作的应用层的第一层，在绘图的过程中充当画布的角色。Axes是应用层的第二层，在绘图的过程中相当于画布上的绘图区的角色。Figure:指整个图形(可以通过plt.figure()设置画布的大小和分辨率等)Axes(坐标系):数据的绘图区域Axis(坐标轴)：坐标系中的一条轴，包含大小限制、刻度和刻度标签特点为：一个figure(画布)可以包含多个axes(坐标系/绘图区)，但是一个axes只能属于一个figure。一个axes(坐标系/绘图区)可以包含多个axis(坐标轴)，包含两个即为2d坐标系，3个即为3d坐标系 2.辅助显示层辅助显示层为Axes(绘图区)内的除了根据数据绘制出的图像以外的内容，主要包括Axes外观(facecolor)、边框线(spines)、坐标轴(axis)、坐标轴名称(axis label)、坐标轴刻度(tick)、坐标轴刻度标签(tick label)、网格线(grid)、图例(legend)、标题(title)等内容。该层的设置可使图像显示更加直观更加容易被用户理解，但又不会对图像产生实质的影响。 3.图像层图像层指Axes内通过plot、scatter、bar、histogram、pie等函数根据数据绘制出的图像。 总结： Canvas（画板）位于最底层，用户一般接触不到 Figure（画布）建立在Canvas之上 Axes（绘图区）建立在Figure之上 坐标轴（axis）、图例（legend）等辅助显示层以及图像层都是建立在Axes之上 Matplotlib基本api]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook使用]]></title>
    <url>%2F2017%2F03%2F05%2FUtils%2FJupyter%20Notebook%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Jupyter Notebook使用使用pip命令安装 pip install jupyter 支持conda虚拟环境 conda install nb_conda 打开Jupyter Notebook # 终端输入jupyter notebook 本地notebook的默认URL为：http://localhost:8888想让notebook打开指定目录，只要进入此目录后执行命令即可 新建notebook文档 notebook的文档格式是.ipynb 内容界面操作-helloworld点击run 标题栏： 点击标题（如Untitled）修改文档名 菜单栏 导航-File-Download as，另存为其他格式 导航-Kernel Interrupt，中断代码执行（程序卡死时） Restart，重启Python内核（执行太慢时重置全部资源） Restart &amp; Clear Output，重启并清除所有输出 Restart &amp; Run All，重启并重新运行所有代码 cell操作cell：一对In Out会话被视作一个代码单元，称为cellJupyter支持两种模式： 编辑模式（Enter） 命令模式下回车Enter或鼠标双击cell进入编辑模式 可以操作cell内文本或代码，剪切／复制／粘贴移动等操作 命令模式（Esc） 按Esc退出编辑，进入命令模式 可以操作cell单元本身进行剪切／复制／粘贴／移动等操作 1.鼠标操作2.快捷键操作 两种模式通用快捷键 Shift+Enter，执行本单元代码，并跳转到下一单元 Ctrl+Enter，执行本单元代码，留在本单元 cell行号前的 * ，表示代码正在运行 命令模式：按ESC进入 Y，cell切换到Code模式 M，cell切换到Markdown模式 A，在当前cell的上面添加cell B，在当前cell的下面添加cell 双击D：删除当前cell Z，回退 L，为当前cell加上行号 &lt;!– Ctrl+Shift+P，对话框输入命令直接运行 快速跳转到首个cell，Crtl+Home 快速跳转到最后一个cell，Crtl+End –&gt; 编辑模式：按Enter进入 多光标操作：Ctrl键点击鼠标（Mac:CMD+点击鼠标） 回退：Ctrl+Z（Mac:CMD+Z） 重做：Ctrl+Y（Mac:CMD+Y) 补全代码：变量、方法后跟Tab键 为一行或多行代码添加/取消注释：Ctrl+/（Mac:CMD+/） 屏蔽自动输出信息：可在最后一条语句之后加一个分号]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Utils</tag>
        <tag>Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的拓扑排序]]></title>
    <url>%2F2016%2F12%2F15%2FPython%2F%E5%9B%BE%E7%9A%84%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Python有向无环图的拓扑排序拓扑排序的官方定义为：由某个集合上的一个偏序得到该集合上的一个全序，这个操作称之为拓扑排序。而个人认为，拓扑排序即是在图的基本遍历法上引入了入度的概念并围绕入度来实现的排序方法，拓扑排序与Python多继承中mro规则的排序类似，若想深入研究mro规则的C3算法，不妨了解一下 DAG(有向无环图) 的拓扑排序。 入度：指有向图中某节点被指向数目之和有向无环图：Directed Acyclic Graph，简称DAG，若熟悉机器学习则肯定对DAG不陌生，如ANN、DNN、CNN等则都是典型的DAG模型，对这类模型此处不再过多敷述，有兴趣的可以自行学习。 以一个有向无环图为例，如下图：123456789# 定义图结构graph = &#123;"A": ["B","C"],"B": ["D","E"],"C": ["D","E"],"D": ["F"],"E": ["F"],"F": [],&#125; 如图A的指向的元素为B、CB的指向的元素为D、EC的指向的元素为D、ED的指向的元素为FE的指向的元素为FF的指向的元素为空即A的入度为0，B的入度为1，C的入度为1，D的入度为2，E的入度为2，F的入度为2在DAG的拓扑排序中，每次都选取入度为 0 的点加入拓扑队列中，再删除与这一点连接的所有边。首先找到入度为0的点A，把A从队列中取出，同时添加到结果中并把A相关的指向移除，即B、C的入度减少1变为0并将B，C添加到队列中，再从队列首部取出入度为0的节点，以此类推，最后输出结果，完成DAG的拓扑排序。123456789101112131415161718192021222324def TopologicalSort(G):# 创建入度字典in_degrees = dict((u, 0) for u in G)# 获取每个节点的入度for u in G:for v in G[u]:in_degrees[v] += 1# 使用列表作为队列并将入度为0的添加到队列中Q = [u for u in G if in_degrees[u] == 0]res = []# 当队列中有元素时执行while Q:# 从队列首部取出元素u = Q.pop()# 将取出的元素存入结果中res.append(u)# 移除与取出元素相关的指向，即将所有与取出元素相关的元素的入度减少1for v in G[u]:in_degrees[v] -= 1# 若被移除指向的元素入度为0，则添加到队列中if in_degrees[v] == 0:Q.append(v)return resprint(TopologicalSort(graph)) 输出结果：1['A', 'C', 'B', 'E', 'D', 'F'] 代码输出结果与上述分析相符]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>图的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的BFS与DFS]]></title>
    <url>%2F2016%2F12%2F11%2FPython%2F%E5%9B%BE%E7%9A%84BFS%E4%B8%8EDFS%2F</url>
    <content type="text"><![CDATA[Python图的BFS与DFS BFS:Breadth First Search，广度优先搜索DFS:Depth First Search，深度优先搜索 BFS与DFS都属于图算法，BFS与DFS分别由队列和堆栈来实现，基本的定义与实现过程见前一篇文章，本篇文章基于树的BFS与DFS进行扩展，实现无向图(即没有指定方向的图结构)的BFS与DFS。以一个无向图为例，如下图：123456789# 定义图结构graph = &#123;"A": ["B","C"],"B": ["A", "C", "D"],"C": ["A", "B", "D","E"],"D": ["B", "C", "E","F"],"E": ["C", "D"],"F": ["D"],&#125; 如图A的相邻元素为B、CB的相邻元素为A、C、DC的相邻元素为A、B、D、ED的相邻元素为B、C、E、FE的相邻元素为C、DF的相邻元素为D BFS优先遍历当前节点的相邻节点，即若当前节点为A时，则继续遍历的节点为B和C；当A的所有相邻节点遍历完以后，再遍历A相邻节点B和C的所有相邻节点，以B为例，在遍历B的相邻节点时，由于A已被访问过，则需要标记为已访问，在遍历B的相邻节点时，不再需要访问A。以此类推，完成无向图的BFS。DFS优先遍历与当前节点0相邻的一个节点1，然后再访问与节点1相邻但与节点0不相邻的节点，即若当前节点为A，则继续遍历B或C，再访问与B或C节点相邻且与A节点不相邻的节点，即节点D或E，若没有未遍历过的相邻节点，则返回访问上一个有未被访问过相邻节点的节点进行访问，依此遍历整个图，完成无向图的DFS。代码中为了更直观地观察遍历顺序，采用直接打印遍历元素的方式输出遍历结果代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def BFS(graph,vertex):# 使用列表作为队列queue = []# 将首个节点添加到队列中queue.append(vertex)# 使用集合来存放已访问过的节点looked = set()# 将首个节点添加到集合中表示已访问looked.add(vertex)# 当队列不为空时进行遍历while(len(queue)&gt;0):# 从队列头部取出一个节点并查询该节点的相邻节点temp = queue.pop(0)nodes = graph[temp]# 遍历该节点的所有相邻节点for w in nodes:# 判断节点是否存在于已访问集合中,即是否已被访问过if w not in looked:# 若未被访问,则添加到队列中,同时添加到已访问集合中,表示已被访问queue.append(w)looked.add(w)print(temp,end=' ')def DFS(graph,vertex):# 使用列表作为栈stack = []# 将首个元素添加到队列中stack.append(vertex)# 使用集合来存放已访问过的节点looked = set()# 将首个节点添加到集合中表示已访问looked.add(vertex)# 当队列不为空时进行遍历while len(stack)&gt;0:# 从栈尾取出一个节点并查询该节点的相邻节点temp = stack.pop()nodes = graph[temp]# 遍历该节点的所有相邻节点for w in nodes:# 判断节点是否存在于已访问集合中,即是否已被访问过if w not in looked:# 若未被访问,则添加到栈中,同时添加到已访问集合中,表示已被访问stack.append(w)looked.add(w)print(temp,end=' ')# 由于无向图无根节点，则需要手动传入首个节点，此处以"A"为例print("BFS",end=" ")BFS(graph,"A")print("")print("DFS",end=" ")DFS(graph,"A") 输出结果：12BFS A B C D E F DFS A C E D F B 代码输出结果与上述分析相符]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>图的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树的BFS与DFS]]></title>
    <url>%2F2016%2F12%2F07%2FPython%2F%E6%A0%91%E7%9A%84BFS%E4%B8%8EDFS%2F</url>
    <content type="text"><![CDATA[Python树的BFS与DFS BFS:Breadth First Search，广度优先搜索DFS:Depth First Search，深度优先搜索 BFS与树的层序遍历类似，DFS则与树的后序遍历有着区别。 BFS(广度优先搜索)： 使用队列实现 每次从队列的头部取出一个元素，查看这个元素所有的下一级元素，再把它们放到队列的末尾。并把这个元素记为它下一级元素的前驱。 优先遍历取出元素下一级的同级元素 DFS(深度优先搜索): 使用栈实现 每次从栈的末尾弹出一个元素，搜索所有在它下一级的元素，把这些元素压入栈中。并把这个元素记为它下一级元素的前驱。 优先遍历弹出元素下一级的下一级元素 以一颗满二叉树为例，如下图123456789# 定义节点类class Node(object):def __init__(self,val,left=None,right=None):self.val = valself.left = leftself.right = right# 创建树模型node = Node("A",Node("B",Node("D"),Node("E")),Node("C",Node("F"),Node("G"))) 如图，A节点的下一级元素为B节点和C节点，B节点的下一级元素为D节点和E节点，C节点的下一级元素为F节点和G节点。BFS优先遍历当前节点下一级节点的同级元素，即若当前节点为A节点，则继续遍历的节点为B和C；当A的所有相邻节点遍历完以后，再遍历B节点的相邻节点D节点和E节点，以及C节点的相邻节点F节点和G节点。至此，所有节点遍历完成。DFS优先遍历当前节点下一级节点的下一级元素，即若当前节点为A节点，则继续遍历的节点为B节点和B节点的下一级节点D节点；D节点没有下一级节点，此时再返回D节点的上一级B节点处，再遍历B节点的另一个下一级元素E节点，若没有未遍历过的下一级元素，则返回上一级，依此规律遍历整个树，完成树的DFS。代码中为了更直观地观察遍历顺序，采用直接打印node.val的方式输出遍历结果代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041def BFS(root):# 使用列表作为队列queue = []# 将首个根节点添加到队列中queue.append(root)# 当队列不为空时进行遍历while queue:# 从队列头部取出一个节点并判断其是否有左右节点# 若有子节点则把对应子节点添加到队列中，且优先判断左节点temp = queue.pop(0)left = temp.leftright = temp.rightif left:queue.append(left)if right:queue.append(right)print(temp.val,end=" ")def DFS(root):# 使用列表作为栈stack = []# 将首个根节点添加到栈中stack.append(root)# 当栈不为空时进行遍历while stack:# 从栈的末尾弹出一个节点并判断其是否有左右节点# 若有子节点则把对应子节点压入栈中，且优先判断右节点temp = stack.pop()left = temp.leftright = temp.rightif right:stack.append(right)if left:stack.append(left)print(temp.val,end=" ")print("BFS",end=" ")BFS(node)print("")print("DFS",end=" ")DFS(node) 输出结果：12BFS A B C D E F G DFS A B D E C F G 代码输出结果与上述分析相符]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>树的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树的前序、中序、后序遍历]]></title>
    <url>%2F2016%2F12%2F05%2FPython%2F%E6%A0%91%E7%9A%84%E5%89%8D%E5%BA%8F%E3%80%81%E4%B8%AD%E5%BA%8F%E3%80%81%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[Python树的前序、中序、后序遍历树的基础遍历分为三种：前序遍历、中序遍历、后序遍历 前序遍历：根节点-&gt;左子树-&gt;右子树中序遍历：左子树-&gt;根节点-&gt;右子树后序遍历：左子树-&gt;右子树-&gt;根节点 首先自定义树模型 1234567class Node(object):def __init__(self,val,left=None,right=None):self.val = valself.left = leftself.right = rightnode = Node("A",Node("B",Node("D"),Node("E")),Node("C",Node("F"),Node("G"))) 代码中为了更直观地观察遍历顺序，采用直接打印node.val的方式输出遍历结果代码实现： 1234567891011121314151617181920212223242526272829303132# 前序遍历def PreTraverse(root):if root == None:returnprint(root.val,end=" ")PreTraverse(root.left)PreTraverse(root.right)# 中序遍历def MidTraverse(root):if root == None:returnMidTraverse(root.left)print(root.val,end=" ")MidTraverse(root.right)# 后序遍历def AfterTraverse(root):if root == None:returnAfterTraverse(root.left)AfterTraverse(root.right)print(root.val,end=" ")print("前序遍历",end="")PreTraverse(node)print("")print("中序遍历",end="")MidTraverse(node)print("")print("后序遍历",end="")AfterTraverse(node) 输出结果：123前序遍历 A B D E C F G 中序遍历 D B E A F C G 后序遍历 D E B F G C A 输入结果与上述分析一致]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>树的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯消去求线性方程的解]]></title>
    <url>%2F2016%2F11%2F03%2FPython%2F%E9%AB%98%E6%96%AF%E6%B6%88%E5%8E%BB%E6%B1%82%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%9A%84%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[数学上，高斯消元法（或译：高斯消去法），是线性代数规划中的一个算法，可用来为线性方程组求解。但其算法十分复杂，不常用于加减消元法，求出矩阵的秩，以及求出可逆方阵的逆矩阵。不过，如果有过百万条等式时，这个算法会十分省时。一些极大的方程组通常会用迭代法以及花式消元来解决。当用于一个矩阵时，高斯消元法会产生出一个“行梯阵式”。高斯消元法可以用在电脑中来解决数千条等式及未知数。亦有一些方法特地用来解决一些有特别排列的系数的方程组。1234567891011121314151617181920212223242526272829'''高斯消去法通过消元过程把一般方程组化成三角方程组再通过回代过程求出方程组的解'''def GaussianElimination(A,B):N = len(A)for i in range(1,N):for j in range(i,N):# 计算消元因子deltadelta = A[j][i-1]/A[i-1][i-1]# 从第i-1行开始消元for k in range(i-1,N):# 对A进行消元A[j][k] = A[j][k] - A[i-1][k]*delta# 对B进行消元B[j] = B[j]-B[i-1]*delta# 进行回代，直接将方程的解保留在B中B[N-1] = B[N-1]/A[N-1][N-1]for i in range(N-2,-1,-1):for j in range(N-1,i,-1):B[i] = B[i]- A[i][j]*B[j]B[i] = B[i]/A[i][i]# 返回所有解的列表return BmatrixA = [[1,3,3],[-2,3,-5],[2,5,6]]matrixB = [4,0,1]print('方程的解为',GaussianElimination(matrixA, matrixB)) 输出的结果：1方程的解为 [148.0, 7.0, -55.0]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>解线性方程</tag>
      </tags>
  </entry>
</search>
