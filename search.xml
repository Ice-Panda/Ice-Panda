<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据与MapReduce学习笔记——MapReduce]]></title>
    <url>%2F2018%2F10%2F27%2FMapReduce%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EMapReduce%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94MapReduce%2F</url>
    <content type="text"><![CDATA[大数据与MapReduce学习笔记——MapReduceMapReduce：分布式计算的框架 优点：可在短时间内完成大量工作 缺点：算法必须经过重写，需要对系统工程有一定的理解 使用数据类型：数值型和标称型数据 MapReduce是一个软件框架，可以将单个计算作业分给多台计算机执行。它假定这些作业在单机熵需要很长的运行时间，因此使用多台机器缩短运行时间。常见的例子是日常统计数字的汇总，该任务单机熵执行时间将超过一整天。MapReduce在大量节点组成的集群上运行。它的工作流程是：单个作业被分成很多小份，输入数据也被切片分发到每个节点，各个节点只在本地数据上做运算，对应的运算代码称为mapper，这个过程被称作map阶段。每个mapper的输出通过某种方式组合(一般还会做排序)。排序后的结果再被分成小份分发到各个节点进行下一步处理工作。第二步的处理阶段被称为reduce阶段，对应的运行代码被称为reducer。reducer的输出就是程序的最终执行结果。MapReduce的优势在于，它使得程序以并行方式执行。如果集群由10个节点组成，而原先的作业需要10个小时来完成，那么应用MapReduce，该作业将在一个多小时之后得到同样的结果。 在任何时候，每个mapper或reducer之间都不进行通讯(这里指mapper各自之间不通信，reducer各自之间不通信，而reducer会接收mapper生成的数据)。每个节点只处理自己的事务，且在本地分配的数据集熵运算。 不同类型的作业可能需要不同数目的reducer。数据会以键值对的形式传递，sort阶段按照key把数据分类，之后合并。最终每个reducer就会收到相同的key值。reducer的数量并不是固定的。MapReduce的框架中海油其他一些灵活。MapReduce的整个编配工作由主节点(master node)控制。这些主节点控制整个MapReduce作业编配，包括每份数据存放的节点位置，以及map、sort和reduce等阶段的时序控制等。此外，主节点还要包含容错机制。一般地，每份mapper的输入数据会同时分发到多个节点形成多份副本，用于事务失效处理。一个MapReduce集群的示意图如下：如上MapReduce框架的示意图。在该集群中有3台双核机器，如果机器0失效，作业扔可以正常继续。图中每台机器都有两个处理器，可以同时处理两个map或者reduce任务。如果机器0在map阶段宕机，主节点在发现该问题之后，会将机器0移出集群，并在剩余的节点上继续执行作业。在一些MapReduce的实现中，在多个机器上都保存有数据的多个备份，例如在机器0上存放的输入数据可能还存放在机器1上，以防机器0出现问题。同时，每个节点都必须与主节点通信，表明自己工作正常。如果某个节点失效或者工作异常，主节点将重启该节点或者将该节点移出可用机器池。 MapReduce总结 主节点控制MapReduce的作业流程 Mapreduce的作业可用分成map任务和reduce任务 map任务之间不做数据交流，reduce任务也一样 在map和reduce阶段中间，有一个sort或combine阶段 数据被重复存放在不同的机器上，以防某个机器失效 mapper和reducer传输的数据形式为键值对]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数]]></title>
    <url>%2F2018%2F06%2F21%2FMachine%20Learning%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[激活函数1.定义神经网络中的每个节点接受输入值，并将输入值传递给下一层，输入节点会将输入属性值直接传递给下一层(隐层或输出层)。在神经网络中，隐层和输出层节点的输入和输出之间具有函数关系，这个函数称之为激活函数。 2.作用如果不适用激活函数，每一层输出都是上一层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机(Perceptron)。如果使用激活函数，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。 3.常用的激活函数3.1.Sigmoid函数Sigmoid函数是一个在生物学中常见的S型函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。公式如下：$$f(x)=\frac{1}{1+e^{-x}}$$函数图像如下： 3.2.tanh函数tanh是上去西安函数中的一个，tanh()为双曲正切。在数学中，双曲正切tanh是由基本双曲函数双曲正弦和双曲余弦推导而来。公式如下：$$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{2}{1+e^{-2x}}-1=2sigmoid(2x)-1$$函数图像如下： 3.3.softplus函数公式如下：$$f(x)=log(1+e^x)$$函数图像如下： 3.4.softsign函数公式如下：$$f(x)=\frac{x}{|x|+1}$$函数图像如下： 3.5.ReLU函数Relu激活函数(Rectified Linear Unit)，线性整流函数，又称修正线性单元，用于隐层神经元输出。ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient。ReLU虽然简单，但是却是近几年的重要成果，有以下几大优点： 1.解决了gradient vanishing问题 (在正区间) 2.计算速度非常快，只需要判断输入是否大于0 3.收敛速度远快于sigmoid和tanh ReLU也有几个需要特别注意的问题： 1.ReLU的输出不是zero-centered 2.Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生：1.非常不幸的参数初始化，这种情况比较少见2.learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 公式如下：$$f(x)=max(0,x)$$函数图像如下： 3.6.ELU函数Exponential Linear unit，指数线性单元，ELU函数是针对ReLU函数的一个改进型，相对于ReLU函数，在输入为复数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除ReLU死掉的问题，不过还是有梯度消失和指数运算的问题。公式如下：$$f(x)=\begin{cases}x,&amp; x&gt;0\\alpha(e^x-1),&amp; x \leq 0\end{cases} $$函数图像如下： 3.7.LReLU函数即Leaky ReLU，泄漏整流线性单元，为了解决ReLU的死去问题，提出了将ReLU的前半段设为$\alpha{x}$而非0，通常$\alpha=0.01$。理论上来讲，LReLU函数有ReLU的所有优点，外加不会有ReLU死去问题，但是在实际操作当中，并没有完全证明LReLU总是优于ReLU。公式如下：$$f(x)=\begin{cases}x,&amp; x&gt;0\\alpha{x},&amp; x \leq 0\end{cases} $$其中$\alpha$是固定值，默认$\alpha=0.01$函数图像如下： 3.8.PReLU函数另外一种解决ReLU死去问题的直观的想法是基于参数的方法，即Parametric ReLU函数，参数化修正线性单元。公式如下：$$f(x)=\begin{cases}x,&amp; x&gt;0\\alpha{x},&amp; x \leq 0\end{cases} $$其中$\alpha$是可以学习的。如果$\alpha=0$，那么PReLU退化为ReLU；如果$\alpha$是一个很小的固定值(如$\alpha=0.01$)，则PReLU退化为Leaky ReLU(LReLU)。PReLU只增加了极少量的参数，也就意味着网络的计算量以及过拟合的危险性都只增加了一点点。特别的，当不同通道使用相同的$\alpha$时候，参数就更少了。BP更新$\alpha$时，采用的是带动量的更新方式。 3.9.RReLU函数即Randomized Leaky ReLU函数，随机带泄露的修正线性单元，与Leaky ReLU以及PReLU很相似，为负责输入添加了一个线性项。而最关键的区别是，这个线性项的斜率在每一个节点上都是随机分配的(通常服从均匀分布)。 函数 优点 缺点 sigmoid函数 在整个定义域内可导。 梯度在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓收敛速度。计算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。 tanh函数 在整个定义域内可导。 梯度在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓收敛速度。计算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。 softplus函数 在整个定义域内可导。 梯度在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓收敛速度。计算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。 ReLU函数 ReLU的梯度在大多数情况下是常数，有助于解决深层网络收敛问题。ReLU更容易学习优化。因为其分段线性性质，导致其前传、后传、求导都是分段线性。ReLU会使一部分神经元的输出为0.这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生，也更接近真实的神经元激活模型。 如果后层的某一个梯度特别大，导致W更新以后变得特别大，导致该层的输入&lt;0，输出为0，这时该层就会死去，没有更新。当学习率比较大时可能会有40%的神经元都会在训练开始就会死去，因此需要对学习率进行一个好的设置。 注意： tanh特征相差明显时的效果，在循环过程中会不断扩大特征效果显示出来，但是有，在特征相差比较复杂或是相差不是特别大时，需要更细微的分类判断的时候，sigmoid效果就好了。 sigmoid和tanh作为激活函数时，需要注意对input进行归一化，否则激活后的值都会进入平坦区，使隐层的输出全部趋同，但是ReLU并不需要输入归一化来放置它们达到饱和。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[太仓房价热力图]]></title>
    <url>%2F2018%2F06%2F20%2FProject%2F%E5%A4%AA%E4%BB%93%E6%88%BF%E4%BB%B7%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[太仓房价热力图 ul,li{list-style: none;margin:0;padding:0;float:left;} html{height:100%} body{height:100%;margin:0px;padding:0px;font-family:"微软雅黑";} #Heatmap{height:70vh;width:100%;} #r-result{width:100%;position: relative;top: -68vh;left: 10px;} button{height: 30px;width:100px;font-size: 16px;border-radius: 6px} p{margin-left:5px; font-size:14px;} 显示热力图 关闭热力图 function loadJScript() { var script = document.createElement("script"); script.type = "text/javascript"; script.src = "https://api.map.baidu.com/api?v=2.0&ak=dTGv8eqlGerVY2gEkO2TyarYozkAKZpM&callback=init"; document.body.appendChild(script); } function init() { var Heatmap = new BMap.Map("Heatmap"); // 创建Map实例 var center_lng = 121.11707440329229; var center_lat = 31.45102211467977; var point = new BMap.Point(center_lng,center_lat); // 创建点坐标 Heatmap.centerAndZoom(point, 15); Heatmap.enableScrollWheelZoom(); //启用滚轮放大缩小 if(!isSupportCanvas()){ alert('热力图目前只支持有canvas支持的浏览器,您所使用的浏览器不能使用热力图功能~') } //详细的参数,可以查看heatmap.js的文档 https://github.com/pa7/heatmap.js/blob/master/README.md //参数说明如下: /* visible 热力图是否显示,默认为true * opacity 热力的透明度,1-100 * radius 势力图的每个点的半径大小 * gradient {JSON} 热力图的渐变区间 . gradient如下所示 * { .2:'rgb(0, 255, 255)', .5:'rgb(0, 110, 255)', .8:'rgb(100, 0, 255)' } 其中 key 表示插值的位置, 0~1. value 为颜色值. */ heatmapOverlay = new BMapLib.HeatmapOverlay({"radius":20}); Heatmap.addOverlay(heatmapOverlay); heatmapOverlay.setDataSet({data:constant,max:50000}); // 是否显示热力图 s = document.getElementById('show'); h = document.getElementById('hide'); // alert(h.innerHTML); flag = false; openHeatmap = function(){ if(flag){ heatmapOverlay.show(); s.style.display = 'none'; h.style.display = 'inline-block'; flag = false } else{ heatmapOverlay.hide(); h.style.display = 'none'; s.style.display = 'inline-block'; flag = true } }; openHeatmap(); function setGradient(){ /*格式如下所示: { 0:'rgb(102, 255, 0)', .5:'rgb(255, 170, 0)', 1:'rgb(255, 0, 0)' }*/ var gradient = {}; var colors = document.querySelectorAll("input[type='color']"); colors = [].slice.call(colors,0); colors.forEach(function(ele){ gradient[ele.getAttribute("data-key")] = ele.value; }); heatmapOverlay.setOptions({"gradient":gradient}); } // 判断浏览区是否支持canvas function isSupportCanvas(){ var elem = document.createElement('canvas'); return !!(elem.getContext && elem.getContext('2d')); } // 添加带有定位的导航控件 var navigationControl = new BMap.NavigationControl({ // 靠左上角位置 anchor: BMAP_ANCHOR_TOP_RIGHT, // LARGE类型 type: BMAP_NAVIGATION_CONTROL_LARGE, // 启用显示定位 enableGeolocation: true }); Heatmap.addControl(navigationControl); // 添加定位控件 var geolocationControl = new BMap.GeolocationControl({anchor:BMAP_ANCHOR_BOTTOM_RIGHT}); geolocationControl.addEventListener("locationSuccess", function(e){ // 定位成功事件 var address = ''; address += e.addressComponent.province; address += e.addressComponent.city; address += e.addressComponent.district; address += e.addressComponent.street; address += e.addressComponent.streetNumber; alert("当前定位地址为：" + address); }); geolocationControl.addEventListener("locationError",function(e){ // 定位失败事件 alert(e.message); }); Heatmap.addControl(geolocationControl); } // 异步加载地图 window.onload = loadJScript;]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>html/css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP算法广义误差]]></title>
    <url>%2F2018%2F06%2F15%2FMachine%20Learning%2FBP%E7%AE%97%E6%B3%95%E5%B9%BF%E4%B9%89%E8%AF%AF%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[BP算法广义误差$二次方误差和E = \frac{1}{2}\sum\limits_{j=1}^{n_l}(期望输出 - 实际输出)^2$$$E = \frac{1}{2}\sum_{j=1}^{n_l} (y_j-a_j)^2$$将误差函数向量化$$E = \frac{1}{2}||y-a^{(n_l)}||^2$$将误差函数进行化简$$\begin{split}E &amp;=\frac{1}{2}||y-a^{(n_l)}||^2\\&amp;=\frac{1}{2}(y-a)^T(y-a)\\&amp;=\frac{1}{2}(y^Ty-y^Ta-a^Ty+a^Ta)\end{split}$$首先给出定义：$a\in\mathbb{R}^{n\times{1}}$ $y\in\mathbb{R}^{n\times{1}}$ $\delta\in\mathbb{R}^{n\times{1}}$则输出层广义误差为：$$\begin{split}\delta^{(n_l)} &amp;= \frac{\partial{E^{(n_l)}}}{\partial{z^{(n_l)}}} \\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}\frac{\partial{E^{(n_l)}}}{\partial{a^{(n_l)}}} \\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}\frac{\partial}{\partial{a^{(n_l)}}}[\frac{1}{2}(y^Ty-y^Ta-a^Ty+a^Ta)] \\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}[\frac{1}{2}(0-y-y+2a^{(n_l)})] \\&amp;=\frac{\partial{g(z^{(n_l)})}}{\partial{z^{(n_l)}}}(a^{(n_l)}-y)\end{split}$$ 隐含层广义误差为：$$\begin{split}\delta^{(n_l-1)}&amp;=\frac{\partial{E^{(n_l)}}}{\partial{z^{(n_l-1)}}}\\&amp;=\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l-1)}}}\frac{\partial{E^{(n_l)}}}{\partial{a^{(n_l)}}}\\&amp;=\frac{\partial{z^{(n_l)}}}{\partial{z^{(n_l-1)}}}\frac{\partial{a^{(n_l)}}}{\partial{z^{(n_l)}}}(a^{(n_l)}-y)\\&amp;=\frac{\partial{z^{(n_l)}}}{\partial{z^{(n_l-1)}}}\delta^{(n_l)}\\&amp;=\frac{\partial{\Theta^{(n_l-1)}g(z^{(n_l-1)})}}{\partial{z^{(n_l-1)}}}\delta^{(n_l)}\\&amp;=\frac{\partial{g(z^{(n_l-1)})}}{\partial{z^{(n_l-1)}}}(\Theta^{(n_l-1)})^T\delta^{(n_l)}\\\end{split}$$$a^{(n_l)}=h_{\Theta}(x)=g(z^{(n_l)})$由于sigmoid标量导数$g’(z)=g(z)[1-g(z)]$，固在向量化后$g’(z^{(n_l)})=a^{(n_l)}*(1-a^{(n_l)})$，所以可得隐含层广义误差与下一层广义误差的关系为：$$\delta^{(n_l-1)}=(\Theta^{(n_l-1)})^T\delta^{(n_l)}*{g’(z^{(n_l-1)})} $$从而实现BP算法]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>BPNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同方差性与异方差性]]></title>
    <url>%2F2018%2F06%2F10%2FMachine%20Learning%2F%E5%90%8C%E6%96%B9%E5%B7%AE%E6%80%A7%E4%B8%8E%E5%BC%82%E6%96%B9%E5%B7%AE%E6%80%A7%2F</url>
    <content type="text"><![CDATA[同方差性与异方差性所谓同方差，是为了保证回归参数估计量具有良好的统计性质，经典线性回归模型的一个重要假定，总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。 若线性回归模型存在异方差性，则用传统的最小二乘法(OLS)估计模型，得到的参数估计量不是有效估计量，甚至也不是渐进有效哦的估计量；此时也无法对模型参数进行有关显著性校验。 异方差的检测事实证明，实际问题中经常会出现异方差性，这将影响回归模型的估计、检验和应用。因此在建立回归模型时应检验模型是否存在异方差性。异方差性检验方法如下: 1.图示校验法 2.Goldfeld-Quandt校验 3.White校验发 4.Park校验法 5.Gleiser校验法 异方差破坏古典模型的基本假定在古典回归模型的假定下，普通最小二乘法估计量是线性、无偏、有效估计量，即在所有无偏估计量中，最小二乘法估计量具有最小方差性——它是个有效估计量。如果在其他假定不变的条件下，允许随机扰动项ui存在异方差性，即ui的方差随观测值的变化而变化，这就违背了最小二乘法估计的高斯——马尔柯夫假设，这时如果继续使用最小二乘法对参数进行估计，就会产生以下后果： 1.参数估计量仍然是线性无偏的，但不是有效的； 2.异方差模型中的方差不再具有最小方差性； 3.T检验失去作用； 4.模型的预测作用遭到破坏。 T检验，亦称student t检验(Student’s t test)，主要用户样本含量较小(例如n&lt;30)总体标准差$\sigma$未知的正态分布。T检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。它与F检验、卡方检验并列。T检验是戈斯特为了观测酿酒质量而发明的，并于1908年在Biometrika上发布。 存在异方差性解决方法: 对模型变换，当可以确定异方差的具体形式时，将模型作适当变换有可能消除或减轻异方差的影响。 使用加权最小二乘法，对原模型变换的方法与加权二乘法实际上是等价的，可以消除异方差。 对数变换，运用对数变换能使测定变量值的尺度缩小。它可以将两个数值之间原来10倍的差异缩小到只有2倍的差异。其次，经过对数变换后的线性模型，其残差表示相对误差，而相对误差往往比绝对误差有较小的差异。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>方差</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归的梯度下降算法]]></title>
    <url>%2F2018%2F06%2F08%2FMachine%20Learning%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[逻辑回归的梯度下降算法 本文阐述逻辑回归代价函数的梯度下降算法推导过程，为满足广义性，采用多变量的逻辑回归代价函数进行推导。 首先给出逻辑回归的代价函数(Cost Function)(即交叉熵)：$$J(\theta) = \frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$$其中假设函数$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx^{(i)}}}$，$m$为样本总数，参数$\theta$为$n+1$维列向量。由于使用多变量进行梯度下降，固可以使用批量梯度下降法来获得全局最优解。推导过程如下，首先化简代价函数：$$\begin{split}J(\theta) &amp;= -\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]\\&amp;= -\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log(\frac{1}{1+e^{-\theta^Tx^{(i)}}})-(1-y^{(i)})log(1-\frac{1}{1+e^{-\theta^Tx^{(i)}}})]\\&amp;= -\frac{1}{m}\sum\limits_{i=1}^m[-y^{(i)}log(1+e^{-\theta^Tx^{(i)}})-(1-y^{(i)})log(1+e^{\theta^Tx^{(i)}})]\\\end{split}$$对$\theta_j$求偏导:$$\begin{split}\frac{\partial}{\partial\theta_j}J(\theta)&amp;=-\frac{\partial}{\partial\theta_j}\frac{1}{m}\sum\limits_{i=1}^m[-y^{(i)}log(1+e^{-\theta^Tx^{(i)}})-(1-y^{(i)})log(1+e^{\theta^Tx^{(i)}})]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[-y^{(i)}\frac{-x_j^{(i)}e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}-(1-y^{(i)})\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{x_j^{(i)}y^{(i)}e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}-\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}+\frac{x_j^{(i)}y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{x_j^{(i)}y^{(i)}e^{-\theta^Tx^{(i)}}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}-\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}+\frac{x_j^{(i)}y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{x_j^{(i)}y^{(i)}-x_j^{(i)}e^{\theta^Tx^{(i)}}+x_j^{(i)}y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{y^{(i)}-e^{\theta^Tx^{(i)}}+y^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[\frac{y^{(i)}(1+e^{\theta^Tx^{(i)}})-e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}-\frac{e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}-\frac{1}{1+e^{-\theta^Tx^{(i)}}}]x_j^{(i)}\\&amp;=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}-h_\theta(x^{(i)})]x_j^{(i)}\\&amp;=\frac{1}{m}\sum\limits_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}\\\end{split}$$在参数$\theta$中引入学习速率$\alpha$：$$\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta),(j=0,1,…,n)$$将$\frac{\partial}{\partial\theta_j}J(\theta)$代入，得出逻辑回归的批量梯度下降算法：$$\theta_j=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m[(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}],(j=0,1,…,n)$$将上述公式进行向量化：$\Theta\in\mathbb{R}^{n+1\times{1}}$ $X\in\mathbb{R}^{m\times{n+1}}$ $y\in\mathbb{R}^{m\times{1}}$则批量梯度下降算法可表示为：$$\Theta=\Theta-\frac{\alpha}{m}X^{T}(X\Theta-y)$$向量化后，可以使计算更简洁，并且也能保证各$\theta$的值保持同步更新，以下是代码实现，其中数据来源于斯坦福Andrew Ng公开课：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import numpy as npimport pandas as pdimport matplotlib.pylab as pltdef Sigmoid(z): return 1 / (1 + np.exp(-z))def h(X, theta): return Sigmoid(X @ theta)def CostFunction(X, y, theta): return -np.mean(np.multiply(y, np.log(h(X, theta))) + np.multiply(1 - y, np.log(1 - h(X, theta))))def Gradient(X, y, theta, alpha): return theta - alpha * X.T @ (h(X, theta) - y) / X.shape[0]def BatchGradientDecent(X, y, theta, alpha=0.01, iters=500000): cost = np.zeros(iters) for i in range(iters): theta = Gradient(X, y, theta, alpha) cost[i] = CostFunction(X, y, theta) return theta, costdef predict(theta, X): return [1 if x &gt;= 0.5 else 0 for x in h(X, theta)]if __name__ == '__main__': path = 'ex2data1.txt' data = pd.read_csv(path, header=None, names=['A', 'B', 'Res']) data.insert(0, 'x0', 1) cols = data.shape[1] X = data.iloc[:, :cols - 1] y = data.iloc[:, cols - 1:cols] X = np.mat(X.values) y = np.mat(y.values) positive = data[data['Res'].isin(['1'])] negative = data[data['Res'].isin(['0'])] theta = np.mat(np.zeros(3)).T # 学习率alpha为0.01时过大，导致CostFunction不收敛 alpha = 0.001 theta, cost = BatchGradientDecent(X, y, theta, alpha) print(theta) # print(cost) # theta = [[-25.16131863], [0.20623159], [0.20147149]] predictions = predict(theta, X) correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)] accuracy = (sum(map(int, correct)) % len(correct)) print('accuracy = &#123;0&#125;%'.format(accuracy)) fig, ax = plt.subplots(figsize=(12, 8)) ax.scatter(positive['A'], positive['B'], marker='o', label='P') ax.scatter(negative['A'], negative['B'], marker='x', label='N') # 显示图例 ax.legend() ax.set_xlabel('A') ax.set_ylabel('B') x = np.linspace(data.A.min(), data.A.max(), 100) y = -(theta[0, 0] + theta[1, 0] * x) / theta[2, 0] ax.plot(x, y, 'r') plt.show()]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息论与熵]]></title>
    <url>%2F2018%2F06%2F07%2FMachine%20Learning%2F%E4%BF%A1%E6%81%AF%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[信息论与熵香农信息量 只考虑连续型随机变量的情况。设p为随机变量X的概率分布，即p(x)为随机变量X在X=x处的概率密度函数值，随机变量X在x处的香农信息量定义为：$$ -logp(x) = log\frac{1}{p(x)}$$香农信息量用于刻画消除随机变量X在x处的不确定性所需的信息量的大小。可以近似地将不确定性视为信息量。即一个消息带来的不确定性大，就是带来的信息量大。 必定——信息量为0 高确定性——低信息量 高不确定性——高信息量 自信息用来衡量单一事件发生时所包含的信息量多寡。 互信息是点间互信息的期望值，是度量两个时间集合之间的相关性。两个离散随机变量X和Y的互信息可以定义为：$$I(X;Y)= \sum\limits_{y\in Y}\sum\limits_{x\in X}p(x,y)log\Big(\frac{p(x,y)}{p(x)p(y)}\Big)$$在连续随机变量的情形下，求和被替换成了二重定积分：$$I(X;Y) = \int_Y\int_Xp(x,y)log\Big(\frac{p(x,y)}{p(x)p(y)}\Big)dxdy$$其中$p(x,y)$是X和Y的联合概率分布函数，而$p(x)$和$p(y)$分别是X和Y的边缘概率分布函数。 熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策数据学习中的信息增益等价于训练数据集中类与特征的互信息。 熵/香农熵/信息熵熵是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。信息熵$H(p)$是香农信息量-log$p(x)$的数学期望，即所有X=x处的香农信息量的和，由于每一个x的出现概率不一样(用概率密度函数值$p(x)$衡量)，需要用$p(x)$加权求和。因此信息熵是用于刻画消除随机变量X的不确定性所需要的总体信息量的大小，其定义如下：$$ H(p)= H(X) = E_{x\to p(x)}[-logp(x)] = -\int{p(x)logp(x)dx}$$概率越大的时间，信息熵反而越小，哪些接近确定性的分布，香农熵比较低，而越是接近平均分布的，香农熵比较高。这个和发生概率越低的事情信息量越大的基本思想是一致的。从这个角度看，信息可以看做是不确定性的衡量，而信息熵就是对这种不确定性的数学描述。信息熵不仅定量衡量了信息的大小，并且为信息编码提供了理论上的最优值：使用的编码平均码长度的理论下界就是信息熵。或者说，信息熵就是数据压缩的极限。 微分熵当随机变量x是连续的，香农熵就被称为微分熵。 相对熵又称KL散度，信息散度，记为DKL(P||Q)。它度量当真实分布为p时，假设分布q的无效性。有人将KL散度称为KL距离，但事实上，KL散度并不满足距离的概念，因为：(1)KL散度不是对称的；(2)KL散度不满足三角不等式。设P(x)和Q(x)是X取值的两个离散概率分布，则P对Q的相对熵为：$$D(P||Q) = \sum{P(x)log(\frac{P(x)}{Q(x)})}$$对于连续的随机变量，定义为：$$D(P||Q) = \int{P(x)log(\frac{P(x)}{Q(x)})dx}$$ 交叉熵交叉熵主要用于度量两个分布间的差异性信息。语言模型的性能通常用交叉熵和复杂度来衡量。交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。对KL散度进行变形可以得到d：$$\begin{split}D(P||Q) &amp;= \sum{P(x)log(\frac{P(x)}{Q(x)})dx}\\&amp;= \sum{P(x)log(P(x))}-\sum{P(x)log(Q(x))}\\&amp;= -H(p(x))+[-\sum{P(x)log(Q(x))]}\end{split}$$交叉熵的公式定义如下：$$H(P,Q) = -\sum{P(x)log(Q(x))}$$由于KL散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>信息论</tag>
        <tag>熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归的正规方程]]></title>
    <url>%2F2018%2F06%2F06%2FMachine%20Learning%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[线性回归的正规方程 本文阐述线性回归的正规方程推导过程，为满足广义性，采用多变量的线性回归代价函数进行推导。 多变量线性回归的梯度下降算法是用来求其代价函数最小值的算法，但是对于某些线性回归问题，可以直接使用正规方程的方法来找出使得代价函数最小的参数，即$\frac{\partial}{\partial\theta_j}J(\theta)=0$。梯度下降与正规方程的比较： 优缺点 梯度下降 正规方程(标准方程) 是否需要引入其他参数 需要选择学习率$\alpha$ 不需要 迭代或运算次数 需要多次迭代 一次运算得出 特征数量是否有影响 当特征数量$n$大时也能较好适用 需要计算$(X^TX)^{-1}$如果特征数量$n$较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说$n$小于10000时还是可以接受的 适用模型类 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 首先给出线性回归的代价函数(Cost Function)的向量化表示：$$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$其中假设函数$h_\theta(x) = \theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$m$为样本总数，参数$\theta$与特征矩阵$X$均为$n+1$维列向量。 将假设函数代入，并将向量表达式转化为矩阵表达式，即将$\sum\limits_{i=1}^m$写成矩阵相乘的形式：$$J(\theta) = \frac{1}{2}(X\theta-y)^2$$其中$X$为$m$行$n+1$列的矩阵，$m$为样本个数，$n+1$为特征个数，$\theta$为$n+1$维行向量，$y$为$m$维行向量。由于$X$非方阵，不存在逆矩阵，固对$J(\theta)$进行如下变换： $$\begin{split}J(\theta) &amp; = \frac{1} {2} (X\theta-y)^T(X\theta-y) \\&amp;= \frac{1}{2}[(X\theta)^T-y^T] (X\theta-y) \\&amp;= \frac{1}{2}(\theta^TX^T-y^T) (X\theta-y) \\&amp;= \frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty)\end{split}$$ 接下来对$J(\theta)$求偏导，需要用到以下几个矩阵对矩阵的分母布局求导法则：①$\frac{dAX}{dX}=A^T$②$\frac{dX^TAX}{dX}=2AX$③$\frac{dX^TA}{dX}=A$ 首先化简$\frac{\partial}{\partial\theta}J(\theta)$$$\begin{split}\frac{\partial}{\partial\theta}J(\theta)&amp;=\frac{1}{2}[2X^TX\theta-X^Ty-(y^TX)^T+0]\\&amp;=\frac{1}{2}[2X^TX\theta-X^Ty-X^Ty+0]\\&amp;=X^TX\theta-X^Ty\end{split}$$ 再令$\frac{\partial}{\partial\theta}J(\theta)=X^TX\theta-X^Ty=0$$$\begin{split}X^TX\theta-X^Ty&amp;=0\X^TX\theta&amp;=X^Ty\end{split}$$ 不难发现，$(X^TX)$为方阵，则有$(X^TX)$的逆矩阵$(X^TX)^{-1}$，固在等式两边同时左乘$(X^TX)^{-1}$，并求出$\theta$$$\begin{split}(X^TX)^{-1}X^TX\theta&amp;=(X^TX)^{-1}X^Ty\\(X^TX)^{-1}(X^TX)\theta&amp;=(X^TX)^{-1}X^Ty\\E\theta&amp;=(X^TX)^{-1}X^Ty\\\theta&amp;=(X^TX)^{-1}X^Ty\end{split}$$至此，完成线性回归的正规方程推导，代码实现如下，其中数据来源于斯坦福Andrew Ng公开课：1234567891011121314151617181920212223242526272829303132import numpy as npimport pandas as pdimport matplotlib.pylab as pltdef NormalEquation(X,y):return ((X.T@X).I)@X.T@yif __name__ == '__main__':path = 'ex1data1.txt'data = pd.read_csv(path,header=None,names=['Population','Profit'])# insert the column of x0data.insert(0,'Ones',1)# get the matrix of X and the matrix of yx = data.iloc[:,:-1]y = data.iloc[:,-1:]X = np.mat(x.values)y = np.mat(y.values)theta = NormalEquation(X,y)x = np.linspace(data.Population.min(),data.Population.max(),100)f = theta[0,0] + theta[1,0] * x# display the resultfig,ax = plt.subplots(2,1,figsize=(8,12))ax[0].scatter(data.Population,data.Profit)ax[0].set_xlabel('Population')ax[0].set_ylabel('Profit')ax[0].set_title('Training Data')ax[1].plot(x,f,'r')ax[1].scatter(data.Population,data.Profit)ax[1].set_xlabel('Population')ax[1].set_ylabel('Profit')ax[1].set_title('Prediction Profit vs. Population Size')plt.show() 输出结果如下：]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>正规方程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归的梯度下降算法]]></title>
    <url>%2F2018%2F06%2F04%2FMachine%20Learning%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[线性回归的梯度下降算法 本文阐述线性回归代价函数的梯度下降算法推导过程，为满足广义性，采用多变量的线性回归代价函数进行推导。 首先给出线性回归的代价函数(Cost Function)：$$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$其中假设函数$h_\theta(x) = \theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$m$为样本总数，参数$\theta$与特征矩阵$X$均为$n+1$维列向量。由于使用多变量进行梯度下降，固可以使用批量梯度下降法来获得全局最优解。推导过程如下，首先将代价函数对$\theta_j$求偏导，得到$\frac{\partial}{\partial\theta_j}J(\theta)$：$$\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum\limits_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}$$在参数$\theta$中引入学习速率$\alpha$：$$\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta),(j=0,1,…,n)$$将$\frac{\partial}{\partial\theta_j}J(\theta)$代入，得出多变量线性回归的批量梯度下降算法：$$\theta_j=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m[(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}],(j=0,1,…,n)$$将上述公式进行向量化：$\Theta\in\mathbb{R}^{n+1\times{1}}$ $X\in\mathbb{R}^{m\times{n+1}}$ $y\in\mathbb{R}^{m\times{1}}$则批量梯度下降算法可表示为：$$\Theta=\Theta-\frac{\alpha}{m}X^{T}(X\Theta-y)$$向量化后，可以使计算更简洁，并且也能保证各$\theta$的值保持同步更新，以下是代码实现，其中数据来源于斯坦福Andrew Ng公开课：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport pandas as pdimport matplotlib.pylab as pltdef CostFunction(X, y, theta):return np.mean(np.power(X @ theta - y, 2)) / 2def Gradient(X, y, theta, alpha):return theta - alpha * (X.T @ (X @ theta - y)) / X.shape[0]def BatchGradientDecent(X, y, theta, alpha=0.01, iters=1000):cost = np.zeros(iters)for i in range(iters):theta = Gradient(X, y, theta, alpha)cost[i] = CostFunction(X, y, theta)return theta, costif __name__ == '__main__':path = 'ex1data1.txt'data = pd.read_csv(path,header=None,names=['Population','Profit'])# insert the column of x0data.insert(0,'Ones',1)# get the matrix of X and the matrix of yX = data.iloc[:,:-1]y = data.iloc[:,-1:]X = np.mat(X.values)y = np.mat(y.values)# init theta,alpha,iterstheta = np.mat([[0,0]])alpha = 0.01iters = 1000# batch gradientdecenttheta,cost = BatchGradientDecent(X,y,theta,alpha,iters)x = np.linspace(data.Population.min(),data.Population.max(),100)f = theta[0,0] + theta[0,1] * x# display the resultfig,ax = plt.subplots(2,1,figsize=(8,12))ax[0].scatter(data.Population,data.Profit)ax[0].set_xlabel('Population')ax[0].set_ylabel('Profit')ax[0].set_title('Training Data')ax[1].plot(x,f,'r')ax[1].scatter(data.Population,data.Profit)ax[1].set_xlabel('Population')ax[1].set_ylabel('Profit')ax[1].set_title('Prediction Profit vs. Population Size ')plt.show() 输出结果如下：]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[积分微分表]]></title>
    <url>%2F2018%2F06%2F02%2FUtils%2F%E7%A7%AF%E5%88%86%E5%BE%AE%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[积分微分表基本积分表24个基本积分：两个由基本积分②推导的常用积分 基本微分表 矩阵微分表①$\frac{dAX}{dX}=A^T$②$\frac{dX^TAX}{dX}=2AX$③$\frac{dX^TA}{dX}=A$]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Utils</tag>
        <tag>微积分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法]]></title>
    <url>%2F2018%2F06%2F02%2FMachine%20Learning%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度下降(Gradient Descent)算法梯度下降是一个用来求函数最小值的算法，是迭代法的一种，可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法(Stochastic Gradient Descent，简称SGD)和批量梯度下降法(Batch Gradient Descent，简称BGD)。随机梯度下降：随机梯度下降是每次迭代使用一个样本来对参数进行更新，其计算速度较快，但由于计算得到的并不是准确的一个梯度，即准确度较低，且容易陷入到局部最优解中，也不易于并行实现。批量梯度下降：批量梯度下降是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新(这里的更新指同步更新)。相对的，批量梯度下降在样本数据较多的情况下，其计算速度较慢，但是可以获得全局最优解，且易于并行实现。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[点到超平面距离]]></title>
    <url>%2F2018%2F05%2F11%2FStatistical%20Learning%2F%E7%82%B9%E5%88%B0%E8%B6%85%E5%B9%B3%E9%9D%A2%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[点到超平面距离首先给出以下定义：输入空间(特征空间) $X\subseteq\mathbb{R}^n$输出空间 $Y= \lbrace +1,-1 \rbrace $特征向量 $x\in{X}$类别 $y\in{Y}$权值(权值向量) $w\in\mathbb{R}^n$偏置 $b\in\mathbb{R}$给出线性方程：$$w\cdot{x}+b=0$$此线性方程对应于特征空间$\mathbb{R}^n$中的一个超平面S，其中$w$是超平面的法向量，$b$是超平面的截距，$w\cdot{x}$表示$w$和$x$的内积。点到超平面距离推导过程如下。 首先给出如下例图：其中，某个超平面S$$w\cdot{x}+b=0$$$w$是超平面的法向量，方向垂直超平面向上。A为平面外一点且坐标为$x_0$，B为超平面S内一点且坐标为$x_1$，$|\vec{AO}|=d$为点A到超平面垂直距离。根据勾股定理，易推得：$$d=|\vec{AB}|cos\angle{OAB}$$根据向量内积进行转化：$$\begin{split}d&amp;=|\vec{AB}|cos\angle{OAB}\\&amp;=||\vec{AB}||\frac{\vec{AB}\cdot\vec{AO}}{|\vec{AB}||\vec{AO}|}\\&amp;=\frac{\vec{AB}\cdot\vec{AO}}{|\vec{AO}|}\end{split}$$由于$|\vec{AO}|$为点A到超平面垂直距离，易证$\vec{AO}//w$，则有：$$\frac{\vec{AO}}{|\vec{AO}|}=-\frac{w}{|w|}$$且$\vec{AB}=x_1-x_0$，可得：$$\begin{split}d&amp;=-\frac{w\cdot{(x_1-x_0)}}{|w|}\\&amp;=\frac{w\cdot{x_0}-w\cdot{x_1}}{|w|}\end{split}$$此时利用条件“B为超平面S内的一点且坐标为$x_1$”，故有$w\cdot{x_1}+b=0$：此时完成距离$d$的推导：$$d=\frac{w\cdot{x_0}+b}{|w|}$$]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>超平面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归问题]]></title>
    <url>%2F2018%2F05%2F10%2FStatistical%20Learning%2F%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[回归问题回归(regression)是监督学习的另一个重要问题。回归用于预测输入变量(自变量)和输出变量(因变量)之间的关系，特别是当输入变量的值发生变化时，输出变量的值岁=随之发生的变化。回归模型正是表示从输入变量到输出变量之间映射的函数。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。回归问题分为学习和预测两个过程，如下图。首先给定一个训练数据集：$$T=\lbrace(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\rbrace$$这里，$x_i\in{R^n}$是输入，$y\in{R}$是对应的输出，$i=1,2,\cdots,N$。学习系统基于训练数据构建一个模型，即函数$Y=f(X)$；对新的输入$x_{N+1}$，预测系统根据学习的模型$Y=f(X)$确定相应的输出$y_{N+1}$。回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型及模型的类型，分为线性回归和非线性回归。回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法(least squares)求解。许多领域的任务都可以形式化为回归问题，比如，回归可以用于商务领域，作为市场趋势预测、产品质量管理、客户满意度调查、投资风险分析的工具。作为例子，简单介绍股价预测问题。假设知道某一公司在过去不同时间点(比如，每天)的市场上的股票价格(比如，股票平均价格)，以及在各个时间点之前可能影响该公司股价的信息(比如，该公司前一周的营业额、利润)。目标是从过去的数据学习一个模型，使它可以基于当前的信息预测该公司下一个时间点的股票价格。可以将这个问题作为回归问题解决。具体地，将影响股价的信息视为自变量(输入的特征)，而将股价视为因变量(输出的值)。将过去的数据作为训练数据，就可以学习一个回归模型，并对未来的股价进行预测。可以看出这是一个困难的预测问题，因为影响股价的因素非常多，我们未必能判断到哪些信息(输入的特征)有用并能得到这些信息。]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>回归问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[标注问题]]></title>
    <url>%2F2018%2F05%2F10%2FStatistical%20Learning%2F%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[标注问题标注(tagging)也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题有时更负责的结构预测(structure prediction)问题的简单形式。标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级别增长的。标注问题分为学习和标注两个过程(如下图所示)。首先给定一个训练数据集：$$T=\lbrace(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\rbrace$$这里，$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,i=1,2,\cdots,N$，是输入观测序列，$y_i=(y_i^{(1)},y_i^{(2)},\cdots,y_i^{(n)})^T,i=1,2,\cdots,N$，是响应的输出标记序列，$n$是序列的长度，对不同样本可以有不同的值。学习系统基于训练数据集构建一个模型，表示为条件概率分布：$$P(Y^{(1)},Y^{(2)},\cdots,Y^{(n)}|X^{(1)},X^{(2)},\cdots,X^{(n)})$$这里，每一个$X^{(i)}(i=1,2,\cdots,n)$取值为所有可能的观测，每一个$Y^{(i)}(i=1,2,\cdots,n)$取值为所有坑你的标记，一般$n&lt;&lt;N$。标注系统按照学习得到的条件概率分布模型，对新的输入观测序列找到相应的输出标记序列。具体地，对一个观测序列$x_{N+1}=(x_{N+1}^{(1)},x_{N+1}^{(2)},\cdots,x_{N+1}^{(n)})^T$找到使条件概率$P((y_{N+1}^{(1)},y_{N+1}^{(2)},\cdots,y_{N+1}^{(n)})^T|(x_{N+1}^{(1)},x_{N+1}^{(2)},\cdots,x_{N+1}^{(n)})^T)$最大标记序列$y_{N+1}=(y_{N+1}^{(1)},y_{N+1}^{(2)},\cdots,y_{N+1}^{(n)})^T$。评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、青雀率和召回率。其定义与分类模型相同。标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。标注问题在信息抽取、自然语言处理等领域被广泛应用，是这些领域的基本问题。例如，自然语言处理中的词性标注(part of speech tagging)就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标标记序列。举一个信息抽取的例子。从英文文章中抽取基本名词短语(base noun phrase)。为此，要对文章进行标注。英文单词是一个观测，英文句子是一个观测序列，标记表示名词短语的“开始”、“结束”或“其他”(分别以B，E，O表示)，标记序列表示英文句子中基本名词短语的所在位置。信息抽取时，将标记“开始”到标记“结束”的单词作为名词短语。列入，给出以下的观测序列，即英文句子，标注系统产生相应的标记序列，即给出句子中的基本名词短语。输入：At Microsoft Research, we have an insatiable curiosity and the desire to create new technology that will help define the computing experience.输出：At/O Microsoft/B Research/E, we/O have/O an/O insatiable/B curiosity/E and/O the/O desire/BE to/O create/O new/B technology/E that/O will/O help/O define/O the/O computing/B experience/E.]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>标注问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类问题]]></title>
    <url>%2F2018%2F05%2F08%2FStatistical%20Learning%2F%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[分类问题分类是监督学习的一个核心问题。在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便成为分类问题。这时，输入变量$X$可以使离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，成为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification)。可能的输出称为类(class)。分类的类别为多个时，称为多类分类问题。分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。分类问题可用下图描述。学习系统由训练数据$(x^{(i)},y^{(i)})$学习一个分类器$P(Y|X)$或$Y=f(X)$；分类系统通过学到的分类器$P(Y|X)$或$Y=f(X)$对于新的输入实例$x_{N+1}$进行分类，即预测其输出的类标记$y_{N+1}$。评价分类器性能的指标一般是分类准确率(accuracy)，其定义是：对于给定的测试数据集，分类器正确份额类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率：$$r_{rest}=\frac{1}{N’}\sum\limits_{i=1}^{N’}I(y_i=\hat{f}(x_i)$$误差率(error rate)为：$$e_{rest}=\frac{1}{N’}\sum\limits_{i=1}^{N’}I(y_i\neq\hat{f}(x_i)$$则显然有：$$r_{rest}+e_{test}=1$$对于二类分类问题常用的评价指标是精确率(precision,查准率)与召回率(recall,查全率)。通常以关注的类(一般为出现较少的类)为正类(y=1)，其他为负类(y=0)，分类器在测试数据集熵的预测或正确或不正确，4种情况出现的总数分别记作： TP——将正类预测为正类数 FN——将正类预测为负类数 FP——将负类预测为正类数 TN——将负类预测为负类数精确率(查准率)定义为：$$P=\frac{TP}{TP+FP}$$召回率(查全率)定义为：$$P=\frac{TP}{TP+FN}$$ 此外，还有$F_1$值，是精确率与召回率的调和均值，即$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$$$F_1=\frac{2TP}{2TP+FP+FN}$$精确率和召回率都高时，$F_1$值也会高。许多统计学习方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。分类在于根据其特性将数据“分门别类”，所以在许多领域都有广泛的应用。例如在银行业务中，可以构建一个客户分类模型，对客户按照贷款风险的大小进行分类；在网络安全区域，可以利用日志数据的分类对非法入侵进行检测；在图像处理中，分类可以用来检测图像中是否有人脸出现；在手写识别中，分类可以用于识别手写的数字；在互联网搜索中，网页的分类可以帮助网页的抓取、索引与排序。举一个分类应用的例子——文本分类(test classification) 。这里的文本可以是新闻报道、网页、电子邮件、学术论文等。类别往往是关于文本内容的，例如政治、经济、体育等；也有关于文本特点的，如正面意见、反面意见；还可以根据应用确定，如垃圾邮件、非垃圾邮件等。文本分类是根据文本的特征将其划分到已有的类中。输入是文本的特征向量，输出是文本的类别。通常把文本中的单词定义为特征，每个单词对应一个特征。单词的特征可以是二值的，如果单词在文本中出现则取值是1，否则是0；也可以是多值的，表示单词在文本中出现的频率。直观地，如果“股票”“银行”“货币”这些词出现很多，这个文本可能属于经济类，如果“网球”“比赛”“运动员”这些词频繁出现，这个文本可能属于体育类。]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成模型与判别模型]]></title>
    <url>%2F2018%2F05%2F06%2FStatistical%20Learning%2F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[生成模型与判别模型监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：$$Y=f(X)$$或者条件概率分布：$$P(Y|X)$$监督学习方又可以分为生成方法(generative approach)和判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative approach)。生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$这样的方法之所以称之为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。典型的生成模型有：朴素贝叶斯法(Naive bayes,NB)和隐马尔可夫模型(Hidden Markov Model,HMM)。判别方法由数据直接学习策略函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：k近邻法(K-Nearest Neighbours Algorithm,KNN)、感知机(Perceptron)、决策树(Decision Tree,DT)、逻辑斯谛回归模型(Logistic Regression,LR)、最大熵模型(Maximum Entropy,ME)、支持向量机(Support Vector Machine,SVM)、提升方法(Boosting)、条件随机场(Conditional Random Fields,CRF)等。在监督学习中，生成方法和判别方法各有优缺点，适合于不同条件下的学习问题。 生成方法的特点生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。 判别方法的特点判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高；由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>生成模型与判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2018%2F05%2F05%2FStatistical%20Learning%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化模型选择的典型方法是正则化(regularization)。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。正则化一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化可以是模型参数向量的范数。正则化一般具有如下形式：$$\min\limits_{f\in{F}}\frac{1}{N}\sum\limits_{i=1}^{N}L(y_i,f(x_i))+\lambda{J(f)}$$其中，第一项$L(y_i,f(x_i))$是经验风险，第二项$J(f)$是正则化项，$\lambda\geq0$为调整两者之间关系的系数。正则化项可以取不通的形式。例如，回归问题中，代价函数是平方损失，正则化项可以是参数向量的$L_2$范数：$$L(w)=\frac{1}{N}\sum\limits_{i=1}^{N}(f(x_i;w)-y_i)^2+\frac{\lambda}{2}||w||^2$$这里，$||w||$表示参数向量$w$的$L_2$范数。正则化项也可以使参数向量的$L_1$范数：$$L(w)=\frac{1}{N}\sum\limits_{i=1}^{N}(f(x_i;w)-y_i)^2+\lambda||w||_1$$这里，$||w||_1$表示参数向量$w$的$L_1$范数。第一项的风险较小的模型可能较复杂(有多个非零参数)，这时第二项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。正则化符合奥尔卡姆剃刀(Occams’s razor)原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，正则化项是对应于模型的先验概率。可以假设模型有较小的先验概率，简单的模型有较大的先验概率。设置惩罚项，即使用正则化，可以使假设函数更简单，且不易发生过拟合问题。在线性回归中，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta$值减少了一个额外的值，即减小$\theta$的平方范数。 先验概率先验概率(prior probability)是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。在贝叶斯统计推断中，不确定数量的先验概率分布是在考虑一些因素之前表达对这一数量的置信程度的概率分布。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。 ####奥尔卡姆剃刀原理奥卡姆剃刀定律（Occam’s Razor, Ockham’s Razor）又称“奥康的剃刀”，它是由14世纪英格兰的逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“简单有效原理”。正如他在《箴言书注》2卷15题说“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。”]]></content>
      <categories>
        <category>Statistical Learning</category>
      </categories>
      <tags>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欠拟合与过拟合]]></title>
    <url>%2F2018%2F05%2F05%2FMachine%20Learning%2F%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%2F</url>
    <content type="text"><![CDATA[欠拟合与过拟合欠拟合模型在训练集上学习的不够好，经验误差大，称为欠拟合。模型训练完成后，用训练数据进行测试，如果错误率高，我们就很容易发现模型还是欠拟合的。 过拟合当模型对训练集学习得太好的时候（学习数据集通性的时候，也学习了数据集上的特性，导致模型在新数据集上表现差，也就是泛化能力差），此时表现为经验误差很小，但泛化误差很大，这种情况称为过拟合。 发生欠拟合的主要原因(高偏差) 训练次数过少 根本的原因是特征维度过少，导致拟合的函数无法满足训练集，误差较大。 由此对应的降低欠拟合(解决高偏差)的方法有： 增加训练次数。 添加其他特征项，例如，组合特征、泛化特征、相关性特征。 添加多项式特征，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。 减少正则化参数，正则化的目的是防止过拟合。发生过拟合的主要原因(高方差) 使用过于复杂的模型(涉及到核函数)； 数据噪声较大； 训练数据少。 由此对应的降低过拟合(解决高方差)的方法有： 1.增加正则化程度,减少特征的数量——简化模型假设，或使用惩罚项限制模型复杂度； 2.进行数据清洗——减少噪声； 3.获得更多的训练样本——收集更多训练数据。 选择合适的核函数以及软边缘参数C就是训练SVM的重要因素。一般来讲，核函数越复杂，模型越偏向于过拟合；C越大模型越偏向于过拟合，反之则拟合不足。 具体模型对应解决欠拟合方法 正则化 正则化方法包括L0正则、L1正则和L2正则。L0范数是指向量中非0的元素的个数。L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。两者都可以实现稀疏性。L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果。 剪枝 剪枝是决策树中一种控制过拟合的方法，预剪枝通过在训练过程中控制树深、叶子节点数、叶子节点中样本的个数等来控制树的复杂度。后剪枝则是在训练好树模型之后，采用交叉验证的方式进行剪枝以找到最优的树模型。提前终止迭代主要是用在神经网络中的，在神经网络的训练过程中我们会初始化一组较小的权值参数，此时模型的拟合能力较弱，通过迭代训练来提高模型的拟合能力，随着迭代次数的增大，部分的权值也会不断的增大。如果我们提前终止迭代可以有效的控制权值参数的大小，从而降低模型的复杂度。上面的几种方法都是操作在一个模型上 ，通过改变模型的复杂度来控制过拟合。另一种可行的方法是结合多种模型来控制过拟合。 Bagging和Boosting 是机器学习中的集成方法，多个模型的组合可以弱化每个模型中的异常点的影响，保留模型之间的通性，弱化单个模型的特性。 Dropout 是深度学习中最常用的控制过拟合的方法，主要用在全连接层处。在一定的概率上（通常设置为0.5，原因是此时随机生成的网络结构最多）隐式的去除网络中的神经元，但会导致网络的训练速度慢2、3倍，而且数据小的时候，Dropout的效果并不会太好。因此只会在大型网络上使用。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>欠拟合与过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——K均值算法(K-means)]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95(K-means)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——K均值算法(K-means)无监督学习 无监督学习，即从无标签的数据开始学习 包含算法 聚类 K均值聚类(K-means) 降维 PCA K-means原理 聚类效果图如下 优点：采用迭代式算法，直观易懂且非常实用 缺点：容易收敛到局部最优解(多次聚类) 聚类一般做在分类之前 K-means聚类步骤 1.随机设置K个特征空间内的点作为初始的聚类中心 2.对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 3.接着对标记的聚类中心之后，重新计算出每个聚类的新中心点(平均值) 4.如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程 具体步骤如下图 K-meansAPI sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’) k-means聚类 n_clusters：开始的聚类中心数量 init：初始化方法，默认为’k-means++’ labels_：默认标记的类型，可以和真实值比较(不是值比较) 1234cust = datakm = KMeans(n_clusters=4)km.fit(cust)pre = km.predict(cust) K-means性能评估指标轮廓系数$$SC_i=\frac{b_i-a_i}{max(b_i,a_i)}$$对于每个点$i$为已聚类数据中的样本，$b_i$为$i$到其他簇的所有样本的距离最小值，$a_i$为$i$到本身簇的距离平均值。最终计算出所有样本点的轮廓系数平均值 轮廓系数数值分析 分析过程(以蓝点为例) 1.计算出蓝1离本身簇所有点的距离平均值$a_i$ 2.蓝1到其他两个簇的距离计算出平均值红平均，绿平均，取最小的那个距离作为$b_i$ 根据公式进行极端值考虑，如果$b_i&gt;&gt;a_i$，那个公式结果趋近于1；如果$a_i&gt;&gt;b_i$，那么公式结果趋近于-1。若$b_i&gt;&gt;a_i$，即公式结果趋近于1，则效果越好；若$a_i&gt;&gt;b_i$，即公式结果趋近于-1，则效果越差。轮廓系数的值时介于[-1,1]，越趋近于1代表内聚度和分离度相对较优 轮廓系数API sklearn.metrics.sihouette_score(X, labels) 计算所有样本的平均轮廓系数 X：特征值 labels：被聚类标记的目标值 1silhourtte_score(cust,pre)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——模型保存和加载]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——模型保存和加载sklearn模型的保存和加载API from sklearn.externals import joblib 保存：joblib.dump(estimator, ‘test.pkl’) 加载：estimator = joblib.load(‘test.pkl’) 线性回归的模型保存加载案例 保存 1234567# 使用线性模型进行预测# 使用正规方程求解estimator = LinearRegression()# 训练模型estimator.fit(x_train, y_train)# 保存模型joblib.dump(estimator,'test.pkl') 加载 12model = joblib.load('test.pkl')y_predict = model.predict(x_test)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——分类的评估方法]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB%E7%9A%84%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——分类的评估方法精确率与召回率混淆矩阵 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类) 精确率(Precision)与召回率(Recall) 精确率(查准率)预测结果为正例样本中真实为正例的比例 $$P=\frac{真正例}{预测为真}=\frac{TP}{TP+FP}$$ 召回率(查全率)：真实为正例的样本中预测结果为正例的比例 $$R=\frac{真正例}{实际为真}=\frac{TP}{TP+FN}$$ F1值 F1-score，反映模型的稳健性，F1值与精确率(P)、召回率(R)之间的关系为 $$\begin{split}\frac{2}{F1}&amp;=&amp;\frac{1}{P}+\frac{1}{R}\\F1&amp;=&amp;\frac{2TP}{2TP+FN+FP}\end{split}$$ 分类评估报告API sklearn.metrics.classification_report(y_true, y_pred, labels=[]. target_names=None) y_true：真实目标值 y_pred：估计器预测目标值 labels：指定类别对应的数字 target_names：目标类别名称 return：每个类别精确率与召回率 ROC曲线与AUC指标真正类率TPR与假正类率FPR 真正类率(true positive rate ,TPR) $$TPR=\frac{TP}{TP+FN}$$ 假正类率(false positive rate, FPR) $$FPR=\frac{FP}{TP+FN}$$ ROC曲线 受试者工作特征曲线(receiver operating characteristic curve，简称ROC曲线)，又称为感受性曲线(sensitivity curve)，ROC曲线的横轴就是FP-Rate，纵轴就是TP-Rate，当二者相等时，表示的意义为：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5 AUC指标 ROC曲线下面积(area under curve，简称AUC指标) AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率 AUC的最小值为0.5，最大值为1，取值越高越好 AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5&lt;AUC&lt;1，优于随机猜测。这个分类器(模型)妥善设定阈值的话，能有预测价值。 AUC只能用来评价二分类 AUC非常适合评价样本不平衡中的分类器性能 最终AUC的范围在[0.5, 1]之间，并且越接近1越好 AUC计算API- sklearn.metrics.roc_auc_score(y_true, y_score) - 计算ROC曲线面积，即AUC值 - y_true：每个样本的真实类别，必须为0(反例)，1(正例)标记 - y_score：预测得分，可以使正类的估计概率、置信值或者分类器方法的返回值]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——逻辑回归(LR)]]></title>
    <url>%2F2018%2F03%2F23%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(LR)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——逻辑回归(LR)逻辑回归的原理逻辑回归(Logistic Regression)是机器学习中的一种分类模型，逻辑回归是一种分类算法。常用于解决解决二分类问题。 输入$$h_{\theta}(x)=\theta_0x_0+\theta_1x_1+theta_2x_2+theta_3x_3+\cdots = \Theta^TX$$ 激活函数 sigmoid函数$$g(z) = \frac{1}{1+e^z}$$其中$z=\Theta^TX$，且输出结果在[0,1]中的一个概率值，默认0.5为阈值(即当输入$z=0$时)12345678910111213141516# sigmoid函数展示import matplotlib.pylab as pltimport numpy as npdef sigmoid(z): return 1/(1+np.exp(-z))x = np.linspace(-10,10,100)y = sigmoid(x)plt.plot(x,y)ax = plt.gca()ax.spines['right'].set_color('none') ax.spines['top'].set_color('none') # 将右边 上边的两条边颜色设置为空 其实就相当于抹掉这两条边ax.xaxis.set_ticks_position('bottom') ax.yaxis.set_ticks_position('left') # 指定下边的边作为 x 轴 指定左边的边为 y 轴ax.spines['bottom'].set_position(('data', 0)) #指定 data 设置的bottom(也就是指定的x轴)绑定到y轴的0这个点上ax.spines['left'].set_position(('data', 0))plt.show() 输入如下sigmoid函数：由图像可知：$$g(z) \begin{cases}&amp;&gt;0.5&amp; z&gt;0\\&amp;=0.5&amp; z=0\\&amp;&lt;0.5&amp; z&lt;0\end{cases}$$ 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例)，另外的一个类别会标记为0(反例)。 逻辑回归中，我们预测$$y = \begin{cases}&amp;1&amp; h_{\theta}(x)\geq0.5\\&amp;0&amp;h_{\theta}(x)&lt;0.5\end{cases}$$又因为$z=\Theta^TX$，可以得到$$y = \begin{cases}&amp;1&amp; \Theta^TX\geq0\\&amp;0&amp; \Theta^TX&lt;0\end{cases}$$ 损失函数逻辑回归的损失，称为对数似然损失，公式如下$$cost(h_{\theta}(x))=\begin{cases}&amp;-log(h_{\theta}(x))&amp;if&amp;y = 1 \\&amp;-log(1-h_{\theta}(x))&amp;if&amp;y = 0\end{cases}$$ 可以化简为$$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)})$$ 优化损失函数同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。 逻辑回归API sklearn.linear_model.LogisticRegression(solver=’liblinear’,penalty=’l2’,C=1.0) solver：优化求解方式(默认通过开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数) sag：根据数据集自动选择，随机平均梯度下降 penalty：正则化的种类 C：正则化力度 默认将类别数量少的当做正例 LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——波士顿房价预测]]></title>
    <url>%2F2018%2F03%2F23%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[项目实战——波士顿房价预测数据介绍 字段名 描述 CRIM 城镇人均犯罪率 ZN 占地面积超过2.5万平方英尺的住宅用地比例 INDUS 城镇非零售业务地区的比例 CHAS 查尔斯河虚拟变量(=1 如果土地在河边;否则是0) NOX 一氧化氮浓度(每1000万份) RM 平均每居民房数 AGE 在1940年之前建成的所有者占用单位的比例 DIS 与五个波士顿就业中心的加权距离 RAD 辐射状公路的可达性指数 TAX 每10000美元的全额物业税率 PTRATIO 城镇师生比例 B 1000(Bk - 0.63)^2 LASTAT 人口中低位较低人群的百分数 MEDV 以1000美元计算的自有住房的中位数(预测值) 需求分析回归中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。 数据分割与标准化 回归预测 线性回归的算法效果评估 线性回归使用正规方程优化线性回归123456789101112131415161718192021222324252627import pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error# 1.获取数据集boston = load_boston()print('boston:\n',boston.DESCR)# 2.划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.traget, random_state=8)# 3.特征工程# 3.1.实例化转化器transfer = StandardScaler()# 3.2.转化数据x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.模型训练# 4.1.实例化估计器estimator = LinearRegression()# 4.2.传入训练数据 进行模型训练estimator.fit(x_train, y_train)# 4.3.模型结果print('正规方程优化的模型结果系数为:\n',estimator.coef_)print('正规方程优化的模型结果偏置为:\n',estimator.intercept_)# 5.模型评估y_predict = estimator.predict(x_test)error = mean_squared_error(y_test, y_predict)print('正规方程优化的结果均方误差为:\n',error) 输出：12345678梯度下降优化的模型结果系数为: [-0.74192506 0.73579142 -0.32384815 0.7751081 -0.75627116 3.20090862 -0.28381128 -2.00606851 0.62594045 -0.42608003 -1.79599478 0.86938128 -3.65789337]梯度下降优化的模型结果偏置为: [22.03114]梯度下降优化的结果均方误差为: 24.267303702468947 使用梯度下降优化线性回归123456789101112131415161718192021# 1.获取数据boston = load_boston()# 2.分割数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=8)# 3.特征工程：标准化# 3.1.实例化转化器transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.模型训练# 4.1.实例化估计器estimator = SGDRegressor()# 4.2.传入训练数据 进行模型训练estimator.fit(x_train, y_train)# 4.3.模型结果print('梯度下降优化的模型结果系数为:\n',estimator.coef_)print('梯度下降优化的模型结果偏置为:\n',estimator.intercept_)# 5.模型评估y_predict = estimator.predict(x_test)error = mean_squared_error(y_test, y_predict)print('梯度嘉奖优化的结果均方误差为:\n',error) 输出：12345678梯度下降优化的模型结果系数为: [-0.74999984 0.53962994 -0.3141274 0.79470893 -0.6952651 3.16754672 -0.13407217 -1.75681033 0.76326425 -0.43241248 -1.81617333 0.85367247 -3.72958928]梯度下降优化的模型结果偏置为: [22.03435752]梯度下降优化的结果均方误差为: 24.164326494635407 岭回归123456789101112131415161718192021# 1.获取数据集boston = load_boston()# 2.划分数据集x_train,x_test,y_train,y_test = train_test_split(boston.data, boston.target,random_state=8)# 3.特征工程：标准化# 3.1.实例化转化器transfer = StandardScaler()# 3.2.传入数据 进行标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.实例化估计器estimator = Ridge()estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print('预测值为:\n',y_predict)# 5.得出模型print('回归系数:\n',estimator.coef_)print('偏置:\n',estimator.intercept_)# 6.模型评估——均方误差error = mean_squared_error(y_test,y_predict)print('均方误差:\n',error) 输出：12345678910111213141516171819202122232425262728293031预测值为: [19.29249509 10.9044467 38.6033501 26.96819247 41.18226428 27.45946881 10.19715176 36.31848668 29.09661004 35.05929037 13.21177162 7.38752095 15.49082238 24.31068901 16.14135614 29.0269088 23.08671884 22.41480902 20.72699958 6.97252278 20.85834773 25.35109948 30.46228136 33.31940905 28.52422234 35.42941678 4.88670106 14.52403709 25.57897217 23.47617658 34.5486346 18.50767426 19.57369094 22.86517531 25.57180901 27.12713609 32.33832511 25.71132131 14.31961378 14.02131841 21.59103876 21.98840868 36.11575743 35.07330795 23.34357369 19.4875536 19.99391045 21.12832977 25.44572146 24.25852502 19.99810516 16.53912451 31.93307255 16.29585862 22.33019461 24.59180264 20.81018269 4.97997805 20.02669797 28.45661718 24.31984154 21.82956466 20.08062459 9.06176246 14.44128436 17.45246292 12.75087695 20.26358367 35.21892937 35.90539814 13.72890447 28.87158618 30.52360529 31.87582711 21.91264297 28.48043189 29.38052967 17.15354022 13.75751895 18.40478712 13.92300614 5.7029655 16.19939976 22.84322293 20.63795823 31.04957851 14.79821092 15.86515596 26.71807603 31.5844528 21.00528122 21.07860099 29.34781794 35.60194418 19.07216227 28.1273874 23.21044017 24.81655864 23.1514886 13.49429145 2.54518224 27.68752227 0.29009813 41.18846989 32.71620063 28.95862556 16.88687507 22.80113434 25.33344261 31.18299913 18.48851751 20.24384335 39.95151299 12.68377544 20.0579597 20.57578142 39.810066 15.81067337 25.53594481 21.70233939 28.65676966 28.58793363 14.92303049 25.0807655 25.38638199 15.52952136 24.05308496]回归系数: [-0.97246454 1.14327275 0.15848304 0.65305661 -1.45541569 2.68212945 -0.17139627 -2.97390427 2.22587256 -1.76604839 -1.91302371 0.8558563 -4.03757414]偏置: 22.52163588390508均方误差: 22.69682807165292]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Linear Regression</tag>
        <tag>Ridge Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——岭回归(RR)]]></title>
    <url>%2F2018%2F03%2F22%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%B2%AD%E5%9B%9E%E5%BD%92(RR)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——岭回归(RR)岭回归的定义岭回归，即在线性回归的基础上加上了L2正则化的限制，从而达到降低过拟合的效果。正则化时，一般不对$\theta_0$进行惩罚，正则化的本质即是减少$\theta_j$的平方范数，从而降低模型复杂度。 岭回归API sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver=’auto’,normalizer=False) 具有L2正则化的线性回归 alpha：正则化参数，$\lambda$ $\lambda$取值：0~1 1~10 solver：会根据数据自动选择优化方法 sag：如果数据集、特征都比较大，选择该随机梯度下降优化 normalize：数据是否进行标准化 normalize=False：可以在fit之前调用preprocessing.StandardScaler标准化数据 Ridge.coef_：回归权重 Ridge.intercept_：回归偏置 Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG) SAG()Stochastic average gradient)，即随机平均梯度下降，在SGD中，由于收敛的速度太慢，所以后面就有人提出SAG基于梯度下降的算法。SAG中的S是随机（Stochastic），A是平均（average），G是梯度（gradient）的意思。可以看到SAG是一种加速版本的SGD。直观上看，利用的信息量大了，收敛速度就应该比单纯用一个样本估计梯度值的SGD要快。在一定条件下SAG线性收敛，与全梯度下降（FGD）收敛速度一样。但是SAG带来的问题就是需要内存来维护（保存）每一个旧梯度值。即用空间换时间。 sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin) 具有L2正则化的线性回归，可以进行交叉验证 coef_：回归系数 12345class _BaseRidgeCV(LinearModel): def __init__(self, alpha=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False,scoring=None, cv=None, gcv_mode=None, store_cv_values=False):]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Ridge Regression</tag>
        <tag>SAG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——线性回归(LR)]]></title>
    <url>%2F2018%2F03%2F22%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(LR)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——线性回归(LR)线性回归的原理目标值是连续变量同时目标值(因变量)与特征值(自变量)呈线性关系 线性回归的定义“回归”即拟合。线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。 只有一个自变量的情况称为单变量回归，多个自变量的情况称为多变量回归 线性回归的公式可以表示为$$h_{\theta}(x)=\theta_0x_0+\theta_1x_1+theta_2x_2+theta_3x_3+\cdots = \Theta^TX$$其中$\Theta = [\theta_0\ \theta_1\ \theta_2\cdots]^T\ \ X=[x_0\ x_1\ x_2\cdots]^T$，$x_0=1$，即$\Theta$与$X$均为列向量，且维度相同。 单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系，多特征即在更高维度进行扩展。 损失函数使用最小二乘法定义损失函数，用来衡量预测值与期望值之间的误差损失函数的公式可以表示为$$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$$ 优化算法线性回归中常用的优化算法 正规方程$$\Theta = (X^TX)^{-1}X^Ty$$其中，$X$为特征矩阵，y为目标值矩阵 优点：直接求解，且误差较小 缺点：当特征过多且过于复杂(维度&gt;10000)时，求解速度太慢且得不到结果 梯度下降$$\theta_1 := \theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta)$$$$\theta_0 := \theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta)$$其中，$\alpha$为学习速率，需要手动指定(超参数)，偏导部分表示梯度下降方向，直到得到最小的$J(\theta)$ 优点：面对大量特征也你能求得结果，且能够找到较好的结果 缺点：速度相对较慢，需要多次迭代，且需要调整学习率$\alpha$，当学习率调整不当时，可能会出现函数不收敛的情况 线性回归API正规方程线性回归((Normal Equation)) sklearn.linear_model.LinearRegression(fit_intercept=True) 通过正规方程优化 fit_intercept：是否计算偏置 LinearRegression.coef_：回归系数 LinearRegression.intercept_：偏置 随机梯度下降线性回归(Stochastic Gradient Descent,SGD) sklearn.linear_model.SGDRegressor(loss=’squared_loss’,fit_intercept=True,learning_rate=’invscaling’,eta0=0.01) SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型 loss：损失类型 loss=’squared_loss’：普通最小二乘法 fit_intercept：是否计算偏置 learning_rate：string，optional 学习率填充 ‘constant’：eta = eta0 ‘optimal’：eta = 1.0 / (alpha * (t + t0)) [default] ‘invscaling’：eta = eta0 / pow(t, power_t) power_t = 0.25：存在父类当中 对于一个常数值的学习率来说，可以使用learning_rate=’constant’，并使用eta0来指定学习率 SGDRegressor.coef_：回归系数 SGDRegressor.intercept_：偏置]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Linear Regression</tag>
        <tag>Normal Equation</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——泰坦尼克号乘客生存预测]]></title>
    <url>%2F2018%2F03%2F21%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[项目实战——泰坦尼克号乘客生存预测使用决策树12345678910111213141516171819202122232425262728293031323334import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier# 获取数据tanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')# 数据预处理# 缺失值处理tanic['age'].fillna(tanic['age'].mean(),inplace=True)# 提取特征值与目标值# 注意x的tanic内部的columns为listx = tanic[['pclass','age','sex']]y = tanic['survived']# 特征工程 字典特征提取# 实例化转化器transfer = DictVectorizer(sparse=False)# x.to_dict(orient='records') 按行转字典x = transfer.fit_transform(x.to_dict(orient='records'))# 数据集划分x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)# 模型训练与评估# 实例化估计器estimator = DecisionTreeClassifier()estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测值为',res[res==True].size/res.size)# 方法2 计算模型准确值print('模型的准确值为:\n',estimator.score(x_test,y_test)) 输出1230.7918781725888325模型准确值为: 0.7918781725888325 使用随机森林1234567891011121314151617181920from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCV# 实例化估计器estimator = RandomForestClassifier()# 网格搜索优化随机森林模型param_dict = &#123;'n_estimators':[120,200,300,500,800,1200],'max_depth':[5,8,15,25,30]&#125;estimator = GridSearchCV(estimator,param_grid=param_dict,cv=5)estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_test# print('预测值为:\n',y_predict)# print('比较真实值与预测值结果为:\n',res)print(res[res==True].size/res.size)# 方法2 计算模型准确值print('模型准确值为:\n',estimator.score(x_test,y_test))print('在交叉验证中最好的结果:\n',estimator.best_score_)print('最好的参数模型:\n',estimator.best_estimator_)# print('每次交叉验证后的结果准确率为:\n',estimator.cv_results_) 输出123456789101112130.817258883248731模型准确值为: 0.817258883248731在交叉验证中最好的结果: 0.8389553862894451最好的参数模型: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Random Forest</tag>
        <tag>Decision Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——集成学习方法之随机森林(RF)]]></title>
    <url>%2F2018%2F03%2F21%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97(RF)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——集成学习方法之随机森林(RF)集成学习方法的定义集成学习通过建立几个模型组合地来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的作出预测。 随机森林的定义在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。 随机森林原理过程学习算法根据下列算法而建造每棵树： 用N来表示训练用例(样本)的个数，M表示特征数目 1.一次随机选出一个样本，重复N次(有可能出现重复的样本) 2.随机去选出m个特征，m&lt;&lt;M，建立决策树 采用bootstrap抽样 即随机有放回抽样 随机，如果不进行随机抽样，每棵树的训练集都相同，从而导致最终训练出的树分类结果也完全相同 有放回，如果不是有放回的抽样，每棵树的训练样本都不同，都是没有交集的，这样每棵树可能是”有偏的”，也可能是绝对”片面的”，即每棵树训练出来都是有很大的差异；而随机森林最后分类取决于多棵树(弱分类器)的投票表决。 随机森林API sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2) 随机森林分类器 n_estimators：integer，optional(default=10)森林里的树木数量 criteria：string，可选(default=’gini’) 分割特征的测量方法 max_depth：integer或None，可选(默认=无) 树的最大深度 max_features=’auto’：每个决策树的最大特征数量 If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features 超参数：n_estimator, max_depth, min_samples_split, min_samples_leaf 实例化随机森林估计器12from sklearn.ensemble import RandomForestClassifier()estimator = RandomForestClassifier() 定义超参数的选择列表1param_dict = &#123;"n_estimators":[120,200,300,500,800,1200],"max_depth":[5,8,15,25,30]&#125; 使用GridSearchCV进行网格搜索1234# 超参数调优 并设置2折验证estimator = GridSeachCV(estimator,param_grid=param_dict,cv=2)estimator.fit(x_train,y_train)print('随机森林的预测准确率为:\n',estimator.score(x_test,y_test)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Random Forest</tag>
        <tag>Decision Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——决策树(DT)]]></title>
    <url>%2F2018%2F03%2F21%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91(DT)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——决策树(DT)原理信息熵、信息增益等 优点 简单的理解和解释，树木可视化 缺点 容易创建出泛化能力较差且过于复杂的模型，称之为过拟合 改进 剪枝cart算法(决策树API中已实现)企业重要决策，由于决策树很好的分析能力，在决策过程应用较多， 可以选择特征信息熵的定义在信息论与概率统计中，熵是表示随机变量不确定性的度量，熵的公式定义为：$$H(X)=-\sum{P(x_i)logP(x_i)}$$其中，对数一般以2为底或以e为底(自然对数)，这时熵的单位分别称作比特(bit)或纳特(nat)。由定义可知，熵只依赖于X的分布，而与X的取值无关。 信息增益当上和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵(empirical entropy)和经验条件熵(conditional entropy)。此时，如果有0概率，令0log0=0。特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。$$g(D,A)=H(D)-H(D|A)$$信息增益表示得知特征X的信息而息的不确定性减少的程度使得类Y的信息熵减少的程度。经验熵的计算：$$H(D)=-\sum\limits_{|k=1|}^{|K|}\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}$$其中，$C_k$表示属于某个类别的样本数。经验条件熵的计算：$$H(D|A)=\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}\sum\limits_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}$$决策树的原理还有其他方法 ID3 信息增益 最大的准则 C4.5 信息增益比 最大的准则 CART 分类树：基尼系数 最小的准则 在sklearn中可以选择划分的默认原则 优势：划分更加细致 决策树API sklearn.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=None,random_state=None) 决策树分类器 criterion：默认是’gini’系数，也可以选择信息增益的熵’entropy’ max_depth：树的深度大小 random_state：随机数种子 决策树可视化保存树的结构到dot文件 sklearn.tree.export_graphviz(estimator,out_file=’tree.dot’,feature_names=[‘’,’’]) 该函数能够导出DOT格式 网站显示结构 http://webgraphviz.com/ 输入DOT格式内容 以鸢尾花数据集的DT为例：1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier# 获取数据集与分割数据集iris = load_iris()x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 模型训练与评估# 实例化一个估计器estimator = DecisionTreeClassifier()# 传入训练数据集，进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测值为:\n',y_predict)print('比较真实值与预测值结果为:\n',res)print(res[res==True].size/res.size)# 方法2 计算模型准确值score = estimator.score(x_test,y_test)print('模型准确值为:\n',score) 输出：1234567891011预测值为: [0 0 0 2 1 0 0 2 2 1 1 0 1 1 1 2 2 2 2 2 1 0 1 1 1 0 2 0 0 2 0 0 0 2 1 1 1 1 0 1 1 0 1 1 2]比较真实值与预测值结果为: [ True True True True True True True True True True True True False True False True True True False True True True True True True True True True True True True True True True True True False True True True True True True True True]0.9111111111111111模型准确值为: 0.9111111111111111]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Decision Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——20类新闻分类]]></title>
    <url>%2F2018%2F03%2F20%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%9420%E7%B1%BB%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[项目实战——20类新闻分类流程分析 获取数据 划分数据集 特征抽取 Tf-idf 朴素贝叶斯 朴素贝叶斯新闻文章分类12345678910111213141516171819202122232425262728from sklearn.datasets import fetch_20newsgroupsfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizer# 1.获取数据news = fetch_20newsgroups()# 2.划分数据集x_train,x_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.3)# 3.特征抽取 Tf-idf# 3.1.实例化转化器transfer = TfidfVectorizer()# 3.2.转化数据x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4.朴素贝叶斯# 4.1.实例化估计器estimator = MultinomialNB()estimator.fit(x_train,y_train)# 4.2.模型评估# 方法1 比较真实值与预测值y_predcit = estimator.predict(x_test)res = y_predict == y_testprint('预测结果为:\n',y_predict)print('比对真实值和预测值:\n',res)print(res[res==True].size/res.size)# 方法2 直接计算准确率score = estimator.score(x_test,y_test)print('准确率为:\n',score) 输出：123456预测值为: [ 4 14 13 ... 14 10 9]比较真实值与预测值结果为: [ True True True ... True True True]模型准确率为: 0.8176730486008836]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——朴素贝叶斯(NB)]]></title>
    <url>%2F2018%2F03%2F20%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF(NB)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——朴素贝叶斯(NB)概率的定义 概率定义为一件事情发生的可能性 $P(X)$：取值在[0, 1] 联合概率、条件概率与相互独立 联合概率：包含多个条件，且所有条件同事成立的概率 记作：$P(A, B)$ 条件概率：就是事件A在另一个事件B已经发生条件下的发生概率 记作：$P(A|B)$ 相互独立：如果$P(A, B)=P(A)P(B)$，则称事件A与事件B相互对立 朴素贝叶斯的定义朴素贝叶斯(naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方法。 优点 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率 对缺失数据不太敏感，算法也比较简单，常用语文本分类 分类准确度高，速度快 缺点 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好 贝叶斯公式公式$$P(C|W) = \frac{P(W|C)P(C)}{P(W)}$$其中W为给定文档的特征值(频数统计，预测文档提供)，C为文档的类别 朴素贝叶斯公式可以理解为$$\begin{split}P(C|F1,F2,\cdots)&amp;=\frac{P(F1,F2,\cdots|C)P(C)}{P(F1,F2,\cdots)}\\&amp;=\frac{P(F1|C)P(F2|C),\cdots{P(C)}}{P(F1)P(F2)\cdots}\end{split}$$且对于任意C=c其分母都一致，则在分类时仅需要比较分子大小 贝叶斯估计若任意一个$P(Fi|C)=0$即后验概率为0，则会导致分类产生偏差，此时采用贝叶斯估计 公式$$P(Fi|C)=\frac{N_i+\alpha}{N+\alpha{m}}$$常取$\alpha=1$，此时称为拉普拉斯平滑(Laplace smoothing)。 API sklearn.native_bayes.MultinomialNB(alpha = 1.0) 朴素贝叶斯分类 alpha：拉普拉斯平滑系数]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实战——预测FaceBook签到位置]]></title>
    <url>%2F2018%2F03%2F20%2FProject%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8BFaceBook%E7%AD%BE%E5%88%B0%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[项目实战——预测FaceBook签到位置train and testrow_id：登记时间的IDx y：坐标准确性：定位准确性时间：时间戳place_id：业务的ID(预测目标) 流程分析 数据预处理 缩小数据集范围 时间特征提取 将签到位置少于n个用户的删除 数据集划分 特征工程：标准化 KNN算法 GSCV优化 模型优化 代码实现 获取数据集 123456# 项目实战——预测facebook签到位置import pandas as pdfrom sklearn.model_selection import train_test_split,GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfacebook = pd.read_csv('FBlocation/train.csv') 缩小数据的范围 选择有用的时间特征和取出标签较少的地点 12345678910111213141516171819# 数据预处理# 缩小数据集 去1.25&lt;x&lt;1.5 and 2.25&lt;y&lt;2.5facebook = facebook.query('x&lt;1.5&amp;x&gt;1.25&amp;y&gt;2.25&amp;y&lt;2.5')# 时间特征提取# pd.to_datetime(column,unit='s') 把对应列的数据按单位秒转化为时间time_value = pd.to_datetime(facebook['time'],unit='s')# pd.DatetimeIndex 将字符类型日期转化为时间戳索引time_value = pd.DatetimeIndex(time_value)# 将时间戳索引添加到facebook数据中facebook['day'] = time_value.dayfacebook['hour'] = time_value.hourfacebook['weekday'] = time_value.weekday# # 删除用户分享小于3的地点# 聚合统计place_count = facebook.groupby(['place_id']).count()# 查询分享大于3的所有地点place_count = place_count.query('row_id&gt;3')# 筛选facebook中的数据facebook = facebook[facebook['place_id'].isin(place_count.index)] 取出数据的特征值和目标值 1234# 数据集划分# 选择多列时需传入列表x = facebook[['x','y','accuracy','day','hour','weekday']]y = facebook['place_id'] 划分训练集与测试集 12# 划分训练集与测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=8) 标准化处理 12345# 特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)# 此处x_test在标准化时需使用x_train的均值与方差x_test = transfer.transform(x_test) K近邻算法模型进行预测 12345678910111213141516171819202122# 模型训练与评估# 实例化估计器estimator = KNeighborsClassifier()# 准备超参数param_dict = &#123;'n_neighbors':[3,5,7,9]&#125;# 运用网格搜索参数优化KNN算法# cv=5 为设置S折验证中的S为5estimator = GridSearchCV(estimator,param_grid=param_dict,cv=5)# 传入训练数据，进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测值为:\n',y_predict)# print('比较真实值与预测值的结果为:\n',res)print(res[res==True].size/res.size)# 方法2 计算模型的准确值print('模型的准确率为:\n',estimator.score(x_test,y_test))print('在交叉验证中最好的结果:\n',estimator.best_score_)print('最好的参数模型:\n',estimator.best_estimator)print('每次交叉验证后的结果准确率为:\n',estimator.cv_results_) 输出：12345678910111213141516预测值为: [4760271365 4760271365 9841447845 ... 9841447845 1226687693 7536975002]0.46121318168562264模型准确率为: 0.46121318168562264在交叉验证中最好的结果: 0.4498468845697144最好的参数模型: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=9, p=2, weights='uniform')每次交叉验证后的结果准确率为: &#123;'mean_fit_time': array([0.00774164, 0.00718284, 0.00750437, 0.00720239]), 'std_fit_time': array([0.00104153, 0.00050873, 0.00082317, 0.00051814]), 'mean_score_time': array([0.06678772, 0.07659383, 0.08839059, 0.09959669]), 'std_score_time': array([0.0063074 , 0.00286544, 0.00336546, 0.00472586]), 'param_n_neighbors': masked_array(data=[3, 5, 7, 9], mask=[False, False, False, False], fill_value='?', dtype=object), 'params': [&#123;'n_neighbors': 3&#125;, &#123;'n_neighbors': 5&#125;, &#123;'n_neighbors': 7&#125;, &#123;'n_neighbors': 9&#125;], 'split0_test_score': array([0.40610826, 0.43241609, 0.43664953, 0.42999698]), 'split1_test_score': array([0.4165132 , 0.4407612 , 0.44229589, 0.4490485 ]), 'split2_test_score': array([0.41081417, 0.43629584, 0.44655065, 0.45090118]), 'split3_test_score': array([0.42793509, 0.45020681, 0.45911549, 0.45752466]), 'split4_test_score': array([0.4295935 , 0.45528455, 0.46178862, 0.46308943]), 'mean_test_score': array([0.41797388, 0.44278483, 0.44903444, 0.44984688]), 'std_test_score': array([0.00923467, 0.00850695, 0.00966491, 0.01127239]), 'rank_test_score': array([4, 3, 2, 1], dtype=int32), 'split0_train_score': array([0.64778636, 0.59476918, 0.57176619, 0.54970852]), 'split1_train_score': array([0.65141646, 0.59664129, 0.56846896, 0.5480656 ]), 'split2_train_score': array([0.64710944, 0.59524368, 0.56841117, 0.54994915]), 'split3_train_score': array([0.64644579, 0.59254939, 0.56159589, 0.54168611]), 'split4_train_score': array([0.64474702, 0.59306823, 0.5649853 , 0.54239517]), 'mean_train_score': array([0.64750102, 0.59445436, 0.5670455 , 0.54636091]), 'std_train_score': array([0.00220288, 0.00148695, 0.00346753, 0.00359357])&#125;]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——K-近邻算法(KNN)]]></title>
    <url>%2F2018%2F03%2F19%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95(KNN)%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——K-近邻算法(KNN)定义如果一个样本在特征空间中的k各最相似(即特征空间中最邻近)的样本中的大多数属于另一个类别，则该样本也属于这个类别。 优点： 简单，易于理解，易于实现，无需训练 缺点： 懒惰算法，对测试样本分类时的计算量大，内存开销大 必须指定K值，K值选择不当则分类精度不能保证 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试 K-近邻算法API sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’) n_neighbors:：int，可选(默认5)，k_neighbors查询默认使用的邻居数 algorithm：{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}，可选用于最近邻居的算法 ‘ball_tree’将会使用BallTree ‘kdtree’将会使用KDTree ‘auto’将尝试根据传递给fit方法的值来决定何时的算法。 不同实现方式影响效率 以鸢尾花数据集的KNN为例：12345678910111213141516171819202122232425262728293031# 鸢尾花种类预测from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier'''KNN算法对鸢尾花数据集分类'''# 1.获取数据集和分割数据集iris = load_iris()x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 2.特征工程，标准化# 2.1.实例化转化器transfer = StandardScaler()# 2.2.对训练集和测试集进行标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 3.模型训练和评估# 3.1.实例化估计器estimator = KNeighborsClassifier()# 3.2.传入训练数据集，进行机器学习estimator.fit(x_train, y_train)# 3.3.模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测结果为:\n', y_predict)print('比对真实值和预测值L\n', res)print(res[res==True].size/res.size)# 方法2 直接计算准确率score = estimator.score(x_test, y_test)print('准确率为:\n', score) 输出：1234567891011预测结果为: [0 0 0 2 1 0 0 2 2 1 1 0 1 1 1 2 2 2 1 2 1 0 1 1 1 0 2 0 0 2 0 0 0 2 1 1 1 1 0 1 1 0 1 1 2]比对真实值和预测值L [ True True True True True True True True True True True True False True False True True True True True True True True True True True True True True True True True True True True True False True True True True True True True True]0.9333333333333333准确率为: 0.9333333333333333]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>k-Nearest Neighbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——模型选择与调优]]></title>
    <url>%2F2018%2F03%2F19%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——模型选择与调优交叉验证交叉验证目的：为了让被评估的模型更加准确可信 超参数搜索-网格搜索(Grid Search)在模型选择与调优的过程中，需要手动指定的参数(如k-邻近算法中的K值)叫做超参数。但是手动过程繁杂，需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。 模型选择与调优API sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None) 对估计器的指定参数值进行详尽搜索 estimator：估计器对象 param_grid：估计器参数(dict)(‘n_neighbors’:[1,3,5]) cv：指定S折交叉验证的S fit：输入训练数据 score：准确率 结果分析： bestscore：在交叉验证中验证的最好结果 bestestimator：最好的参数模型 cvresults：每次交叉验证后的验证集准确率结果和训练集准确率结果 以鸢尾花数据集的KNN为例：1234567891011121314151617181920212223242526272829303132333435363738# 鸢尾花种类预测from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import GridSearchCV'''KNN算法对鸢尾花数据集分类'''# 1.获取数据集和分割数据集iris = load_iris()x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 2.特征工程，标准化# 2.1.实例化转化器transfer = StandardScaler()# 2.2.对训练集和测试集进行标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 3.模型训练和评估# 3.1.实例化估计器estimator = KNeighborsClassifier()# 3.2.网络搜索和交叉验证# 3.2.1.准备超参数param_dict = &#123;'n_neighbors':[1,3,5,7,9]&#125;estimator = GridSearchCV(estimator, param_grid=param_dict, cv=6)# 3.2.传入训练数据集，进行机器学习estimator.fit(x_train, y_train)# 3.3.模型评估# 方法1 比较真实值与预测值y_predict = estimator.predict(x_test)res = y_predict == y_testprint('预测结果为:\n', y_predict)print('比对真实值和预测值L\n',res)print(res[res==True].size/res.size)# 方法2 直接计算准确率score = estimator.score(x_test, y_test)print('准确率为:\n', score)print("在交叉验证中验证的最好结果：\n", estimator.best_score_)print("最好的参数模型：\n", estimator.best_estimator_)print("每次交叉验证后的准确率结果：\n", estimator.cv_results_) 输出：12345678910111213141516171819202122预测结果为: [0 0 0 2 1 0 0 2 2 1 1 0 1 1 1 2 2 2 2 2 1 0 1 1 1 0 2 0 0 2 0 0 0 2 1 1 1 1 0 1 1 0 1 1 2]比对真实值和预测值L [ True True True True True True True True True True True True False True False True True True False True True True True True True True True True True True True True True True True True False True True True True True True True True]0.9111111111111111准确率为: 0.9111111111111111在交叉验证中验证的最好结果： 0.9714285714285714最好的参数模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=1, p=2, weights='uniform')每次交叉验证后的准确率结果： &#123;'mean_fit_time': array([0.00038306, 0.00037567, 0.00055607]), 'std_fit_time': array([1.02261530e-04, 4.53648737e-05, 2.36380916e-04]), 'mean_score_time': array([0.00077566, 0.00076612, 0.00082179]), 'std_score_time': array([0.00018651, 0.00024699, 0.00017762]), 'param_n_neighbors': masked_array(data=[1, 3, 5], mask=[False, False, False], fill_value='?', dtype=object), 'params': [&#123;'n_neighbors': 1&#125;, &#123;'n_neighbors': 3&#125;, &#123;'n_neighbors': 5&#125;], 'split0_test_score': array([1. , 0.88888889, 0.88888889]), 'split1_test_score': array([0.94444444, 0.94444444, 0.94444444]), 'split2_test_score': array([1., 1., 1.]), 'split3_test_score': array([1., 1., 1.]), 'split4_test_score': array([1., 1., 1.]), 'split5_test_score': array([0.875, 0.875, 0.875]), 'mean_test_score': array([0.97142857, 0.95238095, 0.95238095]), 'std_test_score': array([0.04575725, 0.05252505, 0.05252505]), 'rank_test_score': array([1, 2, 2], dtype=int32), 'split0_train_score': array([1. , 0.97701149, 0.96551724]), 'split1_train_score': array([1. , 0.98850575, 0.96551724]), 'split2_train_score': array([1. , 0.95402299, 0.95402299]), 'split3_train_score': array([1. , 0.97701149, 0.96551724]), 'split4_train_score': array([1. , 0.97727273, 0.95454545]), 'split5_train_score': array([1., 1., 1.]), 'mean_train_score': array([1. , 0.97897074, 0.96752003]), 'std_train_score': array([0. , 0.01394093, 0.01537038])&#125;]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——转换器和估计器]]></title>
    <url>%2F2018%2F03%2F19%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——转换器和估计器转换器即特征工程的实现(面向对象)步骤 实例化转换器 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用) fit_transform 先进行fit，再进行transform fit 保存相关参数，如均值、方差、标准差等 transform 使用转化器中已保存的相关参数进行转化 估计器即sklearn机器学习算法的实现(面向对象)步骤 实例化估计器 用于分类的估计器： sklearn.neighbors k-近邻算法 sklearn.naive_bayes 贝叶斯 sklearn.linear_model.LogisticRegression 逻辑回归 sklearn.tree 决策树与随机森林 用于回归的估计器 sklearn.linear_model.LinearRegression 线性回归 sklearn.linear_model.Ridge 岭回归 用于无监督学习的估计器 sklearn.cluster.KMeans 聚类 传入训练数据集，进行机器训练 模型估计 比较真实值与预测值 y_predict = estimator.predict(x_test) 计算模型准确率 estimator.score(x_test,y_test)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Transfer</tag>
        <tag>Estimator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征降维]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征降维降维的定义降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程 降低随机变量的个数 相关特征(correlated feature) 由于在进行训练的时使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大。 降维的原因 使得数据集更易使用 降低很多算法的计算开销 去除噪声 使得结果易懂 降维的几种方法 1.特征选择(Feature Selection) 2.主成分分析(Principal Component Analysis,PCA) 3.因子分析(Factor Analysis) 4.独立成分分析(Independent Component Analysis,ICA)特征选择sklearn.feature_selection定义数据中包含冗余或无关变量(或称特征、属性、指标等)，旨在从原有特征中找出主要特征方法 Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联 方差选择法：低方差特征过滤 相关系数 Embedded(嵌入式)：算法自动选择特征(特征与目标值之间的关联) 决策树：信息熵、信息增益 正则化：L1、L2 深度学习：卷积等 过滤式低方差特征过滤删除低方差的一些特征 特征方差小：某个特征大多样本的值比较相近 特征方差大：某个特征很多样本的值都有差别 API sklearn.feature_selection.VarianceThreshold(threshold=0.0) 删除所有低方差特征 Variance.fit_transform(X) X：numpy array格式的数据[n_samples,n_features] 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同的特征。123456789101112# 删除低方差特征值from sklearn.feature_selection import VarianceThresholdimport pandas as pd# 获取数据data = pd.read_csv('data.csv')# 查看列print(data.columns)print(data[data.columns[1:-2]].shape)# 实例化转换器，同时设置阈值transfer = VarianceThreshold(threshold=3)# 转换数据，选择有效特征列data = transfer.fit_transform(data[data.columns[1:-2]]) 相关系数 皮尔逊相关系数(Pearson Correlation Coefficient) 反应变量之间相关关系密切程度的统计指标 公式$$r = \frac{n\sum{xy}-\sum{x}\sum{y}}{\sqrt{n\sum{x^2}-(\sum{x})^2\sqrt{n\sum{y^2}-(\sum{y})^2}}}$$ 特点 相关系数的值介于-1与+1之间，即$-1\leq{r}\leq{+1}$ 当$r&gt;0$时，表示量变量正相关，$r&lt;0$时，两变量为负相关 当$|r|=1$时，表示量变量为完全相关，当$r=0$时，表示量变量无相关关系 当$0&lt;|r|&lt;1$时，表示量变量存在一定程度的相关。且$|r|$越接近1，量变量间线性关系越密切；$|r|$越接近于0，表示量变量的新型相关越弱 一般可以按三级划分：$|r|&lt;0.4$为低度相关;$0.4\geq{|r|}\geq{0.7}$为显著相关；$0.7\geq{|r|}\geq{1}$为高度线性相关 API from scipy.stats import pearsonr x : (N,) array_like y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value 1234567data = pd.read_csv('data.csv')factor = data.columns[1:-2]for i in range(len(factor)): for j in range(i,len(factor)-1): # 错位比较 x = pearsonr(data[factor[i]],data[factor[j+1]]) print(factor[i],factor[j+1],x) 画图观察 123456import pandas as pdimport matplotlib.pylab as pltdata = pd.read_csv('data.csv')# 选择所有有效特征列pd.scatter_matrix(data[data.columns[1:-2]],figsize=(20,8))plt.show() PCAPCA定义高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量 PCA作用数据维数压缩，尽可能降低原数据的维数(复杂度)，损失少量信息 优点：降低数据的复杂性，识别最重要的多个特性 缺点：不一定需要，且可能损失有用信息 适用数据类型：数值型数据 PCA应用应用于回归分析或者聚类分析中 API sklearn.decomposition.PCA(n_components=None) 将数据分解为较低维数空间 n_components: 小数：表示保留百分之多少信息 整数：减少到多少个特征 PCA.fit_transform(X) X：numpy array格式的数据[n_samples,n_features] 返回值：转换后指定维度的array 12345678from sklearn.decomposition import PCA# 获取数据data = [[2,8,4,5],[6,3,0,8],[5,4,9,1]]# 实例化一个转换器(保留3个特征)transfer = PCA(n_components=3)# 传入数据进行转化data = transfer.fit_transform(data)print(data) 输出：123[[ 1.22879107e-15 3.82970843e+00 2.65047672e-17] [ 5.74456265e+00 -1.91485422e+00 2.65047672e-17] [-5.74456265e+00 -1.91485422e+00 2.65047672e-17]]]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征预处理]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征预处理特征预处理定义通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程 特征预处理内容： 数据型数据的无量纲化 归一化 标准化 特征预处理的意义 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征 特征预处理sklearn.preprocessing 归一化定义通过对原始数据进行变换把数据映射到(feature_range，默认为[0,1])之间。由于最大值最小值是变化的，并且最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。 公式$$X’=\frac{x-min}{max-min}&emsp;X’’=X’*(max-min)+min$$ 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0 API sklearn.preprocessing.MinMaxScaler(feature_range(0,1)…) MinMaxScalar.fit_transform(X) X : array-like, shape (n_samples, n_features)The data. feature_range : tuple (min, max), default=(0, 1)Desired range of transformed data. axis : int (0 by default)axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample. copy : boolean, optional, default is TrueSet to False to perform inplace scaling and avoid a copy (if the input is already a numpy array). 返回值：转换后的形状相同的array 1234567891011import pandas as pdfrom sklearn.preprocessing import MinMaxScaler# 1.获取数据data = pd.read_csv("data.txt")# 2.实例化一个转换器类# feature_range 期望的转换数据范围，即输出在2到3之间transfer = MinMaxScaler(feature_range=(2, 3))# 3.调用fit_transformdata = transfer.fit_transform(data[columns])# 4.输出print(data) 标准化定义通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 公式$$X’=\frac{x-mean}{\sigma}$$ 作用于每一列，mean为平均值，σ为标准差 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。 API sklearn.preprocessing.StandardScaler() 处理之后每列来说所有数据都聚集在均值0附近标准差差为1 StandardScaler.fit_transform(X) X ： numpy array格式的数据[n_samples,n_features] 返回值：转换后的形状相同的array 1234567891011121314import pandas as pdfrom sklearn.preprocessing import StandardScaler# 1.获取数据data = pd.read_csv("data.txt")# 2.实例化一个转换器类# feature_range 期望的转换数据范围，即输出在2到3之间transfer = StandardScaler(feature_range=(2, 3))# 3.调用fit_transformdata = transfer.fit_transform(data[columns])# 4.输出print(data)# 可以使用transfer.mean_查看平均值，transfer.var_查看方差print('均值',transfer.mean_)print('方差',transfer.var_)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征抽取]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征抽取特征抽取 sklearn.feature_extraction 将任意数据（如文本或图像）转换为可用于机器学习的数字特征，即特征值化。特征值化是为了计算机更好的去理解数据。 字典特征提取 文本特征提取 图像特征提取 字典特征提取对字典数据进行特征值化 sklearn.feature_extraction.DictVectorizer(sparse=True….) DictVectorizer.fit_transform(X) X：字典或者包含字典的迭代器返回值，返回sparse矩阵，使用sparse矩阵可以节省内存，效率高。 DictVectorizer.inverse_transform(X) X：array数组或者sparse矩阵，返回转换之前数据格式 DictVectorizer.get_feature_names() 返回类别名称 12345678910111213141516# 字典特征提取from sklearn.feature_extraction import DictVectorizer# 1.获取数据data = [&#123;'course': '语文','score':100&#125;,&#123;'course': '数学','score':60&#125;,&#123;'course': '英语','score':30&#125;]# 2.实例化一个转换器# 此处可以传入sparse=False直接返回arraytransfer = DictVectorizer()# 3.对数据进行转换data = transfer.fit_transform(data)# 4.输出结果print('特征名称\n',transfer.get_feature_names())# 返回sparse矩阵 节省内存 效率高print('data\n',data)print('type\n',type(data)) 输出：1234567891011特征名称 ['course=数学', 'course=英语', 'course=语文', 'score']data (0, 2) 1.0 (0, 3) 100.0 (1, 0) 1.0 (1, 3) 60.0 (2, 1) 1.0 (2, 3) 30.0type &lt;class 'scipy.sparse.csr.csr_matrix'&gt; 文本特征提取对文本数据进行特征值化 sklearn.feature_extracion.text.CountVectorizer(stop_words=[]) 返回词频矩阵，忽略了单个字母单词和符号作为特征值 stop_words：iter,限制某些单词作为特征值英文文本特征提取1234567891011121314151617# 文本特征提取from sklearn.feature_extraction.text import CountVectorizer'''英文文本特征提取'''# 获取数据data = ["life is short,i like python","life is too long,i dislike python"]# 实例化一个转换器# stop_words iter,限制某些单词作为特征值# api忽略了单个字母单词和符号作为特征值transfer = CountVectorizer(stop_words=['is'])# 传入数据进行转换data = transfer.fit_transform(data)print('特征名称\n',transfer.get_feature_names())# 输出是特征出现频率print('data\n',data)print('type\n',type(data))# toarray()转ndarray对象print(data.toarray()) 输出：12345678910111213141516特征名称 ['dislike', 'life', 'like', 'long', 'python', 'short', 'too']data (0, 4) 1 (0, 2) 1 (0, 5) 1 (0, 1) 1 (1, 0) 1 (1, 3) 1 (1, 6) 1 (1, 4) 1 (1, 1) 1type &lt;class 'scipy.sparse.csr.csr_matrix'&gt;[[0 1 1 0 1 1 0] [1 1 0 1 1 0 1]] 中文文本特征提取先使用jieba分词进行分词123456789import jiebafrom sklearn.feature_extraction.text import CountVectorizer# jieba分词 中文文本特征提取'''中文文本特征提取演示'''def cut_word(data): text = ' '.join(list(jieba.cut(data))) return texttext = "人生苦短，我喜欢Python；生活太长久，我不喜欢Python"cut_word(text) 再进行特征提取123456789101112text_list = []data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。", "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。", "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]for s in data: text_list.append(cut_word(s))print(text_list)# 实例化一个转换器transfer = CountVectorizer()data = transfer.fit_transform(text_list)print('特征值名称\n',transfer.get_feature_names())print(data.toarray()) 输出：123456789['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']特征值名称 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样'][[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0] [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1] [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]] TF-IDF文本特征提取 TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 TF-IDF是分类机器学习算法进行文章分类中前期数据处理方式公式 词频(term frequency, tf)指的是某一个给定的词语在该文件中出现的频率 逆向文档频率(inverse document frequency,idf)是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到$$tfidf_{i,j}=tf_{ij}\times{idf_i}$$最终得出结果可以理解为重要程度 注：假如一篇文件的总词语数是100个，而词语”非常”出现了5次，那么”非常”一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现”非常”一词的文件数。所以，如果”非常”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后”非常”对于这篇文档的tf-idf的分数为0.05 * 3=0.15 12345678910111213141516171819202122import jiebafrom sklearn.feature_extraction.text import TfidfVectorizer# jieba分词 中文文本特征提取'''中文文本tf-idf提取演示'''def cut_word(data): text = ' '.join(list(jieba.cut(data))) return texttext_list = []data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。", "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。", "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]for s in data: text_list.append(cut_word(s))print(text_list)# 实例化一个转换器transfer = TfidfVectorizer()data = transfer.fit_transform(text_list)print('特征值名称\n',transfer.get_feature_names())print(data.toarray())x = data.toarray()# 打印得分最高的特征索引print('max_index',np.where( x== np.max(x))) 输出：12345678910111213141516171819202122232425['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']特征值名称 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样'][[0.30847454 0. 0.20280347 0. 0. 0. 0.40560694 0. 0. 0. 0. 0. 0.20280347 0. 0.20280347 0. 0. 0. 0. 0.20280347 0.20280347 0. 0.40560694 0. 0.20280347 0. 0.40560694 0.20280347 0. 0. 0. 0.20280347 0.20280347 0. 0. 0.20280347 0. ] [0. 0. 0. 0.2410822 0. 0. 0. 0.2410822 0.2410822 0.2410822 0. 0. 0. 0. 0. 0. 0. 0.2410822 0.55004769 0. 0. 0. 0. 0.2410822 0. 0. 0. 0. 0.48216441 0. 0. 0. 0. 0. 0.2410822 0. 0.2410822 ] [0.12001469 0.15780489 0. 0. 0.63121956 0.47341467 0. 0. 0. 0. 0.15780489 0.15780489 0. 0.15780489 0. 0.15780489 0.15780489 0. 0.12001469 0. 0. 0.15780489 0. 0. 0. 0.15780489 0. 0. 0. 0.31560978 0.15780489 0. 0. 0.15780489 0. 0. 0. ]]max_index (array([2]), array([4]))]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——特征工程]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——特征工程特征工程的定义(Feature Engineering)特征工程是使用专业背景只是和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。即直接影响机器学习的效果。 特征工程的作用 Andrew Ng said；”Coming up with features is difficult, time-consuming, requires expert knowledge.’Applied machine learning’ is basically feature engineering.”注：业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 特征工程的位置与数据处理的比较 pandas：一个数据读取非常方便以及基本的处理格式的工具 sklearn：对于特征的处理提供了强大的接口 特征工程包含内容 特征抽取 特征预处理 特征降维]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn学习笔记——scikit-learn介绍]]></title>
    <url>%2F2018%2F03%2F18%2FSklearn%2FSklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94scikit-learn%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Sklearn学习笔记——scikit-learn介绍可用数据集Kaggle网址：https://www.kaggle.com/datasetsUCI数据集网址： http://archive.ics.uci.edu/ml/scikit-learn网址：http://scikit-learn.org/stable/datasets/index.html#datasets scikit-learn介绍及其特点 Python语言的机器学习工具 Scikit-learn包括许多知名的机器学习算法的实现 Scikit-learn文档完善，容易上手，丰富的API 目前稳定版本0.19.1scikit-learn安装1pip3 install Scikit-learn==0.19.1 scikit-learn数据集 sklearn.datasets 加载获取流行数据集 datasets.load_*() 获取小规模数据集，数据包含在datasets里 datasets.fetch_*(data_home=None) 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集瞎子啊的目录，默认是~/scikit_learn_data/ sklearn小数据集加载并返回鸢尾花数据集 sklearn.datasets.load_iris()加载并返回波士顿房价数据集 sklearn.datasets.load_boston() sklearn大数据集 sklearn.datasets.fetch_20newsgroups(data_home=None,subset=’train’) subset：’train’ or ‘test’，’all’，可选，选择要加载的数据集。 训练集的”训练”，测试集的”测试”，两者的”全部” sklearn数据集返回值 load和fetch返回的数据类型datasets.base.Bunch(字典格式) data：特征数据数组，是[n_samples * n_features]的二维numpy.ndarray数组 target：标签数组，是n_samples的一维numpy.ndarray数组 DESCR：数据描述 feature_names：特征名，新闻数据，手写数字、回归数据集没有 target_names：标签名 数据集的划分机器学习一般数据集会划分为两个部分 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 划分比例 训练集：70% 80% 75% 测试集：30% 20% 30% 数据划分 sklearn.model_selection.train_test_split(arrays,*options) x数据集的特征值 y数据集的标签值 test_size测试集的大小，一般为float，默认0.25 random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。 returnx_train, x_test, y_train, y_test (默认随机取) 1x_train,x_test,y_train,y_test = train_test_split(data,target,test_size=0.2,random_state=8)]]></content>
      <categories>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闭包]]></title>
    <url>%2F2017%2F11%2F03%2FPython%2F%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[闭包实现闭包的基石闭包的创建通常是利用嵌套函数来完成的。在PyCodeObject中，与嵌套函数相关的属性是co_cellvars和co_freevars。具体两者的的含义如下：co_cellvars：tuple，保存嵌套的作用域中使用的变量名集合co_freevars：tuple，保存使用了的外层作用域中的变量名集合closure.py会编译出3个PyCodeObject，其中有两个，一个与函数get_func对应，一个与函数inner_func对应，那么，与get_func对应的PyCodeObject对象中的co_cellvars就应该包含外层函数内的变量，即字符串”value”，因为其嵌套作用域(inner_func的作用域)中可以使用这个变量；同理，与函数inner_func对应的PyCodeObject对象中的co_freevars中应该也有该变量。在PyFrameObject对象中，也有一个属性与闭包的实现相关，这个属性就是f_localsplus，在PyFrame_New中的extras正是f_localsplus指向的那边内存的大小。1extras = code-&gt;co_stacksize + code-&gt;co_nlocals + ncells + nfrees; f_localsplus的完整内存布局：运行时栈(co_stacksize)、局部变量(co_nlocals)、cell对象(对应co_cellvars)、free对象(对应co_freevars) 闭包的实现创建 closure在python虚拟机执行CALL_FUNCTION指令时，会进入fast_function函数。而在fast_function函数中，由于当前的PyCodeObject为get_func对应之PyCodeObject，其中的co_flags为3(CO_OPTIMIZED|CO_NEWLOCALS)，所以最终不符合进入快速通道的条件，而会进入PyEval_EvalCodeEx。在PyEval_EvalCodeEx中，Python虚拟机会如同处理默认参数一样，将co_cellvars中的东西拷贝到新创建的PyFrameObject的f_localsplus中。嵌套函数有时候会很复杂，比如内层嵌套函数引用的不是外层嵌套函数的局部变量，而是外层嵌套函数的一个拥有默认值的参数。Python虚拟机会获得被内层嵌套函数引用的符号名，即字符串”value”，即获得被嵌套函数共享的符号名cellname，通过判断标识位found来处理被嵌套函数共享外层函数的默认参数，若found标识位为0时，Python虚拟机会创建一个cell对象——PyCellObject。cell对象仅维护一个ob_ref，指向一个Python中的对象。一开始cell对象维护的ob_ref指向了NULL，但当外层局部变量被创建时，即value=”inner”这个赋值语句执行的时候，这个cell对象会被拷贝到新创建的PyFrameObject对象的f_localsplus中，且这个对象呗拷贝到的位置是co-&gt;co_nlocals + i，说明在f_localsplus中，cell对象的位置是在局部变量之后的。 PyEval_CodeEx中的found标志位，指的是被内层嵌套函数引用的符号是否已经与某个值绑定的标识，或者说与某个对象建立了约束关系。只有在内层嵌套函数引用的是外层函数的一个有默认值的参数值时，这个标识才可能为1。 在处理co\cellvars即cell对象时，之前获得的cellname会被忽略，因为在get_func函数执行的过程中，对value这个cell变量的访问将通过基于索引访问f_localsplus完成，因而完全不需要再知道cellname了。这个cellname实际上是在处理内层嵌套函数引用外层函数的默认参数时产生的。在处理了cell对象之后，Python虚拟机将进入PyEval_EvalFrameEx，从而正是开始对函数get_func的调用过程。首先将PyStringObject对象(即外层函数的)压入到运行时栈，然后Python虚拟机开始执行STORE_DEREF。从运行时栈弹出的是PyStringObject对象”inner”，而从f_localsplus中取得的是PyCellObject对象，通过PyCell_Set来设置PyCellObject对象中的ob_ref。从而，f_localsplus就发生了变化。设置cell对象之后的get_func函数的PyFrameObject对象，如图： 在get_func的环境中，value符号对应着一个PyStringObject对象，但是closure的作用是将这个约束进行冻结，使得在嵌套函数inner_func被调用时还能使用这个约束。在执行”def inner_func()”表达式时，Python虚拟机就会将(value,”inner”)这个约束塞到PyFunctionObject中。首先将刚刚放置好的PyCellObject对象取出，并压入运行时栈，接着将PyCellObject对象打包进一个tuple中，tuple中可以放置多个PyCellObject。随后将inner_func对应的PyCodeObject对象也压入到运行时栈中，接着完成约束与PyCodeObject的绑定。表达式”def inner_func()”对应的将新创建的PyFunctionObject对象放置到了f_localsplus中，从而使f_localsplus发生了变化。设置function对象之后的get_func函数的PyFrameObject对象，如图：在get_func的最后，新建的PyFunctionObject对象作为返回值给了上一个栈帧，并被压入到该栈帧的运行时栈中。 使用 closureclosure是在get_func中被创建的，而对closure的使用，则是在inner_inner中。在执行”show_value()”对应的CALL_FUNCTION指令时，和inner_func对应的PyCodeObject中co_flags里包含了CO_NESTED，所以在fast_function中不能通过快速通道的验证，从而智能进入PyEval_EvalCodeEx。inner_func对应的PyCodeObject中的co_freevars里有引用的外层作用域中的富豪命，在PyEval_EvalCodeEx中，就会对这个co_freevars进行处理。其中的closure变量是作为最后一个函数参数传递进来的，即在PyFunctionObject对象中与PycodeObject对象绑定的装满PyCellObject对象的tuple。因此在PyEval_EvalCodeEx中，进行的动作就是讲这个PyCellObject对象一个一个放入到f_loaclsplus中相应的位置。在处理完closure之后，inner_func对应的PyFrameObject中的f_loaclsplus再次发生变化。设置cell对象之后的inner_func函数的PyFrameObject对象，如图：这里的动作与调用get_func是一致的，在inner_func调用的过程中，当引用外层作用域的符号时，其实是到f_localsplus中的free变量区域中获得符号对应的值。其实这就是inner_func函数中”print(value)”表达式对应的第一条字节码的意义。 闭包代码1234567def get_func():value = "inner"def inner_func():print(value) return inner_funcshow_value = get_func()show_value()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习笔记——基本使用]]></title>
    <url>%2F2017%2F03%2F18%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[OpenCV学习笔记——基本使用pip安装1pip install opencv-python 导入OpenCV1import cv2 # 即OpenCV 原图展示 文件读取与展示OpenCV是以BGR的顺序读取图像，而matplotlib则是以RGB的顺序读取图像，于是蓝色和红色的通道颠倒了。1234import cv2import matplotlib.pylab as pltinput_image = cv2.imread('test.jpeg')plt.imshow(input_image) 输出图像： 图像的大小、尺寸和类型参数123print('size',input_image.size)print('shape',input_image.shape)print('dtype',input_image.dtype) 输出：123size 1195350shape (613, 650, 3)dtype uint8 注意 最后一个（数据类型）是用Python工作的棘手问题之一。由于它不是强类型化的，Python将允许您具有不同类型的数组，但大小相同，并且一些函数将返回可能不需要的类型数组。能够像这样检查和检查数据类型是非常有用的，也是我经常在调试中发现的事情之一。 颜色之间的转换、分离与合并为了调整回正确的颜色，首先要分离图像三个颜色通道。 注意：一般计算机中显示图像是由蓝色、绿色和红色三个色层叠加而成的，这也使得我们可以分离三个颜色通道，即在分离时，使用b,g,r的顺序获取。 在OpenCV中，可以使用 split 函数分离，使用 merge 合并。 分离1234# 分离b,g,r=cv2.split(input_image)# 展示其中一种颜色 (此处展示了红色，你可以用b来展示蓝色)plt.imshow(r, cmap='gray') 输出图像： 合并1234# 合并merged=cv2.merge([r,g,b])# 合并采用单通道矩阵数组plt.imshow(merged) 输出图像：除了以上合并方法，OpenCV还有专门解决这一问题的功能，即 CORLYBBGR2RGB 。于是我们不用通过手动拆分合并就能显示正确的颜色，它的使用方式如下：12opencv_merged=cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)plt.imshow(opencv_merged) 输出图像： 获取图像数据和设置图像数据Python OpenCV中的图像是以Numpy数组的形式保存。类似于Numpy中的数组索引，我们也可以提取和更改每个像素点的数值。12pixel = input_image[100,100]print(pixel) 输出：1[ 26 84 243] 123input_image[100,100] = [0,0,0]pixelnew = input_image[100,100]print(pixelnew) 输出：1[0 0 0] 获取和设置图像区域我们可以得到或设置单个像素，同样地，我们可以获取或设置图像的区域。这类似于Numpy中的数组切片：12center = opencv_merged[201:501, 202:502]plt.imshow(center) 输出图像： 注意：图像切片选取的方向与Numpy数组索引的方向也是一致的，在上述代码中，“201:501”表示竖直向下201至501的像素行，“202:502”表示水平向右202至502列的像素。 同样，我们也可以对选中的区域进行修改：1234fresh_image=opencv_merged.copy() # 复制一张图像fresh_image[20:20+center.shape[0], 30:30+center.shape[1]]=center # 修改选中区域print(center.shape)plt.imshow(fresh_image) 输出图像： 注意：在对选中区域进行修改时，选中的区域必须与传入的图像大小一致，故可以在切片时使用[a:a+test.shape[0],b:b+test.shape[1]]来确保选中区域的大小与传入图像大小一致。 矩阵切片在OpenCV Python风格中，正如我所提到的，图像是Numpy数组。在Numpy教程中有一些很棒的数组操作：如果你以前没有做过，那么这是一个很好的介绍。上面的区域的获取和设置使用切片，尽管如此，我还是想把这本书的内容更详细地讲完。123freshim2 = opencv_merged.copy()crop = freshim2[100:200, 130:400] plt.imshow(crop) 输出图像： 切片可以由以下几种方式：[top_y:bottom_y, left_x:right_x]或者[y:y+height, x:x+width]同时，您也可以在切片时，直接提取颜色[y:y+height, x:x+width, channel]其中channel你感兴趣的颜色。这里定义 0=蓝色，1=绿色，2=红色。下面我们采用第三种方式获取切片，并转换到HSV的例子。 HSV（色相，饱和度，明度）为另一种色彩空间，有兴趣可以了解色彩空间理论！123hsvim=cv2.cvtColor(freshim2,cv2.COLOR_BGR2HSV)bcrop =hsvim[100:200, 130:400, 1]plt.imshow(bcrop, cmap="gray") 输出图像：]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——高级处理]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%AB%98%E7%BA%A7%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——高级处理1.缺失值处理处理nan或者默认标记在pandas中我们处理起来非常容易 判断数据是否为NaN pd.isnull(df), pd.notnull(df)处理方式： 存在缺失值nan, 并且是np.nan: 1 删除存在缺失值的：dropna(axis=’rows’) 注：不会修改原数据，需要接受返回值 2 替换缺失值：fillna(value, inplace=True) value：替换成的值 inplace： True：会修改原数据 False：不替换修改原数据，生成新的对象 不是缺失值nan，有默认标记的，如问号 先替换’?’为np.nan，再进行缺失值的处理 df.replace(to_replace=, value=) to_replace:替换前的值 value:替换后的值 2.数据离散化连续属性的离散化就是将连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数 值代表落在每个子区间中的属性值。 读取数据 数据分组 使用的工具： pd.qcut(data, bins)： 对数据进行分组将数据分组 一般会与value_counts搭配使用，统计每组的个数 series.value_counts()：统计分组次数 pd.cut(data, bins)：自定义区间分组 pandas.get_dummies(data, prefix=None):获取one-hot编码矩阵 data:array-like, Series, or DataFrame prefix:分组名字 3.合并3.1.pd.concat实现合并 pd.concat([data1, data2], axis=1) 按照行或列进行合并,axis=0为列索引，axis=1为行索引3.2.pd.merge实现合并 pd.merge(left, right, how=’inner’, on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=(‘_x’, ‘_y’), copy=True, indicator=False,validate=None) 可以指定按照两组数据的共同键值对合并或者左右各自 left: A DataFrame object right: Another DataFrame object on: Columns (names) to join on. Must be found in both the left and right DataFrame objects. left_on=None, right_on=None：指定左右键 Merge method SQL Join Name Description left LEFT OUTER JOIN Use keys from left frame only right RIGHT OUTER JOIN Use keys from right frame only outer FULL OUTER JOIN Use union of keys from both frames inner INNER JOIN Use intersection of keys from both frames 4.交叉表与透视表4.1.crosstab(交叉表)实现交叉表：交叉表用于计算一列数据对于另外一列数据的分组个数(寻找两个列之间的关系) pd.crosstab(value1, value2)4.2.pivot_table(透视表)实现 pd.pivot_table([], index=[]) 5.分组与聚合 DataFrame.groupby(key, as_index=False) key:分组的列数据，可以多个]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——文件读取与存储]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——文件读取与存储文件读取与存储1.csv1.1.读取csv文件-read_csv() pandas.read_csv(filepath_or_buffer, sep =’,’ , delimiter = None) filepath_or_buffer:文件路径 usecols:指定读取的列名，列表形式1.2.写入csv文件-to_csv() DataFrame.to_csv(path_or_buf=None, sep=’, ’, columns=None, header=True, index=True, index_label=None, mode=’w’, encoding=None) path_or_buf :string or file handle, default None sep :character, default ‘,’ columns :sequence, optional mode:’w’：重写； ‘a’ 追加，若使用mode=’a’，需要同时指定header=False，否侧会吧column名一起追加入数据中 index:是否写进行索引 header :boolean or list of string, default True,是否写进列索引值 Series.to_csv(path=None, index=True, sep=’, ‘, na_rep=’’, float_format=None, header=False, index_label=None, mode=’w’, encoding=None, compression=None, date_format=None, decimal=’.’)Write Series to a comma-separated values (csv) file2.hdf5优先选择使用HDF5文件存储 HDF5在存储的是支持压缩，使用的方式是blosc，这个是速度最快的也是pandas默认支持的 使用压缩可以提磁盘利用率，节省空间 HDF5还是跨平台的，可以轻松迁移到hadoop 上面2.1.read_hdf()与to_hdf()HDF5文件的读取和存储需要指定一个键，值为要存储的DataFrame pandas.read_hdf(path_or_buf，key =None，** kwargs)从h5文件当中读取数据 path_or_buffer:文件路径 key:读取的键 mode:打开文件的模式 return:Theselected object DataFrame.to_hdf(path_or_buf, key, \kwargs) 需要安装安装tables模块避免不能读取HDF5文件1pip install tables ps:可以直接使用pandas.read_table()读取table 3.json3.1.read_json() pandas.read_json(path_or_buf=None, orient=None, typ=’frame’, lines=False) 将JSON格式准换成默认的Pandas DataFrame格式 orient : string,Indication of expected JSON string format. ‘split’ : dict like {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]} ‘records’ : list like [{column -&gt; value}, … , {column -&gt; value}] ‘index’ : dict like {index -&gt; {column -&gt; value}} ‘columns’ : dict like {column -&gt; {index -&gt; value}},默认该格式 ‘values’ : just the values array lines : boolean, default False 按照每行读取json对象 typ : default ‘frame’， 指定转换成的对象类型series或者dataframe 3.2.to_json() DataFrame.to_json(path_or_buf=None, orient=None, lines=False) 将Pandas 对象存储为json格式 path_or_buf=None：文件地址 orient:存储的json形式，{‘split’,’records’,’index’,’columns’,’values’} lines:一个对象存储为一行]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——Pandas画图]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Pandas%E7%94%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——Pandas画图Pandas画图1.pandas.DataFrame.plot DataFrame.plot(x=None, y=None, kind=’line’) x : label or position, default None y : label, position or list of label, positions, default None Allows plotting of one column versus another kind : str ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘pie’ : pie plot ‘scatter’ : scatter plot2.pandas.Series.plot]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——DataFrame运算]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94DataFrame%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——DataFrame运算DataFrame运算1.算术运算 add(other) 比如进行数学运算加上具体一个数字 也可以加上某一列 sub(other) 比如进行数学运算加上具体一个数字 也可以减去某一列2.逻辑运算2.1.逻辑运算符号&lt;、&gt;、|、&amp;、== ==等于 大于 &lt;小于 |或 &amp;且 2.2.逻辑运算函数 query(expr) expr:查询字符串(查询条件，如’column01 &gt; 1 &amp; column02 &lt; 2’) isin(values) values:tuple,list,ndarray,series 3.统计运算3.1.describe()综合分析: 能够直接得出很多统计结果,count, mean, std, min, max 等12# 计算平均值、标准差、最大值、最小值df.describe 3.2.统计函数 count Number of non-NA observations sum Sum of values mean Mean of values median Arithmetic median of values min Minimum max Maximum mode Mode abs Absolute Value prod Product of values std Bessel-corrected sample standard deviation var Unbiased variance idxmax compute the index labels with the maximum idxmin compute the index labels with the minimum nunique repeat for all columns 对于单个函数去进行统计的时候，坐标轴还是按照这些默认为“columns” (axis=0, default)，如果要对行“index” 需要指定(axis=1)。例如1234567# 对列求结果df.min(0)# 对行求结果df.max(0)# 对于一位数组或者列表，unique函数去重，并按元素由大到小返回一个新的无重复的元组或者列表pd.unique(df[column])df.nunique()[column] 4.累计统计函数 函数 作用 cumsum 计算前1/2/3/…/n个数的和 cummax 计算前1/2/3/…/n个数的最大值 cummin 计算前1/2/3/…/n个数的最小值 cumprod 计算前1/2/3/…/n个数的积 12345678910111213&gt;&gt;&gt; df.cumsum() month year sale0 1 2012 551 2 4026 952 3 6039 1793 4 8053 210&gt;&gt;&gt; df['year'].cumsum()0 20121 40262 60393 8053Name: year, dtype: int64 5.自定义运算 apply(func,axis=0) func:自定义函数 axis=0:默认是列，axis=1为行进行运算 123456789101112&gt;&gt;&gt; df month year sale test0 1 2012 55 20141 1 2014 40 20142 1 2013 84 20143 1 2014 31 2014# 指定列 最大值-最小值&gt;&gt;&gt; df[['year','test']].apply(lambda x:x.max()-x.min(), axis=0)year 2test 0dtype: int64]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——基本数据操作]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——基本数据操作基本数据操作1.索引操作Numpy当中我们已经讲过使用索引选取序列和切片选择，pandas也支持类似的操作，也可以直接使用列名、行名称，甚至组合使用。 1.1.直接使用行列索引(先列后行)1234567891011121314151617&gt;&gt;&gt; df = pd.DataFrame(&#123;'month': [1, 4, 7, 10],'year': [2012, 2014, 2013, 2014],'sale':[55, 40, 84, 31]&#125;)&gt;&gt;&gt; df month year sale0 1 2012 551 4 2014 402 7 2013 843 10 2014 31&gt;&gt;&gt; df['month']0 11 42 73 10Name: month, dtype: int64&gt;&gt;&gt; df['month'][1]4 1.2.结合loc或者iloc使用索引 loc 12345# 使用loc:只能指定行列索引的名字&gt;&gt;&gt; df.loc['month':'sales']Empty DataFrameColumns: [month, year, sale]Index: [] iloc 123456# 使用iloc可以通过索引的下标去获取&gt;&gt;&gt; df.iloc[0:3, 0:2] month year0 1 20121 4 20142 7 2013 1.3.使用ix组合索引 ix123456789101112131415161718192021222324252627282930&gt;&gt;&gt; df.ix[0:4,['month']] month0 11 42 73 10&gt;&gt;&gt; df.ix[0:4,['month','year']] month year0 1 20121 4 20142 7 20133 10 2014# 使用loc与iloc也可以完成组合索引# loc&gt;&gt;&gt; df.loc[df.index[0:4], ['month','year']] month year0 1 20121 4 20142 7 20133 10 2014# iloc&gt;&gt;&gt; df.iloc[0:4, df.columns.get_indexer(['month','year'])] month year0 1 20121 4 20142 7 20133 10 2014 2.赋值操作 与字典操作类似，可以新增也可以修改1234567891011121314151617181920212223242526272829303132333435# 新增&gt;&gt;&gt; df['test'] = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 4 2014 40 12 7 2013 84 13 10 2014 31 1&gt;&gt;&gt; df.test = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 4 2014 40 12 7 2013 84 13 10 2014 31 1df.test = 1# 修改&gt;&gt;&gt; df['month'] = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 1 2014 40 12 1 2013 84 13 1 2014 31 1&gt;&gt;&gt; df.month = 1&gt;&gt;&gt; df month year sale test0 1 2012 55 11 1 2014 40 12 1 2013 84 13 1 2014 31 1 3.排序排序有两种形式，一种对内容进行排序，一种对索引进行排序DataFrame 使用df.sort_values(by=column01, ascending=)对内容进行排序 单个键或者多个键进行排序,默认升序 ascending=False:降序 ascending=True:升序 使用df.sort_index对索引进行排序Series 使用series.sort_values(ascending=True)对内容进行排序 series排序时，只有一列，不需要参数 使用series.sort_index()对索引进行排序]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——MultiIndex、Panel与Series]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94MultiIndex%E3%80%81Panel%E4%B8%8ESeries%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——MultiIndex、Panel与Series1234567891011121314151617181920&gt;&gt;&gt; df = pd.DataFrame(&#123;'month': [1, 4, 7, 10],'year': [2012, 2014, 2013, 2014],'sale':[55, 40, 84, 31]&#125;)&gt;&gt;&gt; df month year sale0 1 2012 551 4 2014 402 7 2013 843 10 2014 31&gt;&gt;&gt; df = df.set_index(['year', 'month'])&gt;&gt;&gt; df saleyear month 2012 1 552014 4 402013 7 842014 10 31&gt;&gt;&gt; df.indexMultiIndex(levels=[[2012, 2013, 2014], [1, 4, 7, 10]], labels=[[0, 2, 1, 2], [0, 1, 2, 3]], names=['year', 'month']) 1.MultiIndex多级或分层索引对象。 index属性 names：levels的名称 levels：每个level的元组值12345&gt;&gt;&gt; df.index.namesFrozenList(['year', 'month'])&gt;&gt;&gt; df.index.levelsFrozenList([[2012, 2013, 2014], [1, 4, 7, 10]]) 2.Pannel class pandas.Panel(data=None, items=None, major_axis=None, minor_axis=None, copy=False, dtype=None) 存储3维数组的Panel结构1234567&gt;&gt;&gt; p = pd.Panel(np.arange(24).reshape(4,3,2),items=list('ABCD'),major_axis=pd.date_range('20130101', periods=3),minor_axis=['first', 'second'])&gt;&gt;&gt; p&lt;class 'pandas.core.panel.Panel'&gt;Dimensions: 4 (items) x 3 (major_axis) x 2 (minor_axis)Items axis: A to DMajor_axis axis: 2013-01-01 00:00:00 to 2013-01-03 00:00:00Minor_axis axis: first to second items - axis 0，每个项目对应于内部包含的数据帧(DataFrame)。 major_axis - axis 1，它是每个数据帧(DataFrame)的索引(行)。 minor_axis - axis 2，它是每个数据帧(DataFrame)的列。 3.Series series结构只有行索引3.1.创建Series通过已有数据创建 指定内容，默认索引 123456789101112&gt;&gt;&gt; pd.Series(np.arange(10))0 01 12 23 34 45 56 67 78 89 9dtype: int64 指定索引 1234567&gt;&gt;&gt; pd.Series(['a', 'b', 'c', 'd', 'e'], index=[1, 2, 3, 4, 5])1 a2 b3 c4 d5 edtype: object 通过字典数据创建123456&gt;&gt;&gt; pd.Series(&#123;'a':'1', 'b':2, 'c': 3, 'd':4&#125;)a 1b 2c 3d 4dtype: object 3.2.Series获取索引和值 index values]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记——DataFrame]]></title>
    <url>%2F2017%2F03%2F17%2FPandas%2FPandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94DataFrame%2F</url>
    <content type="text"><![CDATA[Pandas学习笔记——DataFrame1.DataFrame123456789101112131415&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; a = np.random.normal(0,1,10).reshape(2,5)&gt;&gt;&gt; aarray([[ 0.27370548, -0.33756615, 1.56610377, 1.16995613, -0.13634255], [-0.15440603, -0.87418791, -0.93990267, 0.81535022, 0.25877249]]) &gt;&gt;&gt; b = pd.DataFrame(a)&gt;&gt;&gt; type(b)&lt;class 'pandas.core.frame.DataFrame'&gt;&gt;&gt;&gt; b 0 1 2 3 40 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772 1.1.DataFrame结构DataFrame对象既有行索引，又有列索引 行索引，表明不同行，横向索引，叫index 列索引，表名不同列，纵向索引，叫columns1.2.DataFrame的属性与方法常用属性 shape 12&gt;&gt;&gt; b.shape(2, 5) index 12&gt;&gt;&gt; b.indexRangeIndex(start=0, stop=2, step=1) columns 12&gt;&gt;&gt; b.columnsRangeIndex(start=0, stop=5, step=1) values 123&gt;&gt;&gt; b.valuesarray([[ 0.27370548, -0.33756615, 1.56610377, 1.16995613, -0.13634255], [-0.15440603, -0.87418791, -0.93990267, 0.81535022, 0.25877249]]) T 1234567&gt;&gt;&gt; b.T 0 10 0.273705 -0.1544061 -0.337566 -0.8741882 1.566104 -0.9399033 1.169956 0.8153504 -0.136343 0.258772 常用方法 head(5):显示前5行内容 如果不补充参数，默认5行。填入参数N则显示前N行123&gt;&gt;&gt; b.head(1) 0 1 2 3 40 0.273705 -0.337566 1.566104 1.169956 -0.136343 tail(5):显示后5行内容 如果不补充参数，默认5行。填入参数N则显示后N行123&gt;&gt;&gt; b.tail(1) 0 1 2 3 41 -0.154406 -0.874188 -0.939903 0.81535 0.258772 DataFrame索引的设置 设置或修改行索引值 必须整体设置或修改(使用列表或元组)1234567891011121314# 修改index&gt;&gt;&gt; i = ['test1','test2']&gt;&gt;&gt; b.index = i&gt;&gt;&gt; b 0 1 2 3 4test1 0.273705 -0.337566 1.566104 1.169956 -0.136343test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772# 直接设置index索引&gt;&gt;&gt; b = pd.DataFrame(a,index=i)&gt;&gt;&gt; b 0 1 2 3 4test1 0.273705 -0.337566 1.566104 1.169956 -0.136343test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772 设置或修改列索引值 必须整体设置或修改(使用列表或元组)12345678910111213&gt;&gt;&gt; c = ['test1','test2','test3','test4','test5']&gt;&gt;&gt; b.columns = c&gt;&gt;&gt; b test1 test2 test3 test4 test50 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772# 直接设置columns索引&gt;&gt;&gt; b = pd.DataFrame(a,columns=c)&gt;&gt;&gt; b test1 test2 test3 test4 test50 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772 重设索引 reset_index(drop=False) 设置新的下标索引 drop:默认为False，不删除原来索引，如果为True,删除原来的索引值1234567891011121314&gt;&gt;&gt; b test1 test2 test3 test4 test5test1 0.273705 -0.337566 1.566104 1.169956 -0.136343test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772&gt;&gt;&gt; b.reset_index() index test1 test2 test3 test4 test50 test1 0.273705 -0.337566 1.566104 1.169956 -0.1363431 test2 -0.154406 -0.874188 -0.939903 0.815350 0.258772&gt;&gt;&gt; b.reset_index(drop=True) test1 test2 test3 test4 test50 0.273705 -0.337566 1.566104 1.169956 -0.1363431 -0.154406 -0.874188 -0.939903 0.815350 0.258772 以某列值设置为新的索引 set_index(keys, drop=True) keys : 列索引名成或者列索引名称的列表 drop : boolean, default True.当做新的索引，删除原来的列1234567891011121314151617181920212223&gt;&gt;&gt; df = pd.DataFrame(&#123;'month': [1, 4, 7, 10],'year': [2012, 2014, 2013, 2014],'sale':[55, 40, 84, 31]&#125;)&gt;&gt;&gt; df month year sale0 1 2012 551 4 2014 402 7 2013 843 10 2014 31&gt;&gt;&gt; df.set_index('month') year salemonth 1 2012 554 2014 407 2013 8410 2014 31&gt;&gt;&gt; df.set_index(['year', 'month']) saleyear month 2012 1 552014 4 402013 7 842014 10 31]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——数组间运算]]></title>
    <url>%2F2017%2F03%2F12%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84%E9%97%B4%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——数组间运算1.数组与数的运算123456789101112&gt;&gt;&gt; A = np.array([[1,2,3,4,5],[6,7,8,9,10]])&gt;&gt;&gt; Aarray([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10]]) &gt;&gt;&gt; A + 1array([[ 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11]]) &gt;&gt;&gt; A / 2array([[0.5, 1. , 1.5, 2. , 2.5], [3. , 3.5, 4. , 4.5, 5. ]]) 2.数组与数组的运算2.1.广播机制数组与数组的运算涉及到numpy的广播机制，执行 broadcast 的前提在于，两个 ndarray 执行的是 element-wise的运算，Broadcast机制的功能是为了方便不同形状的ndarray（numpy库的核心数据结构）进行数学运算。当操作两个数组时，numpy会逐个比较它们的shape（构成的元组tuple），只有在下述情况下，两个数组才能够进行数组与数组的运算。 维度相等 shape（其中相对应的一个地方为1）1234567891011121314&gt;&gt;&gt; a = np.array([1,2,3])# 查看a的维度&gt;&gt;&gt; a.shape(3,)&gt;&gt;&gt; b = np.array([[1,],[2,],[3]])# 查看b的维度&gt;&gt;&gt; b.shape(3, 1)&gt;&gt;&gt; b - aarray([[ 0, -1, -2], [ 1, 0, -1], [ 2, 1, 0]]) 根据广播原则，b满足其中一方轴长度为1，那么广播会沿着长度为1的轴，及axis=1进行，对数组b沿着axis=1即水平方向进行复制，相当于b变成一个shape为(3,3)且各列均为[1,2,3]的数组，一个维度为(3,3)的数组减去一个维度为(3,)的数组，满足后缘维度轴长度相等,数组a沿着axis=0即竖直方向进行广播，相当远a变成一个shape为(3,3)且个行均为[1,2,3]的数组。相减的时候，b被广播成为$$\begin{bmatrix}1&amp;1&amp;1\\2&amp;2&amp;2\\3&amp;3&amp;3\end{bmatrix}$$a被广播为$$\begin{bmatrix}1&amp;2&amp;3\\1&amp;2&amp;3\\1&amp;2&amp;3\end{bmatrix}$$结果为$$\begin{bmatrix}0&amp;-1&amp;-2\\1&amp;0&amp;-1\\2&amp;1&amp;0\end{bmatrix}$$ 2.2.乘法运算 a * b 对ndarray对象进行对应位置相乘 对matrix对象进行矩阵乘法 np.multiply(a ,b) 对ndarray对象和matrix对象都进行对应位置相乘 np.matmul 对ndarray对象和matrix对象 np.dot 对ndarray对象和matrix对象 a.dot(b) 对ndarray对象和matrix对象 a @ b 对ndarray对象和matrix对象 ndarray对象123456789101112131415161718192021222324&gt;&gt;&gt; a = np.array([1,2,3])&gt;&gt;&gt; b = np.array([[1,],[2,],[3]])# a,b矩阵由于广播机制，已被广播为3x3的数组&gt;&gt;&gt; a * barray([[1, 2, 3], [2, 4, 6], [3, 6, 9]]) &gt;&gt;&gt; np.multiply(a, b)array([[1, 2, 3], [2, 4, 6], [3, 6, 9]]) &gt;&gt;&gt; np.matmul(a, b)array([14])&gt;&gt;&gt; np.dot(a, b)array([14])&gt;&gt;&gt; a.dot(b)array([14])&gt;&gt;&gt; a @ barray([14]) matrix对象123456789101112131415161718192021&gt;&gt;&gt; A = np.matrix([1,2,3])&gt;&gt;&gt; B = np.matrix([[1,],[2,],[3]])&gt;&gt;&gt; A*Bmatrix([[14]])&gt;&gt;&gt; np.multiply(A, B)matrix([[1, 2, 3], [2, 4, 6], [3, 6, 9]]) &gt;&gt;&gt; np.matmul(A ,B)matrix([[14]])&gt;&gt;&gt; np.dot(A, B)matrix([[14]])&gt;&gt;&gt; A.dot(B)matrix([[14]])&gt;&gt;&gt; A @ Bmatrix([[14]]) 3.合并与分割3.1.合并 numpy.concatenate((a1, a2, …), axis=0) numpy.hstack(tup) Stack arrays in sequence horizontally (column wise). numpy.vstack(tup) Stack arrays in sequence vertically (row wise). np.concatenate() axis=0以列方式进行合并 axis=1以行方式进行合并12345678910&gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])&gt;&gt;&gt; b = np.array([[5, 6]])&gt;&gt;&gt; np.concatenate((a, b), axis=0)array([[1, 2], [3, 4], [5, 6]]) &gt;&gt;&gt; np.concatenate((a, b.T), axis=1)array([[1, 2, 5], [3, 4, 6]]) np.hstack() 以行方式进行合并1234567891011&gt;&gt;&gt; a = np.array((1,2,3))&gt;&gt;&gt; b = np.array((2,3,4))&gt;&gt;&gt; np.hstack((a, b))array([1, 2, 3, 2, 3, 4])&gt;&gt;&gt; a = np.array([[1],[2],[3]])&gt;&gt;&gt; b = np.array([[2],[3],[4]])&gt;&gt;&gt; np.hstack((a, b))array([[1, 2], [2, 3], [3, 4]]) np.vstack() 以列方式进行合并 123456789101112131415&gt;&gt;&gt; a = np.array([1, 2, 3])&gt;&gt;&gt; b = np.array([2, 3, 4])&gt;&gt;&gt; np.vstack((a, b))array([[1, 2, 3], [2, 3, 4]])&gt;&gt;&gt; a = np.array([[1], [2], [3]])&gt;&gt;&gt; b = np.array([[2], [3], [4]])&gt;&gt;&gt; np.vstack((a, b))array([[1], [2], [3], [2], [3], [4]]) 3.2.分割 numpy.split(ary, indices_or_sections, axis=0) Split an array into multiple sub-arrays. 1234567891011&gt;&gt;&gt; a = np.linspace(1,10,10)&gt;&gt;&gt; aarray([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])# np.split(a, int) int必须是len(a)的因数&gt;&gt;&gt; np.split(a, 2)[array([1., 2., 3., 4., 5.]), array([ 6., 7., 8., 9., 10.])]# np.split(a, [index_0,index_1,index_2...])&gt;&gt;&gt; np.split(a,[1,2,5])[array([[1, 2, 3, 4]]), array([[5, 6, 7, 8]]), array([], shape=(0, 4), dtype=int64), array([], shape=(0, 4), dtype=int64)]]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——ndarray运算]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94ndarray%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——ndarray运算1.逻辑运算1234567891011121314# 生成数据&gt;&gt;&gt; A = np.random.normal(0,1,(2,2))&gt;&gt;&gt; Aarray([[-0.7572373 , 2.13520462], [-0.72749164, -1.20845879]])# 逻辑判断，如果数据大于0.5就标记为True，否则为False&gt;&gt;&gt; A &gt; 0.5array([[False, True], [False, False]])# Bool赋值，将满足条件的设置为指定的值-布尔索引&gt;&gt;&gt; A[A &gt; 0.5] = 1&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]]) 2.通用判断函数 np.all() 123456&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]])# 判断A中所有元素是否全是大于0&gt;&gt;&gt; np.all(A &gt; 0)False np.any() 123456&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]])# 判断A中是否有元素大于0&gt;&gt;&gt; np.any(A &gt; 0)True 3.三元运算符 np.where1234567&gt;&gt;&gt; Aarray([[-0.7572373 , 1. ], [-0.72749164, -1.20845879]])# 判断A中的元素 小于-1的置为1，否则置为0&gt;&gt;&gt; np.where(A &lt; -1, 1, 0 )array([[0, 0], [0, 1]]) 4.统计运算 np.min(a[, axis, out, keepdims]) Return the minimum of an array or minimum along an axis. np.max(a[, axis, out, keepdims]) Return the maximum of an array or maximum along an axis. np.median(a[, axis, out, overwrite_input, keepdims]) Compute the median along the specified axis. np.mean(a[, axis, dtype, out, keepdims]) Compute the arithmetic mean along the specified axis. np.std(a[, axis, dtype, out, ddof, keepdims]) Compute the standard deviation along the specified axis. np.var(a[, axis, dtype, out, ddof, keepdims]) Compute the variance along the specified axis.]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——基本操作]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——基本操作1.生成0和1的数组 empty(shape[, dtype, order]) empty_like(a[, dtype, order, subok])eye(N[, M, k, dtype, order]) identity(n[, dtype]) ones(shape[, dtype, order]) ones_like(a[, dtype, order, subok]) zeros(shape[, dtype, order]) zeros_like(a[, dtype, order, subok])full(shape, fill_value[, dtype, order]) full_like(a, fill_value[, dtype, order, subok])1234&gt;&gt;&gt; zero = np.zeros([3, 4])array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) 2.从现有数组生成 array(object[, dtype, copy, order, subok, ndmin]) asarray(a[, dtype, order]) asanyarray(a[, dtype, order]) ascontiguousarray(a[, dtype]) asmatrix(data[, dtype]) copy(a[, order])12345&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6]])# 从现有的数组当中创建&gt;&gt;&gt; a1 = np.array(a)# 相当于索引的形式，并没有真正的创建一个新的&gt;&gt;&gt; a2 = np.asarray(a) 关于array和asarray的不同12345678910111213141516171819202122&gt;&gt;&gt; data = np.ones([3,4])&gt;&gt;&gt; dataarray([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]])&gt;&gt;&gt; data1 = np.array(data)&gt;&gt;&gt; data2 = np.asarray(data)&gt;&gt;&gt; data1,data2(array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]), array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]))&gt;&gt;&gt; data[1] = 2&gt;&gt;&gt; data2array([[1., 1., 1., 1.], [2., 2., 2., 2.], [1., 1., 1., 1.]])&gt;&gt;&gt; data1array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) 3.生成固定范围的数组 np.linspace (start, stop, num, endpoint, retstep, dtype) 生成等间隔的序列 start 序列的起始值stop 序列的终止值，如果endpoint为true，该值包含于序列中num 要生成的等间隔样例数量，默认为50endpoint 序列中是否包含stop值，默认为tureretstep 如果为true，返回样例，以及连续数字之间的步长dtype 输出ndarray的数据类型 12生成等间隔的数组np.linspace(0, 100, 10) 返回结果：123array([ 0. , 11.11111111, 22.22222222, 33.33333333, 44.44444444, 55.55555556, 66.66666667, 77.77777778, 88.88888889, 100. ]) 其他的还有 numpy.arange(start,stop, step, dtype) numpy.logspace(start,stop, num, endpoint, base, dtype) 1np.arange(10, 50, 2) 返回结果：1array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42,44, 46, 48]) 4.生成随机数组 np.random模块 均匀分布 np.random.rand(d0, d1, …, dn)返回[0.0，1.0)内的一组均匀分布的数。 np.random.uniform(low=0.0, high=1.0, size=None)功能：从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high.参数介绍:low: 采样下界，float类型，默认值为0；high: 采样上界，float类型，默认值为1；size: 输出样本数目，为int或元组(tuple)类型，例如，size=(m,n,k), 则输出mnk个样本，缺省时输出1个值。返回值：ndarray类型，其形状和参数size中描述一致。 np.random.randint(low, high=None, size=None, dtype=’l’)从一个均匀分布中随机采样，生成一个整数或N维整数数组，取数范围：若high不为None时，取[low,high)之间随机整数，否则取值[0,low)之间随机整数。 正态分布 np.random.randn(d0, d1, …, dn)功能：从标准正态分布中返回一个或多个样本值 np.random.normal(loc=0.0, scale=1.0, size=None)loc：float此概率分布的均值（对应着整个分布的中心centre）scale：float​ 此概率分布的标准差（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高）size：int or tuple of ints输出的shape，默认为None，只输出一个值 np.random.standard_normal(size=None)返回指定形状的标准正态分布的数组。 均匀分布均匀分布（Uniform Distribution）是概率统计中的重要分布之一。顾名思义，均匀，表示可能性相等的含义。均匀分布在自然情况下极为罕见，而人工栽培的有一定株行距的植物群落即是均匀分布。12345# 生成均匀分布的随机数&gt;&gt;&gt; X = np.random.uniform(-1, 1, 100000000)&gt;&gt;&gt; Xarray([ 0.22411206, 0.31414671, 0.85655613, ..., -0.92972446,0.95985223, 0.23197723]) 正态分布 1.什么是正态分布正态分布是一种概率分布。正态分布是具有两个参数$\mu$和$\sigma$的连续型随机变量的分布，第一参数$\mu$是服从正态分布的随机变量的均值，第二个参数$\sigma$是此随机变量的方差，所以正态分布记作$N(\mu, \sigma )$。 2.正态分布的应用生活、生产与科学实验中很多随机变量的概率分布都可以近似地用正态分布来描述。 3.正态分布的特点$\mu$决定了其位置，其标准差$sigma$。决定了分布的幅度。当$\mu = 0$，$\sigma = 1$时的正态分布是标准正态分布。$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ 3.1.方差和标准差在概率论和统计学中衡量一组数据离散程度的度量方差：$$\sigma^2=\frac{1}{N}\sum\limits_{i=1}{N}(x_i-\mu)^2$$标准差：$$\sigma^2=\sqrt{\frac{1}{N}\sum\limits_{i=1}{N}(x_i-\mu)^2}$$ 3.2.方差和标准差的意义可以理解成数据的一个离散程度的衡量 5.数组的索引、切片1234567891011# 三维数组&gt;&gt;&gt; A = np.array([ [[1,2,3],[4,5,6]], [[12,3,34],[5,6,7]]])&gt;&gt;&gt; Aarray([[[ 1, 2, 3], [ 4, 5, 6]], [[12, 3, 34], [ 5, 6, 7]]])# 索引、切片&gt;&gt;&gt; A[0, 0, 1]2 6.形状修改 ndarray.reshape(shape[, order]) Returns an array containing the same data with a new shape. ndarray.T 数组的转置 ndarray.resize(new_shape[, refcheck]) Change shape and size of array in-place. 7.类型修改 ndarray.astype(type) ndarray.tostring([order])或者ndarray.tobytes([order]) Construct Python bytes containing the raw data bytes in the array. 如果遇到 123456&gt; IOPub data rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_data_rate_limit`.&gt; 这个问题是在jupyer当中对输出的字节数有限制，需要去修改配置文件创建配置文件 123&gt; jupyter notebook --generate-configvi ~/.jupyter/jupyter_notebook_config.py&gt; 取消注释,多增加123&gt; ## (bytes/sec) Maximum rate at which messages can be sent on iopub before they are limited.c.NotebookApp.iopub_data_rate_limit = 10000000&gt; 但是不建议这样去修改，jupyter输出太大会崩溃 8.数组的去重 ndarray.unique123temp = np.array([[1, 2, 3, 4],[3, 4, 5, 6]])&gt;&gt;&gt; np.unique(temp)array([1, 2, 3, 4, 5, 6])]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——ndarray属性和类型]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94ndarray%E5%B1%9E%E6%80%A7%E5%92%8C%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——ndarray属性和类型ndarray属性 属性名字 属性解释 返回类型 ndarray.shape 数组维度 tuple ndarray.ndim 数组维度(空间) int ndarray.size 数组中的元素数量 int ndarray.itemsize 一个数组元素的长度(字节) int ndarray.dtype 数组元素的类型 dtype object ndarray类型 名称 描述 简写 np.bool 用一个字节存储的布尔类型（True或False） ‘b’ np.int8 一个字节大小，-128 至 127 ‘i’ np.int16 整数，-32768 至 32767 ‘i2’ np.int32 整数，-2 31 至 2 32 -1 ‘i4’ np.int64 整数，-2 63 至 2 63 - 1 ‘i8’ np.uint8 无符号整数，0 至 255 ‘u’ np.uint16 无符号整数，0 至 65535 ‘u2’ np.uint32 无符号整数，0 至 2 ** 32 - 1 ‘u4’ np.uint64 无符号整数，0 至 2 ** 64 - 1 ‘u8’ np.float16 半精度浮点数：16位，正负号1位，指数5位，精度10位 ‘f2’ np.float32 单精度浮点数：32位，正负号1位，指数8位，精度23位 ‘f4’ np.float64 双精度浮点数：64位，正负号1位，指数11位，精度52位 ‘f8’ np.complex64 复数，分别用两个32位浮点数表示实部和虚部 ‘c8’ np.complex128 复数，分别用两个64位浮点数表示实部和虚部 ‘c16’ np.object_ python对象 ‘O’ np.string_ 字符串 ‘S’ np.unicode_ unicode类型 ‘U’]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy学习笔记——Numpy介绍]]></title>
    <url>%2F2017%2F03%2F11%2FNumpy%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Numpy%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Numpy学习笔记——Numpy介绍Numpy介绍Numpy（Numerical Python）是一个开源的Python科学计算库，用于快速处理任意维度的数组。Numpy支持常见的数组和矩阵操作。对于同样的数值计算任务，使用Numpy比直接使用Python要简洁的多。Numpy使用ndarray对象来处理多维数组，该对象是一个快速而灵活的大数据容器。 ndarray介绍1234import numpy as np# 创建ndarraya = np.array([[1, 2], [3, 4]]) 返回结果为ndarray对象12array([[1, 2], [3, 4]]) ndarray的优势1.内存块风格ndarray在存储数据的时候，数据与数据的地址都是连续的，这样就给使得批量操作数组元素时速度更快。ndarray中的所有元素的类型都是相同的，而Python列表中的元素类型是任意的，所以ndarray在存储元素时内存可以连续，而python原生lis就t只能通过寻址方式找到下一个元素，这虽然也导致了在通用性能方面Numpy的ndarray不及Python原生list，但在科学计算中，Numpy的ndarray就可以省掉很多循环语句，代码使用方面比Python原生list简单的多。 2.支持并行化运算ndarray支持并行化运算，即支持向量化运算。 3.底层运算Numpy底层使用C语言编写，内部解除了GIL(全局解释器锁)，其对数组的操作速度不受Python解释器的限制，效率远高于纯Python代码。]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——饼图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F08%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%A5%BC%E5%9B%BE%E7%BB%98%E5%88%B6(pie)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——饼图绘制(pie)饼图广泛得应用在各个领域，用于表示不同分类的占比情况，通过弧度大小来对比各种分类。饼图通过将一个圆饼按照分类的占比划分成多个区块，整个圆饼代表数据的总量，每个区块（圆弧）表示该分类占总体的比例大小，所有区块（圆弧）的加和等于 100%。 1.饼图绘制与显示 注意显示的百分比的位数 plt.pie(x, labels=,autopct=,colors) x:数量，自动算百分比 labels:每部分名称 autopct:占比显示指定%1.2f%% colors:每部分颜色123456789101112import matplotlib.pylab as plt# 1.准备数据x = ['A', 'B', 'C', 'D', 'E']y = [1984, 3514, 4566, 7812, 1392]# 2.创建画布plt.figure(figsize=(8, 6), dpi=100)# 3.绘制饼图plt.pie(y, labels=x, autopct="%1.2f%%", colors=['b','r','g','y','c'])# 显示图例plt.legend()# 4.显示图像plt.show() 2.圆形饼图在plt.show()前添加代码1plt.axis('equal') 3.饼图应用场景 分类的占比情况（不超过9个分类）例如：班级男女分布占比，公司销售额占比]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——直方图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%BB%98%E5%88%B6(histogram)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——直方图绘制(histogram)直方图，形状类似柱状图却有着与柱状图完全不同的含义。直方图涉及统计学概念，首先要对数据进行分组，然后统计每个分组内数据元的数量。在坐标系中，横轴标出每个组的端点，纵轴表示频数，每个矩形的高代表对应的频数，称这样的统计图为频数分布直方图。 1.相关概念 组数：在统计数据时，我们把数据按照不同的范围分成几个组，分成的组的个数称为组数 组距：每一组两个端点的差 高度：表示频数 面积：表示数量 2.直方图与柱状图的对比 柱状图是以矩形的长度表示每一组的频数或数量，其宽度(表示类别)则是固定的，利于较小的数据集分析。 直方图是以矩形的长度表示每一组的频数或数量，宽度则表示各组的组距，因此其高度与宽度均有意义，利于展示大量数据集的统计结果。 由于分组数据具有连续性，直方图的各矩形通常是连续排列，而柱状图则是分开排列。 3.直方图绘制与显示 matplotlib.pyplot.hist(x, bins=None, normed=None, **kwargs) Parameters:x : (n,) array or sequence of (n,) arrays bins : integer or sequence or ‘auto’, optional 设置组距 设置组数（通常对于数据较少的情况，分为5~12组，数据较多，更换图形显示方式） 通常设置组数会有相应公式：组数 = 极差/组距= (max-min)/distance1234567891011121314151617# 1.准备数据x = [1, 2, 1, 2, 3, 4, 5, 7, 7, 8, 3, 4, 5, 2, 4, 7, 8, 9, 0 ,9, 8, 0, 8, 8, 0, 7, 4, 8, 8, 9, 9, 9, 7, 8, 9, 5, 3]# 2.创建画布plt.figure(figsize=(8, 6), dpi=100)# 3.绘制直方图# 设置组距distance = 1# 计算组数bins = int((max(x) - min(x)) / distance)# 绘制直方图plt.hist(x, bins=bins)# 修改x轴刻度显示plt.xticks(range(min(x), max(x))[::1])# 添加网格显示plt.grid(linestyle=&quot;--&quot;, alpha=0.5)# 4.显示图像plt.show() 4.直方图的应用场景 用于表示分布情况 通过直方图还可以观察和估计哪些数据比较集中，异常或者孤立的数据分布在何处例如：用户年龄分布，商品价格分布]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——柱状图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9F%B1%E7%8A%B6%E5%9B%BE%E7%BB%98%E5%88%B6(bar)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——柱状图绘制(bar)1.柱状图绘制与显示 plt.bar(x, width, align=’center’, **kwargs) Parameters:x : sequence of scalars. width : scalar or array-like, optional柱状图的宽度 align : {‘center’, ‘edge’}, optional, default: ‘center’Alignment of the bars to the x coordinates:‘center’: Center the base on the x positions.‘edge’: Align the left edges of the bars with the x positions.每个柱状图的位置对齐方式 **kwargs :color:选择柱状图的颜色 Returns:.BarContainerContainer with all the bars and optionally errorbars.123456789101112import matplotlib.pylab as plt# 1.准备数据x = ['A', 'B', 'C', 'D', 'E']y = [1984, 3514, 4566, 7812, 1392]# 2.创建画布plt.figure(figsize=(8,6),dpi=100)# 3.绘制柱状图plt.bar(x, y, width=0.5, color=['b','r','g','y','c'])# 添加网格显示plt.grid(linestyle="--", alpha=0.5)# 4.显示图像plt.show() 2.柱状图对比1234567891011121314151617181920import matplotlib.pylab as plt# 1.准备数据A = ['A', 'B', 'C', 'D', 'E']x = range(len(A))y = [1984, 3514, 4566, 7812, 1392]x_ = [i+0.2 for i in x]y_ = [3154, 1571, 4566, 9858, 2689]# 2.创建画布plt.figure(figsize=(8,6),dpi=100)# 3.绘制柱状图plt.bar(x, y, width=0.2, label='1')plt.bar(x_,y_, width=0.2, label='2')# 显示图例plt.legend()# 修改x轴刻度显示plt.xticks([i+0.1 for i in x], A)# 添加网格显示plt.grid(linestyle="--", alpha=0.5)# 4.显示图像plt.show() 柱状图应用场景适合用在分类数据对比场景上 数量统计 用户数量对比分析]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——散点图绘制(scatter)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%95%A3%E7%82%B9%E5%9B%BE%E7%BB%98%E5%88%B6(scatter)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——散点图绘制(scatter)1.散点图绘制与显示12345678910import matplotlib.pylab as plt# 1.准备数据x = [6.1101,5.5277,8.5186,7.0032,5.8598,8.3829,7.4764,8.5781,6.4862,5.0546,5.7107,14.164,]y = [17.592,9.1302,13.662,11.854,6.8233,11.886,4.3483,12,6.5987,3.8166,3.2522,15.505]# 2.创建画布plt.figure(figsize=(8, 6), dpi=100)# 3.绘制散点图plt.scatter(x, y)# 4.显示图像plt.show() 2.散点形状修改代码12# marker:str,可以更改散点的形状plt.scatter(x, y, marker='x') 3.应用场景 探究不同变量之间的内在关系]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——折线图绘制(plot)]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%8A%98%E7%BA%BF%E5%9B%BE%E7%BB%98%E5%88%B6%E4%B8%8E%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87(plot)%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——折线图绘制(plot)1.matplotlib.pyplot模块matplotlib.pytplot包含了一系列类似于matlab的画图函数。 它的函数作用于当前图形(figure)的当前坐标系(axes)。1import matplotlib.pyplot as plt 2.折线图绘制与显示12345678910111213# 1.创建画布(容器层)# figsize:指定图的长宽# dpi:图像的清晰度# plt.figure()返回fig对象plt.figure(figsize=(6, 4), dpi=100)# 2.绘制折线图(图像层)# 需要保证x,y维度一致# label:str,标签名x = [1, 2, 3, 4, 5, 6, 7]y = [4, 5, 6, 8, 9, 2, 3]plt.plot(x, y, label='A')# 3.显示图像plt.show() 3.图片保存1234# 1.创建画布，并设置画布属性plt.figure(figsize=(20, 8), dpi=80)# 2.保存图片到指定路径plt.savefig(path) 注意:plt.show()会释放figure资源，如果在显示图像之后保存图片将只能保存空图片。 4.添加自定义x,y刻度 plt.xticks(x, **kwargs)x:要显示的刻度值 plt.yticks(y, **kwargs)y:要显示的刻度值 在plt.show()前添加代码1234567# 构造x轴刻度标签x_ticks = range(10)# 构造y轴刻度y_ticks = range(10)# 修改x,y轴坐标的刻度显示plt.xticks(x_ticks[::2])plt.yticks(y_ticks[::2]) 5.添加网格显示在plt.show()前添加代码1234# True:显示网格# linestyle:str,网格线条形状# alpha:int,0到1,透明度plt.grid(True, linestyle='--', alpha=0.5) 6.添加描述信息添加x轴、y轴描述信息及标题在plt.show()前添加代码123456# 设置x轴描述信息plt.xlabel("x")# 设置y轴描述信息plt.ylabel("y")# 设置z轴描述信息plt.title("title") 注意:若使用中文需根据各操作系统添加中文支持 7.多次plot在plt.show()前添加代码123x_ = [1, 2, 3, 4, 5, 6, 7]y_ = [2, 3, 4, 2, 1, 0, 9]plt.plot(x_, y_, label="B") 8.设置图形风格 颜色字符 风格字符 r 红色 - 实线 g 绿色 - - 虚线 b 蓝色 -. 点划线 w 白色 : 点虚线 c 青色 ‘ ‘ 留空、空格 m 洋红 y 黄色 k 黑色 修改代码1plt.plot(x_, y_, 'r', linestyle='--', label="B") 9.显示图例在plt.show()前添加代码使用Location String1plt.legend(loc="best") 或者使用Location Code1plt.legend(loc=0) 颜色字符 风格字符 ‘best’ 0 ‘upper right’ 1 ‘upper left’ 2 ‘lower left’ 3 ‘lower right’ 4 ‘right’ 5 ‘center left’ 6 ‘center right’ 7 ‘lower center’ 8 ‘upper center’ 9 ‘center ‘ 10 折线图的应用场景 呈现公司产品(不同区域)每天活跃用户数 呈现app每天下载数量 呈现产品新功能上线后,用户点击次数随时间的变化 拓展：画各种数学函数图像 注意：plt.plot()除了可以画折线图，也可以用于画各种数学函数图像]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib学习笔记——Matplotlib介绍]]></title>
    <url>%2F2017%2F03%2F07%2FMatplotlib%2FMatplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Matplotlib%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Matplotlib学习笔记——Matplotlib介绍Matplotlib基本介绍 专门用于开发2D图表(包括3D图表) 使用起来及其简单 以渐进、交互式方式实现数据可视化 Matplotlib作用可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。 能将数据进行可视化,更直观的呈现 使数据更加客观、更具说服力 Matplotlib图像结构 Matplotlib三层结构1.容器层容器层主要由Canvas、Figure、Axes组成。Canvas是位于最底层的系统层，在绘图的过程中充当画板的角色，即放置画布(Figure)的工具。Figure是Canvas上方的第一层，也是需要用户来操作的应用层的第一层，在绘图的过程中充当画布的角色。Axes是应用层的第二层，在绘图的过程中相当于画布上的绘图区的角色。Figure:指整个图形(可以通过plt.figure()设置画布的大小和分辨率等)Axes(坐标系):数据的绘图区域Axis(坐标轴)：坐标系中的一条轴，包含大小限制、刻度和刻度标签特点为：一个figure(画布)可以包含多个axes(坐标系/绘图区)，但是一个axes只能属于一个figure。一个axes(坐标系/绘图区)可以包含多个axis(坐标轴)，包含两个即为2d坐标系，3个即为3d坐标系 2.辅助显示层辅助显示层为Axes(绘图区)内的除了根据数据绘制出的图像以外的内容，主要包括Axes外观(facecolor)、边框线(spines)、坐标轴(axis)、坐标轴名称(axis label)、坐标轴刻度(tick)、坐标轴刻度标签(tick label)、网格线(grid)、图例(legend)、标题(title)等内容。该层的设置可使图像显示更加直观更加容易被用户理解，但又不会对图像产生实质的影响。 3.图像层图像层指Axes内通过plot、scatter、bar、histogram、pie等函数根据数据绘制出的图像。 总结： Canvas（画板）位于最底层，用户一般接触不到 Figure（画布）建立在Canvas之上 Axes（绘图区）建立在Figure之上 坐标轴（axis）、图例（legend）等辅助显示层以及图像层都是建立在Axes之上 Matplotlib基本api]]></content>
      <categories>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook使用]]></title>
    <url>%2F2017%2F03%2F05%2FUtils%2FJupyter%20Notebook%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Jupyter Notebook使用使用pip命令安装 pip install jupyter 打开Jupyter Notebook # 终端输入jupyter notebook 本地notebook的默认URL为：http://localhost:8888想让notebook打开指定目录，只要进入此目录后执行命令即可 新建notebook文档 notebook的文档格式是.ipynb 内容界面操作-helloworld点击run 标题栏： 点击标题（如Untitled）修改文档名 菜单栏 导航-File-Download as，另存为其他格式 导航-Kernel Interrupt，中断代码执行（程序卡死时） Restart，重启Python内核（执行太慢时重置全部资源） Restart &amp; Clear Output，重启并清除所有输出 Restart &amp; Run All，重启并重新运行所有代码 cell操作cell：一对In Out会话被视作一个代码单元，称为cellJupyter支持两种模式： 编辑模式（Enter） 命令模式下回车Enter或鼠标双击cell进入编辑模式 可以操作cell内文本或代码，剪切／复制／粘贴移动等操作 命令模式（Esc） 按Esc退出编辑，进入命令模式 可以操作cell单元本身进行剪切／复制／粘贴／移动等操作 1.鼠标操作2.快捷键操作 两种模式通用快捷键 Shift+Enter，执行本单元代码，并跳转到下一单元 Ctrl+Enter，执行本单元代码，留在本单元 cell行号前的 * ，表示代码正在运行 命令模式：按ESC进入 Y，cell切换到Code模式 M，cell切换到Markdown模式 A，在当前cell的上面添加cell B，在当前cell的下面添加cell 双击D：删除当前cell Z，回退 L，为当前cell加上行号 &lt;!– Ctrl+Shift+P，对话框输入命令直接运行 快速跳转到首个cell，Crtl+Home 快速跳转到最后一个cell，Crtl+End –&gt; 编辑模式：按Enter进入 多光标操作：Ctrl键点击鼠标（Mac:CMD+点击鼠标） 回退：Ctrl+Z（Mac:CMD+Z） 重做：Ctrl+Y（Mac:CMD+Y) 补全代码：变量、方法后跟Tab键 为一行或多行代码添加/取消注释：Ctrl+/（Mac:CMD+/） 屏蔽自动输出信息：可在最后一条语句之后加一个分号]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Utils</tag>
        <tag>Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的拓扑排序]]></title>
    <url>%2F2016%2F12%2F15%2FPython%2F%E5%9B%BE%E7%9A%84%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Python有向无环图的拓扑排序拓扑排序的官方定义为：由某个集合上的一个偏序得到该集合上的一个全序，这个操作称之为拓扑排序。而个人认为，拓扑排序即是在图的基本遍历法上引入了入度的概念并围绕入度来实现的排序方法，拓扑排序与Python多继承中mro规则的排序类似，若想深入研究mro规则的C3算法，不妨了解一下 DAG(有向无环图) 的拓扑排序。 入度：指有向图中某节点被指向数目之和有向无环图：Directed Acyclic Graph，简称DAG，若熟悉机器学习则肯定对DAG不陌生，如ANN、DNN、CNN等则都是典型的DAG模型，对这类模型此处不再过多敷述，有兴趣的可以自行学习。 以一个有向无环图为例，如下图：123456789# 定义图结构graph = &#123;"A": ["B","C"],"B": ["D","E"],"C": ["D","E"],"D": ["F"],"E": ["F"],"F": [],&#125; 如图A的指向的元素为B、CB的指向的元素为D、EC的指向的元素为D、ED的指向的元素为FE的指向的元素为FF的指向的元素为空即A的入度为0，B的入度为1，C的入度为1，D的入度为2，E的入度为2，F的入度为2在DAG的拓扑排序中，每次都选取入度为 0 的点加入拓扑队列中，再删除与这一点连接的所有边。首先找到入度为0的点A，把A从队列中取出，同时添加到结果中并把A相关的指向移除，即B、C的入度减少1变为0并将B，C添加到队列中，再从队列首部取出入度为0的节点，以此类推，最后输出结果，完成DAG的拓扑排序。123456789101112131415161718192021222324def TopologicalSort(G):# 创建入度字典in_degrees = dict((u, 0) for u in G)# 获取每个节点的入度for u in G:for v in G[u]:in_degrees[v] += 1# 使用列表作为队列并将入度为0的添加到队列中Q = [u for u in G if in_degrees[u] == 0]res = []# 当队列中有元素时执行while Q:# 从队列首部取出元素u = Q.pop()# 将取出的元素存入结果中res.append(u)# 移除与取出元素相关的指向，即将所有与取出元素相关的元素的入度减少1for v in G[u]:in_degrees[v] -= 1# 若被移除指向的元素入度为0，则添加到队列中if in_degrees[v] == 0:Q.append(v)return resprint(TopologicalSort(graph)) 输出结果：1['A', 'C', 'B', 'E', 'D', 'F'] 代码输出结果与上述分析相符]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>图的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的BFS与DFS]]></title>
    <url>%2F2016%2F12%2F11%2FPython%2F%E5%9B%BE%E7%9A%84BFS%E4%B8%8EDFS%2F</url>
    <content type="text"><![CDATA[Python图的BFS与DFS BFS:Breadth First Search，广度优先搜索DFS:Depth First Search，深度优先搜索 BFS与DFS都属于图算法，BFS与DFS分别由队列和堆栈来实现，基本的定义与实现过程见前一篇文章，本篇文章基于树的BFS与DFS进行扩展，实现无向图(即没有指定方向的图结构)的BFS与DFS。以一个无向图为例，如下图：123456789# 定义图结构graph = &#123;"A": ["B","C"],"B": ["A", "C", "D"],"C": ["A", "B", "D","E"],"D": ["B", "C", "E","F"],"E": ["C", "D"],"F": ["D"],&#125; 如图A的相邻元素为B、CB的相邻元素为A、C、DC的相邻元素为A、B、D、ED的相邻元素为B、C、E、FE的相邻元素为C、DF的相邻元素为D BFS优先遍历当前节点的相邻节点，即若当前节点为A时，则继续遍历的节点为B和C；当A的所有相邻节点遍历完以后，再遍历A相邻节点B和C的所有相邻节点，以B为例，在遍历B的相邻节点时，由于A已被访问过，则需要标记为已访问，在遍历B的相邻节点时，不再需要访问A。以此类推，完成无向图的BFS。DFS优先遍历与当前节点0相邻的一个节点1，然后再访问与节点1相邻但与节点0不相邻的节点，即若当前节点为A，则继续遍历B或C，再访问与B或C节点相邻且与A节点不相邻的节点，即节点D或E，若没有未遍历过的相邻节点，则返回访问上一个有未被访问过相邻节点的节点进行访问，依此遍历整个图，完成无向图的DFS。代码中为了更直观地观察遍历顺序，采用直接打印遍历元素的方式输出遍历结果代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def BFS(graph,vertex):# 使用列表作为队列queue = []# 将首个节点添加到队列中queue.append(vertex)# 使用集合来存放已访问过的节点looked = set()# 将首个节点添加到集合中表示已访问looked.add(vertex)# 当队列不为空时进行遍历while(len(queue)&gt;0):# 从队列头部取出一个节点并查询该节点的相邻节点temp = queue.pop(0)nodes = graph[temp]# 遍历该节点的所有相邻节点for w in nodes:# 判断节点是否存在于已访问集合中,即是否已被访问过if w not in looked:# 若未被访问,则添加到队列中,同时添加到已访问集合中,表示已被访问queue.append(w)looked.add(w)print(temp,end=' ')def DFS(graph,vertex):# 使用列表作为栈stack = []# 将首个元素添加到队列中stack.append(vertex)# 使用集合来存放已访问过的节点looked = set()# 将首个节点添加到集合中表示已访问looked.add(vertex)# 当队列不为空时进行遍历while len(stack)&gt;0:# 从栈尾取出一个节点并查询该节点的相邻节点temp = stack.pop()nodes = graph[temp]# 遍历该节点的所有相邻节点for w in nodes:# 判断节点是否存在于已访问集合中,即是否已被访问过if w not in looked:# 若未被访问,则添加到栈中,同时添加到已访问集合中,表示已被访问stack.append(w)looked.add(w)print(temp,end=' ')# 由于无向图无根节点，则需要手动传入首个节点，此处以"A"为例print("BFS",end=" ")BFS(graph,"A")print("")print("DFS",end=" ")DFS(graph,"A") 输出结果：12BFS A B C D E F DFS A C E D F B 代码输出结果与上述分析相符]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>图的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树的BFS与DFS]]></title>
    <url>%2F2016%2F12%2F07%2FPython%2F%E6%A0%91%E7%9A%84BFS%E4%B8%8EDFS%2F</url>
    <content type="text"><![CDATA[Python树的BFS与DFS BFS:Breadth First Search，广度优先搜索DFS:Depth First Search，深度优先搜索 BFS与树的层序遍历类似，DFS则与树的后序遍历有着区别。 BFS(广度优先搜索)： 使用队列实现 每次从队列的头部取出一个元素，查看这个元素所有的下一级元素，再把它们放到队列的末尾。并把这个元素记为它下一级元素的前驱。 优先遍历取出元素下一级的同级元素 DFS(深度优先搜索): 使用栈实现 每次从栈的末尾弹出一个元素，搜索所有在它下一级的元素，把这些元素压入栈中。并把这个元素记为它下一级元素的前驱。 优先遍历弹出元素下一级的下一级元素 以一颗满二叉树为例，如下图123456789# 定义节点类class Node(object):def __init__(self,val,left=None,right=None):self.val = valself.left = leftself.right = right# 创建树模型node = Node("A",Node("B",Node("D"),Node("E")),Node("C",Node("F"),Node("G"))) 如图，A节点的下一级元素为B节点和C节点，B节点的下一级元素为D节点和E节点，C节点的下一级元素为F节点和G节点。BFS优先遍历当前节点下一级节点的同级元素，即若当前节点为A节点，则继续遍历的节点为B和C；当A的所有相邻节点遍历完以后，再遍历B节点的相邻节点D节点和E节点，以及C节点的相邻节点F节点和G节点。至此，所有节点遍历完成。DFS优先遍历当前节点下一级节点的下一级元素，即若当前节点为A节点，则继续遍历的节点为B节点和B节点的下一级节点D节点；D节点没有下一级节点，此时再返回D节点的上一级B节点处，再遍历B节点的另一个下一级元素E节点，若没有未遍历过的下一级元素，则返回上一级，依此规律遍历整个树，完成树的DFS。代码中为了更直观地观察遍历顺序，采用直接打印node.val的方式输出遍历结果代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041def BFS(root):# 使用列表作为队列queue = []# 将首个根节点添加到队列中queue.append(root)# 当队列不为空时进行遍历while queue:# 从队列头部取出一个节点并判断其是否有左右节点# 若有子节点则把对应子节点添加到队列中，且优先判断左节点temp = queue.pop(0)left = temp.leftright = temp.rightif left:queue.append(left)if right:queue.append(right)print(temp.val,end=" ")def DFS(root):# 使用列表作为栈stack = []# 将首个根节点添加到栈中stack.append(root)# 当栈不为空时进行遍历while stack:# 从栈的末尾弹出一个节点并判断其是否有左右节点# 若有子节点则把对应子节点压入栈中，且优先判断右节点temp = stack.pop()left = temp.leftright = temp.rightif right:stack.append(right)if left:stack.append(left)print(temp.val,end=" ")print("BFS",end=" ")BFS(node)print("")print("DFS",end=" ")DFS(node) 输出结果：12BFS A B C D E F G DFS A B D E C F G 代码输出结果与上述分析相符]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>树的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树的前序、中序、后序遍历]]></title>
    <url>%2F2016%2F12%2F05%2FPython%2F%E6%A0%91%E7%9A%84%E5%89%8D%E5%BA%8F%E3%80%81%E4%B8%AD%E5%BA%8F%E3%80%81%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[Python树的前序、中序、后序遍历树的基础遍历分为三种：前序遍历、中序遍历、后序遍历 前序遍历：根节点-&gt;左子树-&gt;右子树中序遍历：左子树-&gt;根节点-&gt;右子树后序遍历：左子树-&gt;右子树-&gt;根节点 首先自定义树模型 1234567class Node(object):def __init__(self,val,left=None,right=None):self.val = valself.left = leftself.right = rightnode = Node("A",Node("B",Node("D"),Node("E")),Node("C",Node("F"),Node("G"))) 代码中为了更直观地观察遍历顺序，采用直接打印node.val的方式输出遍历结果代码实现： 1234567891011121314151617181920212223242526272829303132# 前序遍历def PreTraverse(root):if root == None:returnprint(root.val,end=" ")PreTraverse(root.left)PreTraverse(root.right)# 中序遍历def MidTraverse(root):if root == None:returnMidTraverse(root.left)print(root.val,end=" ")MidTraverse(root.right)# 后序遍历def AfterTraverse(root):if root == None:returnAfterTraverse(root.left)AfterTraverse(root.right)print(root.val,end=" ")print("前序遍历",end="")PreTraverse(node)print("")print("中序遍历",end="")MidTraverse(node)print("")print("后序遍历",end="")AfterTraverse(node) 输出结果：123前序遍历 A B D E C F G 中序遍历 D B E A F C G 后序遍历 D E B F G C A 输入结果与上述分析一致]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>树的遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯消去求线性方程的解]]></title>
    <url>%2F2016%2F11%2F03%2FPython%2F%E9%AB%98%E6%96%AF%E6%B6%88%E5%8E%BB%E6%B1%82%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%9A%84%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[数学上，高斯消元法（或译：高斯消去法），是线性代数规划中的一个算法，可用来为线性方程组求解。但其算法十分复杂，不常用于加减消元法，求出矩阵的秩，以及求出可逆方阵的逆矩阵。不过，如果有过百万条等式时，这个算法会十分省时。一些极大的方程组通常会用迭代法以及花式消元来解决。当用于一个矩阵时，高斯消元法会产生出一个“行梯阵式”。高斯消元法可以用在电脑中来解决数千条等式及未知数。亦有一些方法特地用来解决一些有特别排列的系数的方程组。1234567891011121314151617181920212223242526272829'''高斯消去法通过消元过程把一般方程组化成三角方程组再通过回代过程求出方程组的解'''def GaussianElimination(A,B):N = len(A)for i in range(1,N):for j in range(i,N):# 计算消元因子deltadelta = A[j][i-1]/A[i-1][i-1]# 从第i-1行开始消元for k in range(i-1,N):# 对A进行消元A[j][k] = A[j][k] - A[i-1][k]*delta# 对B进行消元B[j] = B[j]-B[i-1]*delta# 进行回代，直接将方程的解保留在B中B[N-1] = B[N-1]/A[N-1][N-1]for i in range(N-2,-1,-1):for j in range(N-1,i,-1):B[i] = B[i]- A[i][j]*B[j]B[i] = B[i]/A[i][i]# 返回所有解的列表return BmatrixA = [[1,3,3],[-2,3,-5],[2,5,6]]matrixB = [4,0,1]print('方程的解为',GaussianElimination(matrixA, matrixB)) 输出的结果：1方程的解为 [148.0, 7.0, -55.0]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>解线性方程</tag>
      </tags>
  </entry>
</search>
